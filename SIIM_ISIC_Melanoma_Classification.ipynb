{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SIIM-ISIC Melanoma Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "CwWrFw5C-R5J",
        "2X-gMuI261-K",
        "Z9m9lx4ewYzk",
        "ndmsl1XUrWFp",
        "6MPMH0blnHpK"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO7rPu8cNNh32Lqu4TYKLtG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "482ba6e80e2c4b32ab8fd3d7ec5bf5cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3d1784873bc046b3921f1dd5f87fa28d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_43af6aa41a6842c5aa4164d57e4d4ac2",
              "IPY_MODEL_8b7331be437149ef9aca79da23736d3f"
            ]
          }
        },
        "3d1784873bc046b3921f1dd5f87fa28d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "43af6aa41a6842c5aa4164d57e4d4ac2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_719eaaee6fac4a0bba11b5010ac90d11",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 43,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 43,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a83e96f806fb4971aad60264a7a4472f"
          }
        },
        "8b7331be437149ef9aca79da23736d3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_040196ee54ec4e70b617b4f76a15bde3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 43/43 [00:38&lt;00:00,  1.10it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_765cd04af6c84804a3a72289df33127e"
          }
        },
        "719eaaee6fac4a0bba11b5010ac90d11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a83e96f806fb4971aad60264a7a4472f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "040196ee54ec4e70b617b4f76a15bde3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "765cd04af6c84804a3a72289df33127e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utsavnandi/Kaggle-SIIM-ISIC-Melanoma-Classification/blob/master/SIIM_ISIC_Melanoma_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-_fb9AEa6Wq",
        "colab_type": "text"
      },
      "source": [
        "## One-time\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymGpJBznaEZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "!pip uninstall kaggle -y\n",
        "!pip install kaggle==1.5.6 -q\n",
        "!pip install -U git+https://github.com/albu/albumentations -q\n",
        "!pip install -U git+https://github.com/rwightman/pytorch-image-models -q\n",
        "!pip install neptune-client -q\n",
        "!mkdir ~/.kaggle/\n",
        "!cp ./kaggle.json  ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d shonenkov/melanoma-merged-external-data-512x512-jpeg\n",
        "!unzip melanoma-merged-external-data-512x512-jpeg.zip -d ./data/    \n",
        "!rm melanoma-merged-external-data-512x512-jpeg.zip\n",
        "!kaggle competitions download siim-isic-melanoma-classification -f sample_submission.csv\n",
        "!kaggle competitions download siim-isic-melanoma-classification -f test.csv\n",
        "!kaggle competitions download siim-isic-melanoma-classification -f train.csv\n",
        "!kaggle datasets download -d nroman/melanoma-hairs\n",
        "!unzip train.csv -d ./data/ &> /dev/null\n",
        "!unzip ./melanoma-hairs.zip -d ./data/\n",
        "!rm ./melanoma-hairs.zip\n",
        "!mv ./test.csv ./data/\n",
        "!mv ./sample_submission.csv ./data/\n",
        "!rm train.csv.zip\n",
        "!mkdir ./logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFoIORVvcWn5",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cB029jhCcJFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "from getpass import getpass\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from google.colab import auth\n",
        "from google.cloud import storage\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from scipy.special import kl_div\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torch.cuda.amp import GradScaler\n",
        "import torchvision\n",
        "\n",
        "import timm\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "\n",
        "import neptune\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(43)\n",
        "\n",
        "#!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UON0HEIZ-K2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PROJECT_ID = getpass(prompt='Enter gcp project id: ')\n",
        "bucket_name = getpass(prompt='Enter gcp bucket name: ')\n",
        "\n",
        "if PROJECT_ID != '' and bucket_name !='':\n",
        "    auth.authenticate_user()\n",
        "\n",
        "!gcloud config set project {PROJECT_ID}\n",
        "\n",
        "if PROJECT_ID != '' and bucket_name !='':\n",
        "    storage_client = storage.Client(project=PROJECT_ID)\n",
        "\n",
        "def upload_blob(source_file_name, destination_blob_name, bucket_name=bucket_name, PROJECT_ID=PROJECT_ID):\n",
        "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
        "    storage_client = storage.Client(project=PROJECT_ID)\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    dt_now = datetime.datetime.now().strftime(\"%d_%B\")\n",
        "    destination_blob_name = 'siim-isic/'+dt_now+'/'+destination_blob_name\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "    blob.upload_from_filename(source_file_name)\n",
        "    print(\"File {} uploaded to {}.\".format(source_file_name, destination_blob_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "284zZDwUjZqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NEPTUNE_API_TOKEN = getpass(prompt='Enter neptune api token: ')\n",
        "\n",
        "if NEPTUNE_API_TOKEN != '':\n",
        "    os.environ['NEPTUNE_API_TOKEN'] = NEPTUNE_API_TOKEN\n",
        "    log = True\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-XASBvOcVwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_DIR = '/content/data/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uG9Xunfy3N3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(DATA_DIR+'folds_13062020.csv')\n",
        "df_test = pd.read_csv(DATA_DIR+'test.csv').rename(columns={'image_name':'image_id'})\n",
        "sample_submission = pd.read_csv(DATA_DIR+'sample_submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIKzfKD2zA9A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train['fold'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOuLmekTzlM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fold_no = 0\n",
        "X_train = df_train[df_train['fold'] != fold_no][[col for col in df_train.columns if col != 'target']]\n",
        "y_train = df_train[df_train['fold'] != fold_no][[col for col in df_train.columns if col == 'target']]\n",
        "#y_train = pd.get_dummies(df_train[df_train['fold'] != fold_no][[col for col in df_train.columns if col == 'target']], columns=['target'])\n",
        "X_val = df_train[df_train['fold'] == fold_no][[col for col in df_train.columns if col != 'target']]\n",
        "y_val = df_train[df_train['fold'] == fold_no][[col for col in df_train.columns if col == 'target']]\n",
        "#y_val = pd.get_dummies(df_train[df_train['fold'] == fold_no][[col for col in df_train.columns if col == 'target']], columns=['target'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmBYqF4fJEgc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.where(y_train.values[:, 0]==1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yovLpNhMcvnW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('X_train', X_train.shape)\n",
        "print('y_train', y_train.shape)\n",
        "print('X_val', X_val.shape)\n",
        "print('y_val', y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWae70vKk9dw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Train target distribution: ')\n",
        "print(y_train['target'].value_counts())\n",
        "print('Val target distribution: ')\n",
        "print(y_val['target'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwWrFw5C-R5J",
        "colab_type": "text"
      },
      "source": [
        "## With External Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wK-NTFy-PwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MelanomaDataset(Dataset):\n",
        "\n",
        "    def __init__(self, df, labels, istrain=False, transforms=None):\n",
        "        super().__init__()\n",
        "        self.image_id = df['image_id'].values\n",
        "        self.transforms = transforms\n",
        "        self.labels = labels.values\n",
        "        self.neg_indices = np.where(self.labels[:, 0] == 0)[0]\n",
        "        self.pos_indices = np.where(self.labels[:, 0] == 1)[0]\n",
        "        self.istrain = istrain\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_id)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if torch.is_tensor(index):\n",
        "            index = index.tolist()\n",
        "        \n",
        "        image, target = self.load_image(index)\n",
        "        \n",
        "        if self.transforms:\n",
        "            image = self.transforms(image=image)['image']\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def load_image(self, index):\n",
        "        if torch.is_tensor(index):\n",
        "            index = index.tolist()\n",
        "        image_name = DATA_DIR + f'512x512-dataset-melanoma/512x512-dataset-melanoma/{self.image_id[index]}.jpg'\n",
        "        image = cv2.imread(image_name, cv2.IMREAD_COLOR).astype(np.uint8)\n",
        "        target = self.labels[index].astype(np.float32)\n",
        "        return image, target\n",
        "\n",
        "    def get_rand_index(self):\n",
        "        if np.random.random()>0.5:\n",
        "            rand_index = np.random.choice(self.pos_indices)\n",
        "        else:\n",
        "            rand_index = np.random.choice(self.neg_indices)\n",
        "        return rand_index\n",
        "\n",
        "def get_datasets(data):\n",
        "    X_train, y_train, X_val, y_val = data\n",
        "    datasets = {}\n",
        "    datasets['train'] = MelanomaDataset(\n",
        "        X_train, y_train, istrain=True, transforms=get_train_transforms()\n",
        "    )\n",
        "    datasets['valid'] = MelanomaDataset(\n",
        "        X_val, y_val, istrain=False, transforms=get_valid_transforms()\n",
        "    )\n",
        "    return datasets\n",
        "\n",
        "class MelanomaEvalDataset(Dataset):\n",
        "    def __init__(self, df, labels, isEval=True, transform=None):\n",
        "        super().__init__()\n",
        "        self.image_id = df['image_id'].values\n",
        "        self.transform = transform\n",
        "        self.isEval = isEval\n",
        "        if not self.isEval:\n",
        "            self.labels = labels.values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_id)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if torch.is_tensor(index):\n",
        "            index = index.tolist()\n",
        "        \n",
        "        if self.isEval:\n",
        "            image_name = DATA_DIR + f'512x512-test/512x512-test/{self.image_id[index]}.jpg'\n",
        "        else:\n",
        "            image_name = DATA_DIR + f'512x512-dataset-melanoma/512x512-dataset-melanoma/{self.image_id[index]}.jpg'\n",
        "\n",
        "        image = cv2.imread(image_name, cv2.IMREAD_COLOR).astype(np.uint8)\n",
        "        #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.uint8)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image=image)['image']\n",
        "        else:\n",
        "            image = np.transpose(image, (2, 1, 0))\n",
        "\n",
        "        if self.isEval:\n",
        "            return image\n",
        "\n",
        "        target = self.labels[index].astype(np.float32)\n",
        "        return image, target\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X-gMuI261-K",
        "colab_type": "text"
      },
      "source": [
        "## Batch cutmix/mixup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGfkLJy6f4K4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rand_bbox(size, lam):\n",
        "    W = size[2]\n",
        "    H = size[3]\n",
        "    cut_rat = np.sqrt(1. - lam)\n",
        "    cut_w = np.int(W * cut_rat)\n",
        "    cut_h = np.int(H * cut_rat)\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "    return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "def cutmix(data, targets1, alpha=1):\n",
        "    indices = np.random.permutation(data.size(0))\n",
        "    shuffled_data = data[indices]\n",
        "    shuffled_targets1 = targets1[indices]\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    bbx1, bby1, bbx2, bby2 = rand_bbox(data.size(), lam)\n",
        "    data[:, :, bbx1:bbx2, bby1:bby2] = data[indices, :, bbx1:bbx2, bby1:bby2]\n",
        "    # adjust lambda to exactly match pixel ratio\n",
        "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (data.size()[-1] * data.size()[-2]))\n",
        "    targets = [targets1, shuffled_targets1, lam]\n",
        "    return data, targets\n",
        "\n",
        "def mixup(data, targets1, alpha=1):\n",
        "    indices = np.random.permutation(data.size(0))\n",
        "    shuffled_data = data[indices]\n",
        "    shuffled_targets1 = targets1[indices]\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    data = data * lam + shuffled_data * (1 - lam)\n",
        "    targets = [targets1, shuffled_targets1, lam]\n",
        "\n",
        "    return data, targets\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFL68tLin0Ky",
        "colab_type": "text"
      },
      "source": [
        "## Augmentations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US0B-vLV6JMn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from albumentations.augmentations import functional as FA\n",
        "from albumentations.core.transforms_interface import DualTransform\n",
        "\n",
        "class GridMask(DualTransform):\n",
        "    def __init__(self, num_grid=3, fill_value=0, rotate=0, mode=0, always_apply=False, p=0.5):\n",
        "        super(GridMask, self).__init__(always_apply, p)\n",
        "        if isinstance(num_grid, int):\n",
        "            num_grid = (num_grid, num_grid)\n",
        "        if isinstance(rotate, int):\n",
        "            rotate = (-rotate, rotate)\n",
        "        self.num_grid = num_grid\n",
        "        self.fill_value = fill_value\n",
        "        self.rotate = rotate\n",
        "        self.mode = mode\n",
        "        self.masks = None\n",
        "        self.rand_h_max = []\n",
        "        self.rand_w_max = []\n",
        "\n",
        "    def init_masks(self, height, width):\n",
        "        if self.masks is None:\n",
        "            self.masks = []\n",
        "            n_masks = self.num_grid[1] - self.num_grid[0] + 1\n",
        "            for n, n_g in enumerate(range(self.num_grid[0], self.num_grid[1] + 1, 1)):\n",
        "                grid_h = height / n_g\n",
        "                grid_w = width / n_g\n",
        "                this_mask = np.ones((int((n_g + 1) * grid_h), int((n_g + 1) * grid_w))).astype(np.uint8)\n",
        "                for i in range(n_g + 1):\n",
        "                    for j in range(n_g + 1):\n",
        "                        this_mask[\n",
        "                             int(i * grid_h) : int(i * grid_h + grid_h / 2),\n",
        "                             int(j * grid_w) : int(j * grid_w + grid_w / 2)\n",
        "                        ] = self.fill_value\n",
        "                        if self.mode == 2:\n",
        "                            this_mask[\n",
        "                                 int(i * grid_h + grid_h / 2) : int(i * grid_h + grid_h),\n",
        "                                 int(j * grid_w + grid_w / 2) : int(j * grid_w + grid_w)\n",
        "                            ] = self.fill_value\n",
        "                \n",
        "                if self.mode == 1:\n",
        "                    this_mask = 1 - this_mask\n",
        "\n",
        "                self.masks.append(this_mask)\n",
        "                self.rand_h_max.append(grid_h)\n",
        "                self.rand_w_max.append(grid_w)\n",
        "\n",
        "    def apply(self, image, mask, rand_h, rand_w, angle, **params):\n",
        "        h, w = image.shape[:2]\n",
        "        mask = FA.rotate(mask, angle) if self.rotate[1] > 0 else mask\n",
        "        mask = mask[:,:,np.newaxis] if image.ndim == 3 else mask\n",
        "        image *= mask[rand_h:rand_h+h, rand_w:rand_w+w].astype(image.dtype)\n",
        "        return image\n",
        "\n",
        "    def get_params_dependent_on_targets(self, params):\n",
        "        img = params['image']\n",
        "        height, width = img.shape[:2]\n",
        "        self.init_masks(height, width)\n",
        "\n",
        "        mid = np.random.randint(len(self.masks))\n",
        "        mask = self.masks[mid]\n",
        "        rand_h = np.random.randint(self.rand_h_max[mid])\n",
        "        rand_w = np.random.randint(self.rand_w_max[mid])\n",
        "        angle = np.random.randint(self.rotate[0], self.rotate[1]) if self.rotate[1] > 0 else 0\n",
        "\n",
        "        return {'mask': mask, 'rand_h': rand_h, 'rand_w': rand_w, 'angle': angle}\n",
        "\n",
        "    @property\n",
        "    def targets_as_params(self):\n",
        "        return ['image']\n",
        "\n",
        "    def get_transform_init_args_names(self):\n",
        "        return ('num_grid', 'fill_value', 'rotate', 'mode')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPWu-IIliVVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%writefile augmentations.txt\n",
        "# Reference IMG_SIZE\n",
        "# B0 - 224\n",
        "# B1 - 240\n",
        "# B2 - 260\n",
        "# B3 - 300\n",
        "# B4 - 380\n",
        "\n",
        "# Transforms\n",
        "\n",
        "IMG_SIZE = 224\n",
        "mean = (0.485, 0.456, 0.406)\n",
        "std = (0.229, 0.224, 0.225)\n",
        "\n",
        "def get_train_transforms(p=1.0):\n",
        "    return A.Compose(\n",
        "        [\n",
        "            A.OneOf(\n",
        "                [\n",
        "                    A.CenterCrop(2 * IMG_SIZE // 3, 2 * IMG_SIZE // 3, p=0.5),\n",
        "                    A.CenterCrop(3 * IMG_SIZE // 4, 3 * IMG_SIZE // 4, p=0.5),\n",
        "                ],\n",
        "                p=0.33,\n",
        "            ),\n",
        "            A.Resize(\n",
        "                IMG_SIZE, IMG_SIZE, interpolation=1, always_apply=True, p=1\n",
        "            ),\n",
        "            A.Flip(),\n",
        "            A.Transpose(),\n",
        "            GridMask(num_grid=(1, 4), rotate=15, p=0.33),\n",
        "            A.OneOf(\n",
        "                [\n",
        "                    A.MedianBlur(blur_limit=3, p=0.5),\n",
        "                    A.Blur(blur_limit=3, p=0.5),\n",
        "                ],\n",
        "                p=0.5,\n",
        "            ),\n",
        "            A.OneOf(\n",
        "                [\n",
        "                    A.ShiftScaleRotate(\n",
        "                        interpolation=1,\n",
        "                        shift_limit=0.05,\n",
        "                        scale_limit=0.1,\n",
        "                        rotate_limit=15,\n",
        "                        p=0.5,\n",
        "                    ),\n",
        "                    A.IAAPiecewiseAffine(scale=(0.02, 0.04), p=0.5),\n",
        "                ],\n",
        "                p=0.33,\n",
        "            ),\n",
        "            A.OneOf(\n",
        "                [\n",
        "                    A.HueSaturationValue(\n",
        "                        hue_shift_limit=20,\n",
        "                        sat_shift_limit=30,\n",
        "                        val_shift_limit=20,\n",
        "                        p=0.5,\n",
        "                    ),\n",
        "                    A.RandomBrightnessContrast(p=0.5),\n",
        "                ],\n",
        "                p=0.5,\n",
        "            ),\n",
        "            A.MultiplicativeNoise(\n",
        "                multiplier=[0.9, 1.1], elementwise=True, p=0.3\n",
        "            ),\n",
        "            A.Normalize(mean, std, max_pixel_value=255.0, always_apply=True),\n",
        "            ToTensorV2(p=1.0),\n",
        "        ],\n",
        "        p=p,\n",
        "    )\n",
        "\n",
        "def get_valid_transforms():\n",
        "    return A.Compose([\n",
        "        A.Resize(IMG_SIZE, IMG_SIZE, interpolation=2, always_apply=True, p=1),\n",
        "        A.Normalize(mean, std, max_pixel_value=255.0, always_apply=True),\n",
        "        ToTensorV2(p=1.0),\n",
        "    ])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGJjzIKAMEy0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# not used\n",
        "class AdvancedHairAugmentation:\n",
        "    def __init__(self, hairs: int = 4, hairs_folder: str = \"\"):\n",
        "        self.hairs = hairs\n",
        "        self.hairs_folder = hairs_folder\n",
        "\n",
        "    def __call__(self, img):\n",
        "        n_hairs = random.randint(0, self.hairs)\n",
        "\n",
        "        if not n_hairs:\n",
        "            return img\n",
        "\n",
        "        height, width, _ = img.shape  # target image width and height\n",
        "        hair_images = [im for im in os.listdir(self.hairs_folder) if 'png' in im]\n",
        "\n",
        "        for _ in range(n_hairs):\n",
        "            hair = cv2.imread(os.path.join(self.hairs_folder, random.choice(hair_images)))\n",
        "            hair = cv2.flip(hair, random.choice([-1, 0, 1]))\n",
        "            hair = cv2.rotate(hair, random.choice([0, 1, 2]))\n",
        "\n",
        "            h_height, h_width, _ = hair.shape  # hair image width and height\n",
        "            roi_ho = random.randint(0, img.shape[0] - hair.shape[0])\n",
        "            roi_wo = random.randint(0, img.shape[1] - hair.shape[1])\n",
        "            roi = img[roi_ho:roi_ho + h_height, roi_wo:roi_wo + h_width]\n",
        "\n",
        "            img2gray = cv2.cvtColor(hair, cv2.COLOR_BGR2GRAY)\n",
        "            ret, mask = cv2.threshold(img2gray, 10, 255, cv2.THRESH_BINARY)\n",
        "            mask_inv = cv2.bitwise_not(mask)\n",
        "            img_bg = cv2.bitwise_and(roi, roi, mask=mask_inv)\n",
        "            hair_fg = cv2.bitwise_and(hair, hair, mask=mask)\n",
        "\n",
        "            dst = cv2.add(img_bg, hair_fg)\n",
        "            img[roi_ho:roi_ho + h_height, roi_wo:roi_wo + h_width] = dst\n",
        "\n",
        "        return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coB5w10-7oBw",
        "colab_type": "text"
      },
      "source": [
        "## Visualise Transforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a7GCYU_7myh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualize_dataset(dataset, shuffle=True):\n",
        "    labels_vcount = y_train['target'].value_counts()\n",
        "    #labels_vcount = pd.Series(y_train).value_counts()\n",
        "    class_counts = [labels_vcount[0].astype(np.float32), labels_vcount[1].astype(np.float32)]\n",
        "    num_samples = sum(class_counts)\n",
        "    class_weights = [num_samples/class_counts[i] for i in range(len(class_counts))]\n",
        "    weights = [class_weights[y_train['target'].values[i]] for i in range(int(num_samples))]\n",
        "    sampler = WeightedRandomSampler(torch.DoubleTensor(weights), int(num_samples))\n",
        "\n",
        "    dl = DataLoader(dt, batch_size=8, sampler=sampler)\n",
        "    for images, targets in dl:\n",
        "        images = images.to('cuda')\n",
        "        targets = targets.to('cuda')\n",
        "        #if np.random.rand()<0.5:\n",
        "        #    images, targets = cutmix(images, targets)\n",
        "        #else:\n",
        "        #    images, targets = mixup(images, targets)\n",
        "        #for i, (image, target, shuffled_target) in enumerate(zip(images, targets[0], targets[1])):\n",
        "        #    print(image.shape)\n",
        "        #    image = image.detach().cpu().numpy().transpose((1,2,0))\n",
        "        #    plt.imshow(image.astype(np.uint8))\n",
        "        #    plt.show()\n",
        "        #    print('actual target', target)\n",
        "        #    print('shuffled target', shuffled_target)\n",
        "        #    print('---------------------------------')\n",
        "        #    if i>9:\n",
        "        #        break\n",
        "        #break\n",
        "        for i, (image, target) in enumerate(zip(images, targets)):\n",
        "            image = image.detach().cpu().numpy().transpose((1,2,0))\n",
        "            image = (image  * np.array([0.229, 0.224, 0.225])) + np.array([0.485, 0.456, 0.406])\n",
        "            plt.figure(figsize=(6,6))\n",
        "            plt.axis('off')\n",
        "            plt.imshow(image)\n",
        "            plt.show()\n",
        "            print('actual target', target)\n",
        "            print('image shape', image.shape)\n",
        "            if i==8:\n",
        "                break\n",
        "        break\n",
        "#dt = MelanomaDataset(X_train[:32], y_train[:32], isEval=False, transform=strong_aug())\n",
        "#dt = MelanomaDataset(X_train, y_train, isEval=False, transform=strong_aug())\n",
        "\n",
        "dt = MelanomaDataset(X_train, y_train, istrain=True, transforms=get_train_transforms())\n",
        "visualize_dataset(dt, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aeIKSF7mJFT",
        "colab_type": "text"
      },
      "source": [
        "## Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbtlOIcVe6SC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.spatial import distance\n",
        "\n",
        "def plot_roc(y_true, y_pred, show=False):\n",
        "    testy, lr_probs = y_true, y_pred\n",
        "    ns_probs = [0 for _ in range(len(testy))]\n",
        "    # calculate roc curves\n",
        "    ns_fpr, ns_tpr, _ = roc_curve(testy, ns_probs)\n",
        "    lr_fpr, lr_tpr, _ = roc_curve(testy, lr_probs) #lr_probs: predictions\n",
        "    # plot the roc curve for the model\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
        "    plt.plot(lr_fpr, lr_tpr, linestyle='-', label='Model')\n",
        "    # axis labels\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    # show the legend\n",
        "    plt.legend()\n",
        "    #save\n",
        "    filename = f'/content/logs/roc_curve_{datetime.datetime.now().strftime(\"%d-%m-%Y-%HH-%MM\")}.png'\n",
        "    plt.savefig(filename)\n",
        "    # show the plot\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.clf()\n",
        "    return filename\n",
        "\n",
        "\n",
        "def draw_hist(y_true, y_pred, show=True):\n",
        "    true_neg_indices = np.where(y_true[:, 0] == 0)[0]\n",
        "    true_pos_indices = np.where(y_true[:, 0] == 1)[0]\n",
        "    pred_true_pos = y_pred[true_pos_indices]\n",
        "    pred_true_neg = y_pred[true_neg_indices]\n",
        "    thresh = 0.2\n",
        "    pred_true_pos_error_count = pred_true_pos[np.where(pred_true_pos<(1-thresh))[0]]\n",
        "    pred_true_neg_error_count = pred_true_neg[np.where(pred_true_neg>(thresh))[0]]\n",
        "    total_error_count = pred_true_pos_error_count.shape[0] + pred_true_neg_error_count.shape[0] \n",
        "    total_error_count_scaled = total_error_count/y_true.shape[0]\n",
        "    # plot\n",
        "    figure = plt.figure(figsize=(14,6))\n",
        "    ax1 = plt.subplot(121)\n",
        "    ax1.hist(pred_true_neg.T[0],bins=10)\n",
        "    plt.ylim((0,2000))\n",
        "    ax2 = plt.subplot(122)\n",
        "    ax2.hist(pred_true_pos.T[0],bins=10)\n",
        "    if show:\n",
        "        plt.show()  \n",
        "    return figure, total_error_count_scaled\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9m9lx4ewYzk",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfPBXkoYyPAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EfficientNet(nn.Module):\n",
        "\n",
        "    def __init__(self, name='tf_efficientnet_b0_ns'):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model(name, pretrained=True)\n",
        "        in_features = self.model.classifier.in_features\n",
        "        self.model.classifier = nn.Linear(in_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class Tf_efficientnet_b0_Mod(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model('tf_efficientnet_b0', pretrained=True)\n",
        "        in_features = self.model.classifier.in_features\n",
        "        self.model.classifier = nn.Linear(in_features, int(in_features/2))\n",
        "        self.bn_1 = nn.BatchNorm1d(int(in_features/2))\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.drop_1 = nn.Dropout(0.2)\n",
        "        self.fc_2 = nn.Linear(int(in_features/2), 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x = self.bn_1(x)\n",
        "        x = self.relu_1(x)\n",
        "        x = self.drop_1(x)\n",
        "        x = self.fc_2(x)\n",
        "        return x\n",
        "\n",
        "class Tf_efficientnet_b1_Mod(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model('tf_efficientnet_b1', pretrained=True)\n",
        "        in_features = self.model.classifier.in_features\n",
        "        self.model.classifier = nn.Linear(in_features, int(in_features/2))\n",
        "        self.bn_1 = nn.BatchNorm1d(int(in_features/2))\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.drop_1 = nn.Dropout(0.2)\n",
        "        self.fc_2 = nn.Linear(int(in_features/2), 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x = self.bn_1(x)\n",
        "        x = self.relu_1(x)\n",
        "        x = self.drop_1(x)\n",
        "        x = self.fc_2(x)\n",
        "        return x\n",
        "\n",
        "class Tf_efficientnet_b3(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model('tf_efficientnet_b3', pretrained=True)\n",
        "        in_features = self.model.classifier.in_features\n",
        "        self.model.classifier = nn.Linear(in_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class Tf_efficientnet_b3_ns_Mod(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model('tf_efficientnet_b3_ns', pretrained=True)\n",
        "        in_features = self.model.classifier.in_features\n",
        "        self.model.classifier = nn.Linear(in_features, int(in_features/2))\n",
        "        self.bn_1 = nn.BatchNorm1d(int(in_features/2))\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.drop_1 = nn.Dropout(0.2)\n",
        "        self.fc_2 = nn.Linear(int(in_features/2), 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x = self.bn_1(x)\n",
        "        x = self.relu_1(x)\n",
        "        x = self.drop_1(x)\n",
        "        x = self.fc_2(x)\n",
        "        return x\n",
        "\n",
        "class Tf_efficientnet_b3_ns_Mod_v2(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model('tf_efficientnet_b3_ns', pretrained=True)\n",
        "        in_features = self.model.classifier.in_features\n",
        "        self.model.classifier = nn.Linear(in_features, in_features)\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.bn_1 = nn.BatchNorm1d(in_features)\n",
        "        self.fc_1 = nn.Linear(in_features, int(in_features/2))\n",
        "        self.bn_2 = nn.BatchNorm1d(int(in_features/2))\n",
        "        self.fc_2 = nn.Linear(int(in_features/2), 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x = self.relu_1(x)\n",
        "        x = self.bn_1(x)\n",
        "        x = self.fc_1(x)\n",
        "        x = self.bn_2(x)\n",
        "        x = self.fc_2(x)\n",
        "        return x\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndmsl1XUrWFp",
        "colab_type": "text"
      },
      "source": [
        "## Custom Losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IICyzNqJrZnl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Wrong implementation\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2, logits=True, reduce=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.logits = logits\n",
        "        self.reduce = reduce\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        if self.logits:\n",
        "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
        "        else:\n",
        "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-BCE_loss)\n",
        "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
        "\n",
        "        if self.reduce:\n",
        "            return torch.mean(F_loss)\n",
        "        else:\n",
        "            return F_loss\n",
        "\n",
        "from torch.nn import functional as F\n",
        "\n",
        "def sigmoid_focal_loss(\n",
        "    inputs: torch.Tensor,\n",
        "    targets: torch.Tensor,\n",
        "    alpha: float = 0.75,\n",
        "    gamma: float = 1.75,\n",
        "    reduction: str = \"mean\"\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n",
        "    Args:\n",
        "        inputs: A float tensor of arbitrary shape.\n",
        "                The predictions for each example.\n",
        "        targets: A float tensor with the same shape as inputs. Stores the binary\n",
        "                 classification label for each element in inputs\n",
        "                (0 for the negative class and 1 for the positive class).\n",
        "        alpha: (optional) Weighting factor in range (0,1) to balance\n",
        "                positive vs negative examples. Default = -1 (no weighting).\n",
        "        gamma: Exponent of the modulating factor (1 - p_t) to\n",
        "               balance easy vs hard examples.\n",
        "        reduction: 'none' | 'mean' | 'sum'\n",
        "                 'none': No reduction will be applied to the output.\n",
        "                 'mean': The output will be averaged.\n",
        "                 'sum': The output will be summed.\n",
        "    Returns:\n",
        "        Loss tensor with the reduction option applied.\n",
        "    \"\"\"\n",
        "    p = torch.sigmoid(inputs)\n",
        "    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n",
        "    p_t = p * targets + (1 - p) * (1 - targets)\n",
        "    loss = ce_loss * ((1 - p_t) ** gamma)\n",
        "\n",
        "    if alpha >= 0:\n",
        "        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
        "        loss = alpha_t * loss\n",
        "\n",
        "    if reduction == \"mean\":\n",
        "        loss = loss.mean()\n",
        "    elif reduction == \"sum\":\n",
        "        loss = loss.sum()\n",
        "\n",
        "    return loss\n",
        "\n",
        "class CutMixCrossEntropyLoss(nn.Module):\n",
        "    def __init__(self, size_average=True):\n",
        "        super().__init__()\n",
        "        self.size_average = size_average\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        if len(target.size()) == 1:\n",
        "            target = torch.nn.functional.one_hot(target, num_classes=input.size(-1))\n",
        "            target = target.float()\n",
        "        return cross_entropy(input, target, self.size_average)\n",
        "\n",
        "def cross_entropy(input, target, size_average=True):\n",
        "    \"\"\" Cross entropy that accepts soft targets\n",
        "    Args:\n",
        "         pred: predictions for neural network\n",
        "         targets: targets, can be soft\n",
        "         size_average: if false, sum is returned instead of mean\n",
        "    Examples::\n",
        "        input = torch.FloatTensor([[1.1, 2.8, 1.3], [1.1, 2.1, 4.8]])\n",
        "        input = torch.autograd.Variable(out, requires_grad=True)\n",
        "        target = torch.FloatTensor([[0.05, 0.9, 0.05], [0.05, 0.05, 0.9]])\n",
        "        target = torch.autograd.Variable(y1)\n",
        "        loss = cross_entropy(input, target)\n",
        "        loss.backward()\n",
        "    \"\"\"\n",
        "    logsoftmax = torch.nn.LogSoftmax(dim=1)\n",
        "    if size_average:\n",
        "        return torch.mean(torch.sum(-target * logsoftmax(input), dim=1))\n",
        "    else:\n",
        "        return torch.sum(torch.sum(-target * logsoftmax(input), dim=1))\n",
        "\n",
        "def ohem_loss(cls_pred, cls_target, rate):\n",
        "    ohem_cls_loss = F.binary_cross_entropy_with_logits(cls_pred, cls_target, reduction='none')\n",
        "    batch_size = cls_pred.size(0)\n",
        "    sorted_ohem_loss, idx = torch.sort(ohem_cls_loss, descending=True)\n",
        "    keep_num = min(sorted_ohem_loss.size()[0], int(batch_size*rate))\n",
        "    if keep_num < sorted_ohem_loss.size()[0]:\n",
        "        keep_idx_cuda = idx[:keep_num]\n",
        "        ohem_cls_loss = ohem_cls_loss[keep_idx_cuda]\n",
        "    cls_loss = ohem_cls_loss.sum() / keep_num\n",
        "    return cls_loss\n",
        "\n",
        "# batch wise\n",
        "def cutmix_mixup_criterion(\n",
        "    preds1, targets, rate=0.7, epsilon=0.05, loss_type=None,\n",
        "    alpha=-1, gamma = 2.0\n",
        "):\n",
        "    targets1, targets2, lam = targets[0], targets[1], targets[2]\n",
        "    if loss_type is None:\n",
        "        if rate>0:\n",
        "            loss = lam * smooth_ohem_criterion(preds1, targets1, rate, epsilon) \n",
        "            + (1 - lam) * smooth_ohem_criterion(preds1, targets2, rate, epsilon)\n",
        "        else:\n",
        "            loss = lam * smooth_criterion(preds1, targets1, epsilon) \n",
        "            + (1 - lam) * smooth_criterion(preds1, targets2, epsilon)\n",
        "    elif loss_type=='focal':\n",
        "        loss = lam * sigmoid_focal_loss(preds1, targets1, alpha, gamma) \n",
        "        + (1 - lam) * sigmoid_focal_loss(preds1, targets2, alpha, gamma)\n",
        "    return loss\n",
        "\n",
        "def smooth_ohem_criterion(y_pred, y_true, rate, e):\n",
        "    e = torch.tensor(e).to(device)\n",
        "    y_true = torch.where(y_true == 0, e, y_true)\n",
        "    y_true = torch.where(y_true == 1, (1-e), y_true)\n",
        "    return ohem_loss(y_pred, y_true, rate)\n",
        "\n",
        "def smooth_criterion(y_pred, y_true, e=0.05):\n",
        "    e = torch.tensor(e).to('device')\n",
        "    y_true = torch.where(y_true == 0, e, y_true)\n",
        "    y_true = torch.where(y_true == 1, (1-e), y_true)\n",
        "    return nn.BCEWithLogitsLoss()(y_pred, y_true)\n",
        "\n",
        "def bce_criterion(y_pred, y_true):\n",
        "    return nn.BCEWithLogitsLoss()(y_pred, y_true)\n",
        "\n",
        "def cutmix_ce_criterion(y_pred, y_true):\n",
        "    return CutMixCrossEntropyLoss()(y_pred, y_true)\n",
        "\n",
        "def focal_criterion(y_pred, y_true):\n",
        "    return FocalLoss(alpha=(43997/4384))(y_pred, y_true)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tub6p0D-qdYU",
        "colab_type": "text"
      },
      "source": [
        "## Train script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-VtUtpsycpH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "def train_one_epoch(loader, model, optimizer, epoch, scheduler=None, scaler=None, log=True):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for idx, (images, targets) in tqdm(enumerate(loader), total=len(loader)): #\n",
        "        images = images.to(device)\n",
        "        targets = targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(images.float())\n",
        "        loss = sigmoid_focal_loss(y_pred, targets, FLAGS['alpha'], FLAGS['gamma'])\n",
        "        running_loss += float(loss)\n",
        "        if scaler:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        if log and (idx+1) % FLAGS['log_interval'] == 0:\n",
        "            neptune.log_metric(\"Loss/train\", float(loss))\n",
        "\n",
        "    return running_loss/len(loader)\n",
        "\n",
        "def val_one_epoch(loader, model):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    y_preds_list = []\n",
        "    targets_list = []\n",
        "    with torch.no_grad():\n",
        "        for idx, (images, targets) in tqdm(enumerate(loader), total=len(loader)):\n",
        "            images = images.to(device)\n",
        "            targets = targets.to(device)#.unsqueeze(1)\n",
        "            y_pred = model(images.float())\n",
        "            loss = sigmoid_focal_loss(y_pred, targets, FLAGS['alpha'], FLAGS['gamma'])\n",
        "            running_loss += float(loss)\n",
        "            y_preds_list.append(torch.sigmoid(y_pred).cpu().numpy())\n",
        "            targets_list.append(targets.cpu().numpy())\n",
        "        y_true = np.vstack(targets_list)\n",
        "        y_pred = np.vstack(y_preds_list)\n",
        "        auc_score = roc_auc_score(y_true, y_pred) # add [:, 1] for cross entropy\n",
        "        roc_plot = plot_roc(y_true, y_pred) # add [:, 1] for cross entropy\n",
        "        hist, error_scaled = draw_hist(y_true, y_pred)\n",
        "        print(f'roc_auc_score: {auc_score:.5f}')\n",
        "        print(f'average loss for val epoch: {running_loss/len(loader):.5f}')\n",
        "        print(f'scaled error: {error_scaled:.5f}')\n",
        "        jsd = jensenshannon(y_true, y_pred, 2.0)[0]\n",
        "        print(f\"JS distance: {jsd:.5f}\")\n",
        "        kld = kl_div(y_true, y_pred).mean()\n",
        "        print(f'mean KL divergence: {kld:.5f}')\n",
        "    return running_loss/len(loader), auc_score, roc_plot, hist, error_scaled, jsd, kld\n",
        "   \n",
        "def save_upload(model, optimizer, score, epoch, fold=None, exp_name='model'):\n",
        "    if fold:\n",
        "        NAME = exp_name+f'_fold_{str(fold+1)}_{str(epoch+1)}.ckpt'\n",
        "    NAME = exp_name+f'_{str(epoch+1)}.ckpt'\n",
        "    MODEL_PATH = NAME\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }, MODEL_PATH)\n",
        "    print(f\"Saved ckpt for epoch {epoch+1}, score: {score:.5f}\")\n",
        "    upload_blob(MODEL_PATH, NAME)\n",
        "    print(f\"Uploaded ckpt for epoch {epoch+1}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TggTDLcOxUT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(data, fold=None, log=True):\n",
        "\n",
        "    best_score = 0.0\n",
        "\n",
        "    model = EfficientNet('tf_efficientnet_b0_ns').to(device)\n",
        "    #model.load_state_dict(\n",
        "    #    torch.load(\n",
        "    #        '/content/siim-isic_efficientnet_b0_15.ckpt'\n",
        "    #        )['model_state_dict']\n",
        "    #)\n",
        "    if log:\n",
        "        neptune.init('utsav/SIIM-ISIC', api_token=NEPTUNE_API_TOKEN)\n",
        "        neptune.create_experiment(\n",
        "            FLAGS['exp_name'], exp_description, params=FLAGS,\n",
        "            upload_source_files='*.txt'\n",
        "        )\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(), \n",
        "        lr=FLAGS['learning_rate'], \n",
        "        weight_decay=FLAGS['weight_decay']\n",
        "    )\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, factor=0.5, cooldown=0, mode='min',\n",
        "        patience=3, verbose=True, min_lr=1e-8\n",
        "    )\n",
        "    \n",
        "    datasets = get_datasets(data)\n",
        "    \n",
        "    #sampler - not needed for focal loss\n",
        "    #labels_vcount = y_train['target'].value_counts()\n",
        "    #class_counts = [labels_vcount[0].astype(np.float32), labels_vcount[1].astype(np.float32)]\n",
        "    #num_samples = sum(class_counts)\n",
        "    #class_weights = [num_samples/class_counts[i] for i in range(len(class_counts))]\n",
        "    #weights = [class_weights[y_train['target'].values[i]] for i in range(int(num_samples))]\n",
        "    #sampler = WeightedRandomSampler(torch.DoubleTensor(weights), int(num_samples))\n",
        "    \n",
        "    #loaders\n",
        "    train_loader = DataLoader(\n",
        "        datasets['train'], batch_size=FLAGS['batch_size'], \n",
        "        num_workers=FLAGS['num_workers'],\n",
        "        shuffle=True, # sampler=sampler,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        datasets['valid'], batch_size=FLAGS['batch_size'] * 2, shuffle=False, \n",
        "        num_workers=FLAGS['num_workers'], drop_last=True\n",
        "    )\n",
        "\n",
        "    scaler = GradScaler()\n",
        "    #train loop\n",
        "    for epoch in range(0, FLAGS['num_epochs']):\n",
        "\n",
        "        print('-'*27 + f'Epoch #{epoch+1} started' + '-'*27)\n",
        "\n",
        "        train_loss = train_one_epoch(\n",
        "            train_loader, \n",
        "            model, optimizer, \n",
        "            epoch, scheduler=None, \n",
        "            scaler=scaler, log=log\n",
        "        )\n",
        "        print(f'\\nAverage loss for epoch #{epoch+1} : {train_loss:.5f}')\n",
        "\n",
        "        val_output = val_one_epoch(val_loader, model)\n",
        "        val_loss, auc_score, roc_plot, hist, error_scaled, jsd, kld = val_output\n",
        "        scheduler.step(error_scaled)\n",
        "\n",
        "        #logs\n",
        "        if log:\n",
        "            neptune.log_metric('AUC/val', auc_score)\n",
        "            neptune.log_image('ROC/val', roc_plot)\n",
        "            neptune.log_metric('Loss/val', val_loss)\n",
        "            neptune.log_image('hist/val', hist)\n",
        "            neptune.log_metric('error_scaled/val', error_scaled)\n",
        "            neptune.log_metric('jsd/val', jsd)\n",
        "            neptune.log_metric('kld/val', kld)\n",
        "\n",
        "        #checkpoint+upload\n",
        "        save_upload(\n",
        "            model, optimizer, \n",
        "            auc_score, epoch,\n",
        "            fold, exp_name=FLAGS['exp_name']\n",
        "        )\n",
        "\n",
        "        print('-'*28 + f'Epoch #{epoch+1} ended' + '-'*28)\n",
        "    \n",
        "    if log:\n",
        "        neptune.stop()\n",
        "    \n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmLSHZZdS_1i",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHJtNpfOUdqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FLAGS = {}\n",
        "FLAGS['batch_size'] = 32\n",
        "FLAGS['num_workers'] = 4\n",
        "FLAGS['learning_rate'] = 1e-4\n",
        "FLAGS['num_epochs'] = 30\n",
        "FLAGS['weight_decay'] = 1e-3\n",
        "FLAGS['log_interval'] = 25\n",
        "FLAGS['img_size'] = IMG_SIZE\n",
        "FLAGS['loss'] = 'focal'\n",
        "FLAGS['optimizer'] = 'AdamW'\n",
        "FLAGS['exp_name'] = 'efficientnet_b0'\n",
        "FLAGS['fold'] = 0\n",
        "FLAGS['alpha'] = (41906 / (41906 + 3874)) #0.9153778942769768\n",
        "FLAGS['gamma'] = 2\n",
        "exp_description = '''\n",
        "efficientnet_b0 with base head,\n",
        "Extra Data\n",
        "No Sampler,\n",
        "changed aug,\n",
        "imsize 224\n",
        "'''\n",
        "\n",
        "fold_no = FLAGS['fold']\n",
        "X_train = df_train[df_train['fold'] != fold_no][[col for col in df_train.columns if col != 'target']]\n",
        "y_train = df_train[df_train['fold'] != fold_no][[col for col in df_train.columns if col == 'target']]\n",
        "X_val = df_train[df_train['fold'] == fold_no][[col for col in df_train.columns if col != 'target']]\n",
        "y_val = df_train[df_train['fold'] == fold_no][[col for col in df_train.columns if col == 'target']]\n",
        "data = X_train, y_train, X_val, y_val\n",
        "try:\n",
        "    trained_model = fit(data, FLAGS['fold'], log=True)\n",
        "except Exception as e:\n",
        "    if log:\n",
        "        neptune.stop()\n",
        "    print(e)\n",
        "except KeyboardInterrupt:\n",
        "    if log:\n",
        "       neptune.stop()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MPMH0blnHpK",
        "colab_type": "text"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TCqUC6orEQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# upload_blob('/content/siim-isic_Tf_efficientnet_b2_ns_10.ckpt', 'new_upload_e2_10.ckpt')\n",
        "# upload_blob('/content/siim-isic_Tf_efficientnet_b2_ns_8.ckpt', 'new_upload_e2_8.ckpt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBuDaNX9wP9-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#trained_model = ResNet34Mod()\n",
        "def run_test(model, test_images, ckpts):\n",
        "    agg_predictions = []\n",
        "    for ckpt in ckpts:\n",
        "        model.load_state_dict(torch.load(ckpt)['model_state_dict'])\n",
        "        model.to(device).eval()\n",
        "        test_dataset = MelanomaEvalDataset(test_images, labels=None, isEval=True, transform=get_valid_transforms())\n",
        "        test_data_loader = DataLoader(\n",
        "            test_dataset, batch_size=256, shuffle=False, \n",
        "            num_workers=4, pin_memory=True\n",
        "        )\n",
        "        predictions = []\n",
        "        with torch.no_grad():\n",
        "            for images in tqdm(test_data_loader, total=len(test_data_loader)):\n",
        "                images = images.to(device)\n",
        "                y_pred = torch.sigmoid(model(images.float()))\n",
        "                predictions.append(y_pred.cpu().numpy())\n",
        "        predictions = np.vstack(predictions)\n",
        "        plt.hist(predictions.T[0],bins=100)\n",
        "        plt.ylim((0,500))\n",
        "        plt.show()\n",
        "        agg_predictions.append(predictions)\n",
        "    #can be vectorized\n",
        "    #avg_predictions = agg_predictions[0]\n",
        "    #avg_predictions = 0\n",
        "    avg_predictions = 0\n",
        "    for predictions in agg_predictions:\n",
        "        #avg_predictions = np.multiply(avg_predictions, predictions).astype(np.float64)\n",
        "        avg_predictions = np.add(avg_predictions, predictions).astype(np.float64)\n",
        "    #avg_predictions = np.power(avg_predictions, (1/float(len(agg_predictions))))\n",
        "    avg_predictions = np.divide(avg_predictions, float(len(agg_predictions)))\n",
        "    print('average:')\n",
        "    plt.hist(avg_predictions.T[0], bins=100)\n",
        "    plt.ylim((0,500))\n",
        "    plt.show()\n",
        "    return avg_predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKkVq0VNc_VG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586,
          "referenced_widgets": [
            "482ba6e80e2c4b32ab8fd3d7ec5bf5cc",
            "3d1784873bc046b3921f1dd5f87fa28d",
            "43af6aa41a6842c5aa4164d57e4d4ac2",
            "8b7331be437149ef9aca79da23736d3f",
            "719eaaee6fac4a0bba11b5010ac90d11",
            "a83e96f806fb4971aad60264a7a4472f",
            "040196ee54ec4e70b617b4f76a15bde3",
            "765cd04af6c84804a3a72289df33127e"
          ]
        },
        "outputId": "61522e1b-57b1-4685-ec57-821950d28334"
      },
      "source": [
        "ckpts = [\n",
        "         '/content/efficientnet_b0_16.ckpt'\n",
        "        ]\n",
        "trained_model = EfficientNet('tf_efficientnet_b0_ns').to(device)\n",
        "avg_predictions = run_test(trained_model, df_test, ckpts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "482ba6e80e2c4b32ab8fd3d7ec5bf5cc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=43.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "average:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4egqmO9ChuY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "3abe63bd-8822-4208-95ab-a69a078154cb"
      },
      "source": [
        "pd.Series(avg_predictions.T[0]).describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    10982.000000\n",
              "mean         0.213347\n",
              "std          0.183402\n",
              "min          0.004614\n",
              "25%          0.052288\n",
              "50%          0.157710\n",
              "75%          0.344794\n",
              "max          0.925111\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_Gd9ZY9t6Os",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "84d09331-8496-4f4b-d84b-8a3826ae2b6b"
      },
      "source": [
        "pd.Series(np.where(avg_predictions>0.5,1,0).T[0]).value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    10084\n",
              "1      898\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIX7N7l3uCHs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_submission.target = avg_predictions.T[0].T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z1-VMhauVUM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_submission.to_csv('submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iuo-xqyru-zW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "33762be6-ce42-4b01-e0ac-0b54c0b2c9b4"
      },
      "source": [
        "pd.read_csv('submission.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ISIC_0052060</td>\n",
              "      <td>0.022483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ISIC_0052349</td>\n",
              "      <td>0.033522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ISIC_0058510</td>\n",
              "      <td>0.087246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ISIC_0073313</td>\n",
              "      <td>0.017514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ISIC_0073502</td>\n",
              "      <td>0.117519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10977</th>\n",
              "      <td>ISIC_9992485</td>\n",
              "      <td>0.080649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10978</th>\n",
              "      <td>ISIC_9996992</td>\n",
              "      <td>0.254249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10979</th>\n",
              "      <td>ISIC_9997917</td>\n",
              "      <td>0.473851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10980</th>\n",
              "      <td>ISIC_9998234</td>\n",
              "      <td>0.042875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10981</th>\n",
              "      <td>ISIC_9999302</td>\n",
              "      <td>0.432175</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10982 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         image_name    target\n",
              "0      ISIC_0052060  0.022483\n",
              "1      ISIC_0052349  0.033522\n",
              "2      ISIC_0058510  0.087246\n",
              "3      ISIC_0073313  0.017514\n",
              "4      ISIC_0073502  0.117519\n",
              "...             ...       ...\n",
              "10977  ISIC_9992485  0.080649\n",
              "10978  ISIC_9996992  0.254249\n",
              "10979  ISIC_9997917  0.473851\n",
              "10980  ISIC_9998234  0.042875\n",
              "10981  ISIC_9999302  0.432175\n",
              "\n",
              "[10982 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-vHUPPBuLHf",
        "colab_type": "text"
      },
      "source": [
        "## Submit to Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhtkQ_LVYp0V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "4b0ff279-7010-4ddf-9aeb-f5d8c6c226d3"
      },
      "source": [
        "!kaggle competitions submit -c siim-isic-melanoma-classification -f submission.csv -m 'focal test exp 115'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0% 0.00/351k [00:00<?, ?B/s]\r100% 351k/351k [00:00<00:00, 1.35MB/s]\n",
            "Successfully submitted to SIIM-ISIC Melanoma Classification"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1mV4vyTfv9l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        },
        "outputId": "45e378b3-d458-4045-fa08-1ee6a855479d"
      },
      "source": [
        "!kaggle competitions submissions siim-isic-melanoma-classification"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fileName             date                 description                                                                     status    publicScore  privateScore  \n",
            "-------------------  -------------------  ------------------------------------------------------------------------------  --------  -----------  ------------  \n",
            "submission.csv       2020-07-05 08:51:14  focal test exp 115                                                              complete  0.901        None          \n",
            "submission.csv       2020-07-05 04:17:45  focal test exp 112+114                                                          complete  0.903        None          \n",
            "submission.csv       2020-07-05 04:11:56  focal test exp 114                                                              complete  0.894        None          \n",
            "submission.csv       2020-07-04 18:41:39  focal test exp 112                                                              complete  0.903        None          \n",
            "submission.csv       2020-07-02 16:09:08  focal test exp 109                                                              complete  0.898        None          \n",
            "submission.csv       2020-07-02 10:07:01  focal test exp 108                                                              complete  0.900        None          \n",
            "submission.csv       2020-07-02 03:58:22  focal test exp 107                                                              complete  0.898        None          \n",
            "submission.csv       2020-07-01 18:53:10  bce test enetb0                                                                 complete  0.895        None          \n",
            "submission.csv       2020-06-30 10:01:03  EffNet_B0 focal CV0.94+ fold 0 epoch 19, 24                                     complete  0.769        None          \n",
            "submission.csv       2020-06-29 19:46:18  EffNet_B0 focal CV0.92 fold 0 epoch 20, 22                                      complete  0.708        None          \n",
            "submission.csv       2020-06-29 15:26:03  EffNet_B0 BCE Ohem CV0.908 fold 0 epoch 10                                      complete  0.739        None          \n",
            "submission.csv       2020-06-29 10:20:51  EffNet_B0 Corrected Balanced Focal Loss CV0.913 fold 0 epoch 20, 22 alpha 0.75  complete  0.762        None          \n",
            "submission.csv       2020-06-28 13:24:25  EffNet_B0 Corrected Balanced Focal Loss CV0.913 fold 0 epoch 18 alpha 0.75      complete  0.771        None          \n",
            "submission.csv       2020-06-28 09:44:33  EffNet_B0 Corrected Focal Loss CV0.913 fold 0 epoch 18 alpha 0.75               complete  0.689        None          \n",
            "submission.csv       2020-06-28 06:26:13  EffNet_B0 Corrected Focal Loss CV0.913 fold 0 epoch 14                          complete  0.629        None          \n",
            "submission.csv       2020-06-08 15:45:04  Exp_98_EffNetB2_8,10_avg                                                        complete  0.883        None          \n",
            "submission.csv       2020-06-08 11:39:22  Exp_96_EffNetB2_10,11_avg Focal                                                 complete  0.891        None          \n",
            "submission.csv       2020-06-08 11:28:43  Exp_96_EffNetB2_10,11,14_avg Focal                                              complete  0.885        None          \n",
            "submission.csv       2020-06-07 14:02:32  Exp_89_EffNetB2_3,4,6,9_geo-mean                                                complete  0.880        None          \n",
            "submission.csv       2020-06-07 12:55:53  Exp_89_EffNetB2_3,4,6,9_avg                                                     complete  0.883        None          \n",
            "submission.csv       2020-06-07 12:51:01  Exp_89_EffNetB2_19                                                              complete  0.865        None          \n",
            "submission.csv       2020-06-07 05:44:25  Tf_efficientnet_b2_ns_extradat_regularized_ckpt_5_6_9_avg                       complete  0.904        None          \n",
            "submission.csv       2020-06-07 05:32:45  Tf_efficientnet_b2_ns_extradat_regularized_ckpt_2_3_4_5_6_9_avg                 complete  0.907        None          \n",
            "submission.csv       2020-06-06 04:03:18  Tf_efficientnet_b3_Mod_extradat_regularized_ckpt_4_6_8_avg                      complete  0.903        None          \n",
            "submission.csv       2020-06-05 20:11:55  Tf_efficientnet_b1_Mod_extradat_regularized_ckpt_5                              complete  0.881        None          \n",
            "submission.csv       2020-06-05 20:05:08  Tf_efficientnet_b5_ns_Mod_extradat_regularized_avg_ckpt_5+7                     complete  0.887        None          \n",
            "submission.csv       2020-06-05 12:17:45  Tf_efficientnet_b1_Mod_regularized_focal_ckpt_4                                 complete  0.860        None          \n",
            "submission.csv       2020-06-05 11:04:07  Tf_efficientnet_b1_Mod_regularized_avg_ckpt_4+6                                 complete  0.904        None          \n",
            "submission.csv       2020-06-05 09:42:13  Tf_efficientnet_b1_Mod_avg_ckpt_4+6                                             complete  0.879        None          \n",
            "submission.csv       2020-06-03 11:23:37  eff0_eff1_avg                                                                   complete  0.889        None          \n",
            "submission.csv       2020-06-03 09:14:10  eff_res_avg                                                                     complete  0.898        None          \n",
            "submission.csv       2020-06-03 08:59:57  swsl_resnext101_32x4d_Mod_avg_ckpt_9+10                                         complete  0.887        None          \n",
            "submission.csv       2020-06-03 07:45:59  swsl_resnext101_32x4d_avg_ckpt_6+8+9                                            complete  0.872        None          \n",
            "submission.csv       2020-06-03 06:00:00  Tf_efficientnet_b0_Mod_avg_ckpt_6+8+9                                           complete  0.891        None          \n",
            "submission.csv       2020-06-02 15:46:05  Tf_efficientnet_b1_ckpt_9                                                       complete  0.875        None          \n",
            "submission.csv       2020-06-02 15:40:04  Tf_efficientnet_b1_ckpt_4                                                       complete  0.882        None          \n",
            "submission.csv       2020-06-01 19:40:10  resnet50mod_fc_1_drop_focal_strong_aug                                          complete  0.872        None          \n",
            "submission.csv       2020-06-01 17:31:28  resnet34mod_fc_1_drop_focal                                                     complete  0.840        None          \n",
            "submission.csv       2020-06-01 15:28:00  resnet34mod_focal_224                                                           complete  0.814        None          \n",
            "submission (11).csv  2020-06-01 07:52:55  ResNet34 Mod 55x Loss Weight                                                    complete  0.845        None          \n",
            "submission (10).csv  2020-06-01 06:00:09  None                                                                            complete  0.870        None          \n",
            "submission (8).csv   2020-05-31 18:22:40  ResNet34                                                                        complete  0.874        None          \n",
            "submission (8).csv   2020-05-31 17:20:54                                                                                  complete  0.801        None          \n",
            "submission (8).csv   2020-05-31 17:19:26                                                                                  error     None         None          \n",
            "submission (8).csv   2020-05-31 17:18:18  None                                                                            error     None         None          \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waAtcELef00L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}