{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TPU SIIM-ISIC Melanoma Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "L-_fb9AEa6Wq",
        "wFoIORVvcWn5",
        "6MPMH0blnHpK",
        "Z-vHUPPBuLHf"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPZP62zqDYH3uFbRiqrdk1z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utsavnandi/Kaggle-SIIM-ISIC-Melanoma-Classification/blob/master/TPU_SIIM_ISIC_Melanoma_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-_fb9AEa6Wq",
        "colab_type": "text"
      },
      "source": [
        "## One-time\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tBeE-hte5Tl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import os\n",
        "# assert os.environ['COLAB_TPU_ADDR']\n",
        "# VERSION = \"nightly\"  #@param [\"1.5\" , \"20200516\", \"nightly\"]\n",
        "# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "# !python pytorch-xla-env-setup.py --version $VERSION # --apt-packages libomp5 libopenblas-dev\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymGpJBznaEZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%time\n",
        "# !pip uninstall kaggle -y\n",
        "# !pip install kaggle==1.5.6 -q\n",
        "# !pip install -U catalyst -q\n",
        "# !pip install -U git+https://github.com/albu/albumentations -q\n",
        "# !pip install -U git+https://github.com/rwightman/pytorch-image-models -q\n",
        "# !pip install -U git+https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer -q\n",
        "# !pip install -U git+https://github.com/PyTorchLightning/pytorch-lightning -q\n",
        "# !pip install neptune-client -q\n",
        "# !mkdir ~/.kaggle/\n",
        "# !cp ./kaggle.json  ~/.kaggle/kaggle.json\n",
        "# !chmod 600 ~/.kaggle/kaggle.json\n",
        "# !kaggle datasets download -d shonenkov/melanoma-merged-external-data-512x512-jpeg\n",
        "# !unzip melanoma-merged-external-data-512x512-jpeg.zip -d ./data/\n",
        "# !rm melanoma-merged-external-data-512x512-jpeg.zip\n",
        "# !kaggle competitions download siim-isic-melanoma-classification -f sample_submission.csv\n",
        "# !kaggle competitions download siim-isic-melanoma-classification -f test.csv\n",
        "# !kaggle competitions download siim-isic-melanoma-classification -f train.csv\n",
        "# !unzip train.csv -d ./data/\n",
        "# !mv ./test.csv ./data/\n",
        "# !mv ./sample_submission.csv ./data/\n",
        "# !rm train.csv.zip\n",
        "# !mkdir ./logs/"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFoIORVvcWn5",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cB029jhCcJFF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "c30fbf7c-0b35-45c6-f673-9802c2619acb"
      },
      "source": [
        "import os\n",
        "#os.environ['XLA_USE_BF16'] = \"0\"\n",
        "import gc\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "from getpass import getpass\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "#import seaborn as sns\n",
        "##from tqdm.notebook import tqdm\n",
        "from google.colab import auth\n",
        "from google.cloud import storage\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torchvision import transforms, models\n",
        "\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "#import torch_xla.utils.serialization as xser\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.utils.utils as xu\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "# from pytorch_lightning.core.lightning import LightningModule\n",
        "# from pytorch_lightning import Trainer\n",
        "# #from pytorch_lightning.loggers import TensorBoardLogger\n",
        "# from pytorch_lightning.metrics.functional import auroc\n",
        "\n",
        "from ranger import Ranger\n",
        "from catalyst.data.sampler import DistributedSamplerWrapper, BalanceClassSampler\n",
        "import timm\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "seed_everything(43)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning:\n",
            "\n",
            "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-XASBvOcVwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_DIR = '/content/data/'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uG9Xunfy3N3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(DATA_DIR+'folds.csv')\n",
        "df_test = pd.read_csv(DATA_DIR+'test.csv').rename(columns={'image_name':'image_id'})\n",
        "sample_submission = pd.read_csv(DATA_DIR+'sample_submission.csv')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIKzfKD2zA9A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "bd55d464-a91d-4573-cd6f-7fb30a620249"
      },
      "source": [
        "df_train['fold'].value_counts()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    12219\n",
              "1    12106\n",
              "2    12072\n",
              "4    12048\n",
              "3    12042\n",
              "Name: fold, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOuLmekTzlM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fold_no = 1\n",
        "X_train = df_train[df_train['fold'] != fold_no][[col for col in df_train.columns if col != 'target']]\n",
        "y_train = df_train[df_train['fold'] != fold_no][[col for col in df_train.columns if col == 'target']]\n",
        "X_val = df_train[df_train['fold'] == fold_no][[col for col in df_train.columns if col != 'target']]\n",
        "y_val = df_train[df_train['fold'] == fold_no][[col for col in df_train.columns if col == 'target']]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yovLpNhMcvnW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "dbae7a4f-4388-4ad0-ba54-47079b4fde4b"
      },
      "source": [
        "print('X_train', X_train.shape)\n",
        "print('y_train', y_train.shape)\n",
        "print('X_val', X_val.shape)\n",
        "print('y_val', y_val.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train (48381, 8)\n",
            "y_train (48381, 1)\n",
            "X_val (12106, 8)\n",
            "y_val (12106, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWae70vKk9dw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "caa7c747-8994-49a2-843d-4bce8ca46c4a"
      },
      "source": [
        "print('Train target distribution: ')\n",
        "print(y_train['target'].value_counts())\n",
        "print('Val target distribution: ')\n",
        "print(y_val['target'].value_counts())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train target distribution: \n",
            "0    43997\n",
            "1     4384\n",
            "Name: target, dtype: int64\n",
            "Val target distribution: \n",
            "0    11011\n",
            "1     1095\n",
            "Name: target, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwWrFw5C-R5J",
        "colab_type": "text"
      },
      "source": [
        "##  Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wK-NTFy-PwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MelanomaDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, df, labels, istrain=False, transforms=None):\n",
        "        super().__init__()\n",
        "        self.image_id = df['image_id'].values\n",
        "        self.transforms = transforms\n",
        "        self.labels = labels.values\n",
        "        self.neg_indices = np.where(self.labels==0)[0]\n",
        "        self.pos_indices = np.where(self.labels==1)[0]\n",
        "        self.istrain = istrain\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_id)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if torch.is_tensor(index):\n",
        "            index = index.tolist()\n",
        "        \n",
        "        image, target = self.load_image(index)\n",
        "\n",
        "        if self.transforms:\n",
        "            image = self.transforms(image=image)['image']\n",
        "\n",
        "        #if self.istrain == False:\n",
        "        #      return image, target\n",
        "\n",
        "        #rng = np.random.choice([1, 2])\n",
        "        #if rng == 1:\n",
        "        #    image, target = self.cutmix(image, target, alpha=1)\n",
        "        #elif rng == 2:\n",
        "        #    image, target = self.mixup(image, target, alpha=1) \n",
        "        ##else:\n",
        "        ##    target = [target]\n",
        "\n",
        "        return image, target#, rng\n",
        "    \n",
        "    def load_image(self, index):\n",
        "        if torch.is_tensor(index):\n",
        "            index = index.tolist()\n",
        "        image_name = DATA_DIR + f'512x512-dataset-melanoma/512x512-dataset-melanoma/{self.image_id[index]}.jpg'\n",
        "        image = cv2.imread(image_name, cv2.IMREAD_COLOR).astype(np.uint8)\n",
        "        target = self.labels[index].astype(np.float32)\n",
        "        return image, target\n",
        "\n",
        "    def cutmix(self, data, target, alpha=1):\n",
        "        rand_index = self.get_rand_index()\n",
        "        random_image, random_target = self.load_image(rand_index)\n",
        "        if self.transforms:\n",
        "            random_image = self.transforms(image=random_image)['image']\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "        bbx1, bby1, bbx2, bby2 = self.rand_bbox(data.size(), lam)\n",
        "        data[:, bbx1:bbx2, bby1:bby2] = random_image[ :, bbx1:bbx2, bby1:bby2]\n",
        "        # adjust lambda to exactly match pixel ratio\n",
        "        lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (data.size()[-1] * data.size()[-2]))\n",
        "        targets = [target, random_target, lam]\n",
        "        return data, targets\n",
        "\n",
        "    def mixup(self, data, target, alpha=1):\n",
        "        rand_index = self.get_rand_index()\n",
        "        random_image, random_target = self.load_image(rand_index)\n",
        "        if self.transforms:\n",
        "            random_image = self.transforms(image=random_image)['image']\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "        data = data * lam + random_image * (1 - lam)\n",
        "        targets = [target, random_target, lam]\n",
        "        return data, targets\n",
        "\n",
        "    def get_rand_index(self):\n",
        "        if random.random()>0.5:\n",
        "            rand_index = np.random.choice(self.pos_indices)\n",
        "        else:\n",
        "            rand_index = np.random.choice(self.neg_indices)\n",
        "        return rand_index\n",
        "\n",
        "    def rand_bbox(self, size, lam):\n",
        "        W = size[1]\n",
        "        H = size[2]\n",
        "        cut_rat = np.sqrt(1. - lam)\n",
        "        cut_w = np.int(W * cut_rat)\n",
        "        cut_h = np.int(H * cut_rat)\n",
        "        cx = np.random.randint(W)\n",
        "        cy = np.random.randint(H)\n",
        "        bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "        bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "        bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "        bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "        return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "def get_datasets():\n",
        "    datasets = {}\n",
        "    datasets['train'] = MelanomaDataset(\n",
        "        X_train, y_train, istrain=True, transforms=get_train_transforms()\n",
        "    )\n",
        "    datasets['valid'] = MelanomaDataset(\n",
        "        X_val, y_val, istrain=False, transforms=get_valid_transforms()\n",
        "    )\n",
        "    return datasets\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFL68tLin0Ky",
        "colab_type": "text"
      },
      "source": [
        "## Augmentations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPWu-IIliVVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%writefile augmentations.txt\n",
        "# Reference IMG_SIZE\n",
        "# B0 - 224\n",
        "# B1 - 240\n",
        "# B2 - 260\n",
        "# B3 - 300\n",
        "# B4 - 380\n",
        "# B5 - 456\n",
        "# B6 - 520\n",
        "# B7 - 600\n",
        "# B8 - 672\n",
        "# L2 NS - 475\n",
        "# L2 - 800\n",
        "# Transforms\n",
        "IMG_SIZE = 224\n",
        "mean = (0.485, 0.456, 0.406)\n",
        "std = (0.229, 0.224, 0.225)\n",
        "\n",
        "def get_train_transforms(p=1.0):\n",
        "    return A.Compose([\n",
        "        A.Resize(IMG_SIZE, IMG_SIZE, interpolation=2, always_apply=True, p=1),\n",
        "        A.RandomResizedCrop(\n",
        "            IMG_SIZE, IMG_SIZE, scale=(0.8, 1.2), interpolation=2, p=0.33\n",
        "        ),\n",
        "        A.Flip(p=0.33),\n",
        "        A.Transpose(p=0.33),\n",
        "        #A.OneOf([\n",
        "        #    A.MedianBlur(blur_limit=3, p=0.5),\n",
        "        #    A.Blur(blur_limit=3, p=0.5),\n",
        "        #], p=0.5),\n",
        "        A.ShiftScaleRotate(\n",
        "            interpolation=2,\n",
        "            shift_limit=0.0625, scale_limit=0.15, \n",
        "            rotate_limit=15, p=0.3\n",
        "        ),\n",
        "        #A.OneOf([\n",
        "        #    A.OpticalDistortion(p=0.3),\n",
        "        #    A.GridDistortion(p=.1),\n",
        "        #    A.IAAPiecewiseAffine(p=0.3),\n",
        "        #], p=0.5),\n",
        "        #A.OneOf([\n",
        "        #    A.CLAHE(clip_limit=2),\n",
        "        #    A.IAASharpen(),\n",
        "        #    A.IAAEmboss(),\n",
        "        #    A.RandomBrightnessContrast(),            \n",
        "        #], p=0.5),\n",
        "        A.HueSaturationValue(\n",
        "            hue_shift_limit=20, sat_shift_limit=30, \n",
        "            val_shift_limit=20, p=0.33\n",
        "        ),\n",
        "        A.MultiplicativeNoise(\n",
        "            multiplier=[0.75, 1.25], \n",
        "            elementwise=True, p=0.33\n",
        "        ),\n",
        "        A.Normalize(mean, std, max_pixel_value=255.0, always_apply=True),\n",
        "        ToTensorV2(p=1.0),\n",
        "    ], p=p)\n",
        "\n",
        "def get_valid_transforms():\n",
        "    return A.Compose([\n",
        "        A.Resize(IMG_SIZE, IMG_SIZE, interpolation=2, always_apply=True, p=1),\n",
        "        A.Normalize(mean, std, max_pixel_value=255.0, always_apply=True),\n",
        "        ToTensorV2(p=1.0),\n",
        "    ])\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9m9lx4ewYzk",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfPBXkoYyPAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResNet18(nn.Module): \n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = models.resnet18(pretrained=True)\n",
        "        in_features = self.model.fc.in_features\n",
        "        self.model.fc = nn.Linear(in_features, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "class Tf_efficientnet_b0_ns(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model('tf_efficientnet_b0_ns', pretrained=True)\n",
        "        in_features = self.model.classifier.in_features\n",
        "        self.model.classifier = nn.Linear(in_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class Tf_efficientnet_b3_ns(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model('tf_efficientnet_b3_ns', pretrained=True)\n",
        "        in_features = self.model.classifier.in_features\n",
        "        self.model.classifier = nn.Linear(in_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class Tf_efficientnet_b3_ns_Mod(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model('tf_efficientnet_b3_ns', pretrained=True)\n",
        "        in_features = self.model.classifier.in_features\n",
        "        self.model.classifier = nn.Linear(in_features, int(in_features/2))\n",
        "        self.bn_1 = nn.BatchNorm1d(int(in_features/2))\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.drop_1 = nn.Dropout(0.2)\n",
        "        self.fc_2 = nn.Linear(int(in_features/2), 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x = self.bn_1(x)\n",
        "        x = self.relu_1(x)\n",
        "        x = self.drop_1(x)\n",
        "        x = self.fc_2(x)\n",
        "        return x\n",
        "\n",
        "class Tf_efficientnet_b3_ns_Mod_v2(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model('tf_efficientnet_b3_ns', pretrained=True)\n",
        "        in_features = self.model.classifier.in_features\n",
        "        self.model.classifier = nn.Linear(in_features, in_features)\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.bn_1 = nn.BatchNorm1d(in_features)\n",
        "        self.fc_1 = nn.Linear(in_features, int(in_features/2))\n",
        "        self.bn_2 = nn.BatchNorm1d(int(in_features/2))\n",
        "        self.fc_2 = nn.Linear(int(in_features/2), 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x = self.relu_1(x)\n",
        "        x = self.bn_1(x)\n",
        "        x = self.fc_1(x)\n",
        "        x = self.bn_2(x)\n",
        "        x = self.fc_2(x)\n",
        "        return x\n",
        "\n",
        "class Gluon_seresnext101_32x4d(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model('gluon_seresnext101_32x4d', pretrained=True)\n",
        "        in_features = self.model.classifier.in_features\n",
        "        self.model.classifier = nn.Linear(in_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class Gluon_seresnext50_32x4d(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model('gluon_seresnext50_32x4d', pretrained=True)\n",
        "        in_features = self.model.fc.in_features\n",
        "        self.model.fc = nn.Linear(in_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndmsl1XUrWFp",
        "colab_type": "text"
      },
      "source": [
        "## Custom Losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IICyzNqJrZnl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2, logits=True, reduce=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.logits = logits\n",
        "        self.reduce = reduce\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        if self.logits:\n",
        "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
        "        else:\n",
        "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-BCE_loss)\n",
        "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
        "\n",
        "        if self.reduce:\n",
        "            return torch.mean(F_loss)\n",
        "        else:\n",
        "            return F_loss\n",
        "\n",
        "def ohem_loss(cls_pred, cls_target, rate):\n",
        "    ohem_cls_loss = F.binary_cross_entropy_with_logits(cls_pred, cls_target, reduction='none')\n",
        "    batch_size = cls_pred.size(0)\n",
        "    sorted_ohem_loss, idx = torch.sort(ohem_cls_loss, descending=True)\n",
        "    keep_num = min(sorted_ohem_loss.size()[0], int(batch_size*rate))\n",
        "    if keep_num < sorted_ohem_loss.size()[0]:\n",
        "        keep_idx_cuda = idx[:keep_num]\n",
        "        ohem_cls_loss = ohem_cls_loss[keep_idx_cuda]\n",
        "    cls_loss = ohem_cls_loss.sum() / keep_num\n",
        "    return cls_loss\n",
        "\n",
        "def criterion(y_pred, y_true):\n",
        "    return nn.BCEWithLogitsLoss()(y_pred, y_true)\n",
        "\n",
        "def focal_criterion(y_pred, y_true):\n",
        "    return FocalLoss(alpha=(43997/4384))(y_pred, y_true)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruo8SHLGnZJU",
        "colab_type": "text"
      },
      "source": [
        "## Train script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmPxF8kEnTrY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class RocAucMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.y_true = np.array([0,1])\n",
        "        self.y_pred = np.array([0.5,0.5])\n",
        "        self.score = 0\n",
        "\n",
        "    def update(self, y_true, y_pred):\n",
        "        y_true = y_true.cpu().numpy()\n",
        "        y_pred = torch.flatten(torch.sigmoid(y_pred)).data.cpu().numpy()\n",
        "        self.y_true = np.append(self.y_true, y_true)\n",
        "        self.y_pred = np.append(self.y_pred, y_pred)\n",
        "        self.score = roc_auc_score(self.y_true, self.y_pred)\n",
        "\n",
        "    @property\n",
        "    def avg(self):\n",
        "        return self.score\n",
        "        "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TggTDLcOxUT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8702ae86-08e0-4ffd-9b6e-90c0c7a7df39"
      },
      "source": [
        "\n",
        "exp_description = '''\n",
        "Tf_efficientnet_b0_ns with base head,\n",
        "Extra Data\n",
        "Focal loss\n",
        "mixup only\n",
        "RandomSampler,\n",
        "changed aug,\n",
        "imsize 300\n",
        "'''\n",
        "\n",
        "#SERIAL_EXEC = xmp.MpSerialExecutor()\n",
        "#WRAPPED_MODEL = xmp.MpModelWrapper(Tf_efficientnet_b0_ns())\n",
        "WRAPPED_MODEL = Tf_efficientnet_b0_ns()\n",
        "\n",
        "def train_model():\n",
        "\n",
        "    best_score = 0.0\n",
        "    datasets = SERIAL_EXEC.run(get_datasets)\n",
        "    #datasets = get_datasets()\n",
        "\n",
        "    #sampler\n",
        "    #labels_vcount = y_train['target'].value_counts()\n",
        "    #class_counts = [labels_vcount[0].astype(np.float32), labels_vcount[1].astype(np.float32)]\n",
        "    #num_samples = sum(class_counts)\n",
        "    #class_weights = [num_samples/class_counts[i] for i in range(len(class_counts))]\n",
        "    #weights = [class_weights[y_train['target'].values[i]] for i in range(int(num_samples))]\n",
        "    #wrsampler = WeightedRandomSampler(torch.DoubleTensor(weights), int(num_samples))\n",
        "    #BalanceClassSampler(labels=y_train['target'].values, mode=\"downsampling\"),\n",
        "    \n",
        "    #train_sampler = DistributedSamplerWrapper(\n",
        "    #    sampler=wrsampler,\n",
        "    #    num_replicas=xm.xrt_world_size(),\n",
        "    #    rank=xm.get_ordinal(),\n",
        "    #    shuffle=True\n",
        "    #)\n",
        "    \n",
        "    train_sampler =  DistributedSampler(\n",
        "        datasets['train'],\n",
        "        num_replicas=xm.xrt_world_size(),\n",
        "        rank=xm.get_ordinal(),\n",
        "        shuffle=True\n",
        "    )\n",
        "    validation_sampler = DistributedSampler(\n",
        "        datasets['valid'],\n",
        "        num_replicas=xm.xrt_world_size(),\n",
        "        rank=xm.get_ordinal(),\n",
        "        shuffle=False\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        datasets['train'],\n",
        "        batch_size=FLAGS['batch_size'], \n",
        "        num_workers=FLAGS['num_workers'],\n",
        "        sampler=train_sampler,\n",
        "        drop_last=True,\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        datasets['valid'],\n",
        "        batch_size=FLAGS['batch_size'],\n",
        "        num_workers=FLAGS['num_workers'],\n",
        "        sampler=validation_sampler,\n",
        "        drop_last=True\n",
        "    )\n",
        "    \n",
        "    device = xm.xla_device()\n",
        "    model = WRAPPED_MODEL.to(device)\n",
        "    #optimizer = Ranger(\n",
        "    #    model.parameters(),\n",
        "    #    lr=FLAGS['learning_rate'] * xm.xrt_world_size(), \n",
        "    #    alpha=0.5, k=6, N_sma_threshhold=5,\n",
        "    #    weight_decay=FLAGS['weight_decay']\n",
        "    #)\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(), \n",
        "        lr=FLAGS['learning_rate'] * xm.xrt_world_size(), \n",
        "        weight_decay=FLAGS['weight_decay']\n",
        "    )\n",
        "\n",
        "    criterion = focal_criterion\n",
        "    #criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def train_one_epoch(loader):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        max_idx = 0.0\n",
        "        for idx, (images, targets) in enumerate(loader):\n",
        "            images = images.to(device)\n",
        "            targets = targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(images.float())\n",
        "            loss = criterion(y_pred, targets)\n",
        "            running_loss += float(loss)\n",
        "            loss.backward()\n",
        "            xm.optimizer_step(optimizer)\n",
        "            max_idx = float(idx)\n",
        "            if idx % FLAGS['log_steps'] == 0 and idx !=0:\n",
        "                #print('[xla:{}]({}) Loss={:.5f} Time={}'.format(\n",
        "                #    xm.get_ordinal(), idx, loss.item(), time.asctime(time.localtime())), flush=True)\n",
        "                xm.master_print('({}) Loss={:.5f} Time={}'.format(\n",
        "                    idx, loss.item(), time.asctime(time.localtime())))\n",
        "        #return running_loss/(max_idx+1)\n",
        "\n",
        "    def val_one_epoch(loader):\n",
        "        model.eval()\n",
        "        running_loss = 0.0\n",
        "        total_samples = 0\n",
        "        max_idx = 0.0\n",
        "        #roc_auc_scores = RocAucMeter()\n",
        "        with torch.no_grad():\n",
        "            for idx, (images, targets) in enumerate(loader):\n",
        "                images = images.to(device)\n",
        "                targets = targets.to(device)\n",
        "                y_pred = model(images.float())\n",
        "                loss = criterion(y_pred, targets)\n",
        "                running_loss += float(loss)\n",
        "                max_idx = float(idx)\n",
        "                total_samples += images.size()[0]\n",
        "                #roc_auc_scores.update(targets, y_pred)\n",
        "\n",
        "        #return running_loss/(max_idx+1), roc_auc_scores\n",
        "\n",
        "    for epoch in range(0, FLAGS['num_epochs']):\n",
        "\n",
        "        xm.master_print('-'*27 + f'Epoch #{epoch+1} started' + '-'*27)\n",
        "        xm.master_print(f'epoch start time: {time.asctime(time.localtime())}')\n",
        "\n",
        "        para_loader = pl.ParallelLoader(train_loader, [device])\n",
        "        train_one_epoch(para_loader.per_device_loader(device)) # train_loss = \n",
        "        xm.master_print(\"finished training epoch {}\".format(epoch+1))\n",
        "        #xm.master_print(f'average loss for epoch #{epoch+1} : {train_loss}')\n",
        "        #gc.collect()\n",
        "        \n",
        "        if (epoch+1) % 1 == 0:\n",
        "            para_loader = pl.ParallelLoader(val_loader, [device])\n",
        "            val_one_epoch(para_loader.per_device_loader(device)) # val_loss, auc_score = \n",
        "            xm.master_print(\"finished validating epoch {}\".format(epoch+1))\n",
        "            #xm.master_print(f'roc_auc_score: {auc_score.avg:.5f}')\n",
        "            #xm.master_print(f'average loss for val epoch: {val_loss}')\n",
        "            #gc.collect()\n",
        "            #if auc_score.avg > best_score:\n",
        "            #    best_score = auc_score.avg\n",
        "                #xm.master_print(f'new best score: {best_score}')\n",
        "                #if epoch > 3:\n",
        "                \n",
        "            xm.save(model.state_dict(), f'./model_{epoch+1}.pth')\n",
        "                # xser.save(\n",
        "                #     model.state_dict(), \n",
        "                #     f'./model_epoch_{epoch+1}.pth', \n",
        "                #     master_only=True\n",
        "                # )\n",
        "                #xser.save(model.state_dict(), f\"model.bin\", master_only=True)\n",
        "                #gc.collect()\n",
        "                #model.load_state_dict(xser.load(f\"./model_epoch_{epoch+1}.pth\"))\n",
        "            xm.master_print(f'saved model...')\n",
        "        \n",
        "        xm.master_print(f'epoch end time: {time.asctime(time.localtime())}')\n",
        "        xm.master_print('-'*28 + f'Epoch #{epoch+1} ended' + '-'*28)\n",
        "    \n",
        "    #return best_score\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b0_ns-c0e6a31c.pth\" to /root/.cache/torch/hub/checkpoints/tf_efficientnet_b0_ns-c0e6a31c.pth\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmLSHZZdS_1i",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHJtNpfOUdqD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6976c81a-eefe-49b1-f196-c92ff85fbc5d"
      },
      "source": [
        "FLAGS = {}\n",
        "FLAGS['batch_size'] = 32\n",
        "FLAGS['num_workers'] = 0\n",
        "FLAGS['learning_rate'] = 2e-4\n",
        "FLAGS['num_epochs'] = 10\n",
        "FLAGS['weight_decay'] = 1e-4\n",
        "FLAGS['log_steps'] = 40\n",
        "FLAGS['img_size'] = IMG_SIZE\n",
        "FLAGS['loss'] = 'BCE'\n",
        "FLAGS['optimizer'] = 'AdamW'\n",
        "FLAGS['exp_name'] = 'Tf_efficientnet_b0_ns'\n",
        "FLAGS['fold'] = 1\n",
        "FLAGS['num_cores'] = 8\n",
        "\n",
        "def _mp_fn(rank, flags):\n",
        "  global FLAGS\n",
        "  #global X_train, y_train, X_val, y_val\n",
        "  FLAGS = flags\n",
        "  torch.set_default_tensor_type('torch.FloatTensor')\n",
        "  #best_score = \n",
        "  train_model()\n",
        "\n",
        "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8,\n",
        "          start_method='fork')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------Epoch #1 started---------------------------\n",
            "epoch start time: Wed Jun 24 05:32:05 2020\n",
            "(40) Loss=0.50114 Time=Wed Jun 24 05:33:51 2020\n",
            "(80) Loss=1.07857 Time=Wed Jun 24 05:34:43 2020\n",
            "(120) Loss=0.54557 Time=Wed Jun 24 05:35:38 2020\n",
            "(160) Loss=1.00449 Time=Wed Jun 24 05:36:28 2020\n",
            "finished training epoch 1\n",
            "finished validating epoch 1\n",
            "saved model...\n",
            "epoch end time: Wed Jun 24 05:38:04 2020\n",
            "----------------------------Epoch #1 ended----------------------------\n",
            "---------------------------Epoch #2 started---------------------------\n",
            "epoch start time: Wed Jun 24 05:38:04 2020\n",
            "(40) Loss=0.42083 Time=Wed Jun 24 05:39:07 2020\n",
            "(80) Loss=0.65260 Time=Wed Jun 24 05:39:56 2020\n",
            "(120) Loss=0.41624 Time=Wed Jun 24 05:40:48 2020\n",
            "(160) Loss=0.74269 Time=Wed Jun 24 05:41:41 2020\n",
            "finished training epoch 2\n",
            "finished validating epoch 2\n",
            "saved model...\n",
            "epoch end time: Wed Jun 24 05:43:16 2020\n",
            "----------------------------Epoch #2 ended----------------------------\n",
            "---------------------------Epoch #3 started---------------------------\n",
            "epoch start time: Wed Jun 24 05:43:16 2020\n",
            "(40) Loss=0.47397 Time=Wed Jun 24 05:44:16 2020\n",
            "(80) Loss=0.84084 Time=Wed Jun 24 05:45:08 2020\n",
            "(120) Loss=0.56335 Time=Wed Jun 24 05:46:01 2020\n",
            "(160) Loss=0.96253 Time=Wed Jun 24 05:46:53 2020\n",
            "finished training epoch 3\n",
            "finished validating epoch 3\n",
            "saved model...\n",
            "epoch end time: Wed Jun 24 05:48:28 2020\n",
            "----------------------------Epoch #3 ended----------------------------\n",
            "---------------------------Epoch #4 started---------------------------\n",
            "epoch start time: Wed Jun 24 05:48:29 2020\n",
            "(40) Loss=0.31013 Time=Wed Jun 24 05:49:31 2020\n",
            "(80) Loss=0.74507 Time=Wed Jun 24 05:50:21 2020\n",
            "(120) Loss=0.39372 Time=Wed Jun 24 05:51:13 2020\n",
            "(160) Loss=0.95314 Time=Wed Jun 24 05:52:05 2020\n",
            "finished training epoch 4\n",
            "finished validating epoch 4\n",
            "saved model...\n",
            "epoch end time: Wed Jun 24 05:53:40 2020\n",
            "----------------------------Epoch #4 ended----------------------------\n",
            "---------------------------Epoch #5 started---------------------------\n",
            "epoch start time: Wed Jun 24 05:53:40 2020\n",
            "(40) Loss=0.38456 Time=Wed Jun 24 05:54:41 2020\n",
            "(80) Loss=0.60307 Time=Wed Jun 24 05:55:32 2020\n",
            "(120) Loss=0.52250 Time=Wed Jun 24 05:56:25 2020\n",
            "(160) Loss=0.79876 Time=Wed Jun 24 05:57:17 2020\n",
            "finished training epoch 5\n",
            "finished validating epoch 5\n",
            "saved model...\n",
            "epoch end time: Wed Jun 24 05:58:53 2020\n",
            "----------------------------Epoch #5 ended----------------------------\n",
            "---------------------------Epoch #6 started---------------------------\n",
            "epoch start time: Wed Jun 24 05:58:53 2020\n",
            "(40) Loss=0.63930 Time=Wed Jun 24 05:59:52 2020\n",
            "(80) Loss=0.68812 Time=Wed Jun 24 06:00:44 2020\n",
            "(120) Loss=0.40068 Time=Wed Jun 24 06:01:37 2020\n",
            "(160) Loss=0.83134 Time=Wed Jun 24 06:02:29 2020\n",
            "finished training epoch 6\n",
            "finished validating epoch 6\n",
            "saved model...\n",
            "epoch end time: Wed Jun 24 06:04:05 2020\n",
            "----------------------------Epoch #6 ended----------------------------\n",
            "---------------------------Epoch #7 started---------------------------\n",
            "epoch start time: Wed Jun 24 06:04:05 2020\n",
            "(40) Loss=0.35197 Time=Wed Jun 24 06:05:06 2020\n",
            "(80) Loss=0.77805 Time=Wed Jun 24 06:06:00 2020\n",
            "(120) Loss=0.38370 Time=Wed Jun 24 06:06:49 2020\n",
            "(160) Loss=0.42575 Time=Wed Jun 24 06:07:42 2020\n",
            "finished training epoch 7\n",
            "finished validating epoch 7\n",
            "saved model...\n",
            "epoch end time: Wed Jun 24 06:09:17 2020\n",
            "----------------------------Epoch #7 ended----------------------------\n",
            "---------------------------Epoch #8 started---------------------------\n",
            "epoch start time: Wed Jun 24 06:09:17 2020\n",
            "(40) Loss=0.21830 Time=Wed Jun 24 06:10:16 2020\n",
            "(80) Loss=0.35622 Time=Wed Jun 24 06:11:09 2020\n",
            "(120) Loss=0.40339 Time=Wed Jun 24 06:12:02 2020\n",
            "(160) Loss=0.50657 Time=Wed Jun 24 06:12:53 2020\n",
            "finished training epoch 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKPjBiClvOzW",
        "colab_type": "text"
      },
      "source": [
        "## Lightning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYYXNPUHvT_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FLAGS = {}\n",
        "# FLAGS['batch_size'] = 32\n",
        "# FLAGS['num_workers'] = 8\n",
        "# FLAGS['learning_rate'] = 3e-4\n",
        "# FLAGS['num_epochs'] = 2\n",
        "# FLAGS['weight_decay'] = 1e-4\n",
        "# FLAGS['img_size'] = IMG_SIZE\n",
        "# FLAGS['loss'] = 'Focal'\n",
        "# FLAGS['optimizer'] = 'AdamW'\n",
        "# FLAGS['exp_name'] = 'Tf_efficientnet_b0_ns'\n",
        "# FLAGS['fold'] = 1\n",
        "# FLAGS['num_cores'] = 8\n",
        "\n",
        "# class LitModel(LightningModule):\n",
        "\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.model = timm.create_model('tf_efficientnet_b0_ns', pretrained=True)\n",
        "#         in_features = self.model.classifier.in_features\n",
        "#         self.model.classifier = nn.Linear(in_features, 1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.model(x)\n",
        "\n",
        "#     def training_step(self, batch, batch_idx):\n",
        "#         image, target = batch\n",
        "#         y_pred = self(image.float())\n",
        "#         loss = focal_criterion(y_pred, target)\n",
        "#         tensorboard_logs = {'train_loss': loss}\n",
        "#         return {'loss': loss, 'log': tensorboard_logs}\n",
        "\n",
        "#     def validation_step(self, batch, batch_idx):\n",
        "#         image, target = batch\n",
        "#         y_pred = self(image.float())\n",
        "#         loss = focal_criterion(y_pred, target)\n",
        "#         score = auroc(y_pred.item().long(), target.item().long())\n",
        "#         return {'val_loss': loss, 'score': score}\n",
        "    \n",
        "#     def validation_epoch_end(self, outputs):\n",
        "#         avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
        "#         avg_score = torch.stack([x['score'] for x in outputs]).mean()\n",
        "#         tensorboard_logs = {'val_loss': avg_loss, 'avg_score': avg_score}\n",
        "#         return {'val_loss': avg_loss, 'avg_score': avg_score, 'log': tensorboard_logs}\n",
        "\n",
        "#     def configure_optimizers(self):\n",
        "#         return torch.optim.AdamW(\n",
        "#             self.parameters(), lr=FLAGS['learning_rate'],\n",
        "#             weight_decay=FLAGS['weight_decay']\n",
        "#         )\n",
        "\n",
        "#     def train_dataloader(self):\n",
        "#         train_ds = MelanomaDataset(\n",
        "#             X_train, y_train, istrain=True, transforms=get_train_transforms()\n",
        "#         )\n",
        "#         train_loader = DataLoader(\n",
        "#             train_ds,\n",
        "#             batch_size=FLAGS['batch_size'], \n",
        "#             num_workers=FLAGS['num_workers'],\n",
        "#             shuffle=True\n",
        "#         )\n",
        "#         return train_loader\n",
        "    \n",
        "#     def val_dataloader(self):\n",
        "#         val_ds = MelanomaDataset(\n",
        "#             X_val, y_val, istrain=False, transforms=get_valid_transforms()\n",
        "#         )\n",
        "#         val_loader = DataLoader(\n",
        "#             val_ds,\n",
        "#             batch_size=FLAGS['batch_size'], \n",
        "#             num_workers=FLAGS['num_workers'],\n",
        "#             shuffle=False,\n",
        "#         )\n",
        "#         return val_loader\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM0a7uvA2Las",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model = LitModel()\n",
        "\n",
        "# trainer = Trainer(\n",
        "#     tpu_cores=8, max_epochs=FLAGS['num_epochs'],\n",
        "#     checkpoint_callback=False,\n",
        "#     row_log_interval=20\n",
        "# )\n",
        "\n",
        "# trainer.fit(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6uiTH8QBQNy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir ./lightning_logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rs9H422-2eak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!rm -rf /content/lightning_logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haUahtOWEC7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}