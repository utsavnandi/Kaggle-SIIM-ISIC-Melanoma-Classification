{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TPU SIIM-ISIC Melanoma Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wFoIORVvcWn5",
        "CwWrFw5C-R5J",
        "kFL68tLin0Ky",
        "Z9m9lx4ewYzk",
        "ndmsl1XUrWFp",
        "PPAHFSOoSiI2"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMieCxOuFgpGP/E2AHB2mmA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utsavnandi/Kaggle-SIIM-ISIC-Melanoma-Classification/blob/master/TPU_SIIM_ISIC_Melanoma_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-_fb9AEa6Wq",
        "colab_type": "text"
      },
      "source": [
        "## One-time\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tBeE-hte5Tl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import os\n",
        "# assert os.environ['COLAB_TPU_ADDR']\n",
        "# VERSION = \"nightly\"  #@param [\"1.5\" , \"20200516\", \"nightly\"]\n",
        "# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "# !python pytorch-xla-env-setup.py --version $VERSION # --apt-packages libomp5 libopenblas-dev\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymGpJBznaEZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%time\n",
        "# !pip uninstall kaggle -y\n",
        "# !pip install kaggle==1.5.6 -q\n",
        "# !pip install -U catalyst -q\n",
        "# !pip install -U git+https://github.com/albu/albumentations -q\n",
        "# !pip install -U git+https://github.com/rwightman/pytorch-image-models -q\n",
        "# !pip install -U git+https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer -q\n",
        "\n",
        "# !mkdir ~/.kaggle/\n",
        "# !cp ./kaggle.json  ~/.kaggle/kaggle.json\n",
        "# !chmod 600 ~/.kaggle/kaggle.json\n",
        "# !kaggle datasets download -d shonenkov/melanoma-merged-external-data-512x512-jpeg\n",
        "# !unzip melanoma-merged-external-data-512x512-jpeg.zip -d ./data/\n",
        "# !rm melanoma-merged-external-data-512x512-jpeg.zip\n",
        "# !kaggle competitions download siim-isic-melanoma-classification -f sample_submission.csv\n",
        "# !kaggle competitions download siim-isic-melanoma-classification -f test.csv\n",
        "# !kaggle competitions download siim-isic-melanoma-classification -f train.csv\n",
        "# !unzip train.csv -d ./data/\n",
        "# !mv ./test.csv ./data/\n",
        "# !mv ./sample_submission.csv ./data/\n",
        "# !rm train.csv.zip\n",
        "# !mkdir ./logs/"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFoIORVvcWn5",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cB029jhCcJFF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "65867b5d-3aa1-4086-9745-1f9825583df4"
      },
      "source": [
        "\n",
        "import gc\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "#os.environ['XLA_USE_BF16'] = \"0\"\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pandas as pd\n",
        "\n",
        "#from google.colab import auth\n",
        "#from google.cloud import storage\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torchvision import transforms, models\n",
        "\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.utils.serialization as xser\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.utils.utils as xu\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "from ranger import Ranger\n",
        "from catalyst.data.sampler import DistributedSamplerWrapper, BalanceClassSampler\n",
        "import timm\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    xm.set_rng_state(seed, device=xm.xla_device())\n",
        "    "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning:\n",
            "\n",
            "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-XASBvOcVwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_DIR = '/content/data/'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uG9Xunfy3N3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(DATA_DIR+'folds.csv')\n",
        "df_test = pd.read_csv(DATA_DIR+'test.csv').rename(columns={'image_name':'image_id'})\n",
        "sample_submission = pd.read_csv(DATA_DIR+'sample_submission.csv')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIKzfKD2zA9A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d12ef3d2-ac58-4287-d3f9-277035e706dd"
      },
      "source": [
        "df_train['fold'].value_counts()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    12219\n",
              "1    12106\n",
              "2    12072\n",
              "4    12048\n",
              "3    12042\n",
              "Name: fold, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOuLmekTzlM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fold_no = 1\n",
        "X_train = df_train[df_train['fold'] != fold_no][[col for col in df_train.columns if col != 'target']]\n",
        "y_train = df_train[df_train['fold'] != fold_no][[col for col in df_train.columns if col == 'target']]\n",
        "X_val = df_train[df_train['fold'] == fold_no][[col for col in df_train.columns if col != 'target']]\n",
        "y_val = df_train[df_train['fold'] == fold_no][[col for col in df_train.columns if col == 'target']]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yovLpNhMcvnW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4a10bfe8-99d6-4811-e96c-25b0ebed5d68"
      },
      "source": [
        "print('X_train', X_train.shape)\n",
        "print('y_train', y_train.shape)\n",
        "print('X_val', X_val.shape)\n",
        "print('y_val', y_val.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train (48381, 8)\n",
            "y_train (48381, 1)\n",
            "X_val (12106, 8)\n",
            "y_val (12106, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWae70vKk9dw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4503161f-43c3-482b-dc28-a0d2d8dbbe64"
      },
      "source": [
        "print('Train target distribution: ')\n",
        "print(y_train['target'].value_counts())\n",
        "print('Val target distribution: ')\n",
        "print(y_val['target'].value_counts())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train target distribution: \n",
            "0    43997\n",
            "1     4384\n",
            "Name: target, dtype: int64\n",
            "Val target distribution: \n",
            "0    11011\n",
            "1     1095\n",
            "Name: target, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwWrFw5C-R5J",
        "colab_type": "text"
      },
      "source": [
        "##  Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wK-NTFy-PwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MelanomaDataset(Dataset):\n",
        "\n",
        "    def __init__(self, df, labels, istrain=False, transforms=None):\n",
        "        super().__init__()\n",
        "        self.image_id = df['image_id'].values\n",
        "        self.transforms = transforms\n",
        "        self.labels = labels.values\n",
        "        self.neg_indices = np.where(self.labels==0)[0]\n",
        "        self.pos_indices = np.where(self.labels==1)[0]\n",
        "        self.istrain = istrain\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_id)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if torch.is_tensor(index):\n",
        "            index = index.tolist()\n",
        "        \n",
        "        image, target = self.load_image(index)\n",
        "        \n",
        "        if not self.istrain:\n",
        "            if self.transforms:\n",
        "                image = self.transforms(image=image)['image']\n",
        "                return image, target\n",
        "\n",
        "        if np.random.random() < 0.5:\n",
        "            image, target = self.cutmix(image, target)\n",
        "\n",
        "        if self.transforms:\n",
        "            image = self.transforms(image=image)['image']\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def load_image(self, index):\n",
        "        if torch.is_tensor(index):\n",
        "            index = index.tolist()\n",
        "        image_name = DATA_DIR + f'512x512-dataset-melanoma/512x512-dataset-melanoma/{self.image_id[index]}.jpg'\n",
        "        image = cv2.imread(image_name, cv2.IMREAD_COLOR).astype(np.uint8)\n",
        "        target = self.labels[index].astype(np.float32)\n",
        "        return image, target\n",
        "\n",
        "    def cutmix(self, data, target, alpha=1):\n",
        "        rand_index = self.get_rand_index()\n",
        "        random_image, random_target = self.load_image(rand_index)\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "        bbx1, bby1, bbx2, bby2 = self.rand_bbox(data.shape, lam)\n",
        "        data[:, bbx1:bbx2, bby1:bby2] = random_image[ :, bbx1:bbx2, bby1:bby2]\n",
        "        lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (data.shape[-1] * data.shape[-2]))\n",
        "        new_target = lam * target + (1-lam) * random_target\n",
        "        return data, new_target\n",
        "\n",
        "    def mixup(self, data, target, alpha=1):\n",
        "        rand_index = self.get_rand_index()\n",
        "        random_image, random_target = self.load_image(rand_index)\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "        data = data * lam + random_image * (1 - lam)\n",
        "        new_target = lam * target + (1-lam) * random_target\n",
        "        return data, new_target\n",
        "\n",
        "    def get_rand_index(self):\n",
        "        if random.random()>0.5:\n",
        "            rand_index = np.random.choice(self.pos_indices)\n",
        "        else:\n",
        "            rand_index = np.random.choice(self.neg_indices)\n",
        "        return rand_index\n",
        "\n",
        "    def rand_bbox(self, size, lam):\n",
        "        W = size[1]\n",
        "        H = size[2]\n",
        "        cut_rat = np.sqrt(1. - lam)\n",
        "        cut_w = np.int(W * cut_rat)\n",
        "        cut_h = np.int(H * cut_rat)\n",
        "        cx = np.random.randint(W)\n",
        "        cy = np.random.randint(H)\n",
        "        bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "        bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "        bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "        bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "        return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "def get_datasets():\n",
        "    datasets = {}\n",
        "    datasets['train'] = MelanomaDataset(\n",
        "        X_train, y_train, istrain=True, transforms=get_train_transforms()\n",
        "    )\n",
        "    datasets['valid'] = MelanomaDataset(\n",
        "        X_val, y_val, istrain=False, transforms=get_valid_transforms()\n",
        "    )\n",
        "    return datasets\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFL68tLin0Ky",
        "colab_type": "text"
      },
      "source": [
        "## Augmentations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPWu-IIliVVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%writefile augmentations.txt\n",
        "# Reference IMG_SIZE\n",
        "# B0    - 224\n",
        "# B1    - 240\n",
        "# B2    - 260\n",
        "# B3    - 300\n",
        "# B4    - 380\n",
        "# B5    - 456\n",
        "# B6    - 520\n",
        "# B7    - 600\n",
        "# B8    - 672\n",
        "# L2 NS - 475\n",
        "# L2    - 800\n",
        "\n",
        "# Transforms\n",
        "IMG_SIZE = 300\n",
        "mean = (0.485, 0.456, 0.406)\n",
        "std = (0.229, 0.224, 0.225)\n",
        "\n",
        "def get_train_transforms(p=1.0):\n",
        "    return A.Compose([\n",
        "        A.Resize(IMG_SIZE, IMG_SIZE, interpolation=2, always_apply=True, p=1),\n",
        "        A.RandomResizedCrop(\n",
        "            IMG_SIZE, IMG_SIZE, scale=(0.8, 1.2), interpolation=2, p=0.33\n",
        "        ),\n",
        "        A.Flip(p=0.33),\n",
        "        A.Transpose(p=0.33),\n",
        "        #A.OneOf([\n",
        "        #    A.MedianBlur(blur_limit=3, p=0.5),\n",
        "        #    A.Blur(blur_limit=3, p=0.5),\n",
        "        #], p=0.5),\n",
        "        A.ShiftScaleRotate(\n",
        "            interpolation=2,\n",
        "            shift_limit=0.0625, scale_limit=0.15, \n",
        "            rotate_limit=15, p=0.3\n",
        "        ),\n",
        "        #A.OneOf([\n",
        "        #    A.OpticalDistortion(p=0.3),\n",
        "        #    A.GridDistortion(p=.1),\n",
        "        #    A.IAAPiecewiseAffine(p=0.3),\n",
        "        #], p=0.5),\n",
        "        #A.OneOf([\n",
        "        #    A.CLAHE(clip_limit=2),\n",
        "        #    A.IAASharpen(),\n",
        "        #    A.IAAEmboss(),\n",
        "        #    A.RandomBrightnessContrast(),            \n",
        "        #], p=0.5),\n",
        "        A.HueSaturationValue(\n",
        "            hue_shift_limit=20, sat_shift_limit=30, \n",
        "            val_shift_limit=20, p=0.33\n",
        "        ),\n",
        "        A.MultiplicativeNoise(\n",
        "            multiplier=[0.75, 1.25], \n",
        "            elementwise=True, p=0.33\n",
        "        ),\n",
        "        A.Normalize(mean, std, max_pixel_value=255.0, always_apply=True),\n",
        "        ToTensorV2(p=1.0),\n",
        "    ], p=p)\n",
        "\n",
        "def get_valid_transforms():\n",
        "    return A.Compose([\n",
        "        A.Resize(IMG_SIZE, IMG_SIZE, interpolation=2, always_apply=True, p=1),\n",
        "        A.Normalize(mean, std, max_pixel_value=255.0, always_apply=True),\n",
        "        ToTensorV2(p=1.0),\n",
        "    ])\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9m9lx4ewYzk",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfPBXkoYyPAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EfficientNet(nn.Module):\n",
        "\n",
        "    def __init__(self, name='tf_efficientnet_b0_ns'):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model(name, pretrained=True)\n",
        "        in_features = self.model.classifier.in_features\n",
        "        self.model.classifier = nn.Linear(in_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class Gluon_seresnext50_32x4d(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model('gluon_seresnext50_32x4d', pretrained=True)\n",
        "        in_features = self.model.fc.in_features\n",
        "        self.model.fc = nn.Linear(in_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndmsl1XUrWFp",
        "colab_type": "text"
      },
      "source": [
        "## Custom Losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IICyzNqJrZnl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2, logits=True, reduce=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.logits = logits\n",
        "        self.reduce = reduce\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        if self.logits:\n",
        "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
        "        else:\n",
        "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-BCE_loss)\n",
        "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
        "\n",
        "        if self.reduce:\n",
        "            return torch.mean(F_loss)\n",
        "        else:\n",
        "            return F_loss\n",
        "\n",
        "def ohem_loss(cls_pred, cls_target, rate):\n",
        "    ohem_cls_loss = F.binary_cross_entropy_with_logits(cls_pred, cls_target, reduction='none')\n",
        "    batch_size = cls_pred.size(0)\n",
        "    sorted_ohem_loss, idx = torch.sort(ohem_cls_loss, descending=True)\n",
        "    keep_num = min(sorted_ohem_loss.size()[0], int(batch_size*rate))\n",
        "    if keep_num < sorted_ohem_loss.size()[0]:\n",
        "        keep_idx_cuda = idx[:keep_num]\n",
        "        ohem_cls_loss = ohem_cls_loss[keep_idx_cuda]\n",
        "    cls_loss = ohem_cls_loss.sum() / keep_num\n",
        "    return cls_loss\n",
        "\n",
        "def bce_criterion(y_pred, y_true):\n",
        "    return nn.BCEWithLogitsLoss()(y_pred, y_true)\n",
        "\n",
        "def focal_criterion(y_pred, y_true):\n",
        "    return FocalLoss(alpha=(43997/4384))(y_pred, y_true)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPAHFSOoSiI2",
        "colab_type": "text"
      },
      "source": [
        "## Metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmPxF8kEnTrY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RocAucMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.y_true = np.array([0,1])\n",
        "        self.y_pred = np.array([0.5,0.5])\n",
        "        self.score = 0\n",
        "\n",
        "    def update(self, y_true, y_pred):\n",
        "        y_true = y_true.cpu().numpy()\n",
        "        y_pred = torch.flatten(torch.sigmoid(y_pred)).data.cpu().numpy()\n",
        "        self.y_true = np.append(self.y_true, y_true)\n",
        "        self.y_pred = np.append(self.y_pred, y_pred)\n",
        "        self.score = roc_auc_score(self.y_true, self.y_pred)\n",
        "\n",
        "    @property\n",
        "    def avg(self):\n",
        "        return self.score\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruo8SHLGnZJU",
        "colab_type": "text"
      },
      "source": [
        "## Train script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3N0J0GZYTUT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# optimizer = Ranger(\n",
        "#     model.parameters(),\n",
        "#     lr=FLAGS['learning_rate'] * xm.xrt_world_size(), \n",
        "#     alpha=0.5, k=6, N_sma_threshhold=5,\n",
        "#     weight_decay=FLAGS['weight_decay']\n",
        "# )"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TggTDLcOxUT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SERIAL_EXEC = xmp.MpSerialExecutor()\n",
        "\n",
        "def train_model(WRAPPED_MODEL, X_train, y_train, X_val, y_val, fold_no):\n",
        "\n",
        "    seed_everything(43)\n",
        "\n",
        "    def get_datasets(X_train, y_train, X_val, y_val):\n",
        "        datasets = {}\n",
        "        datasets['train'] = MelanomaDataset(\n",
        "            X_train, y_train, istrain=True, transforms=get_train_transforms()\n",
        "        )\n",
        "        datasets['valid'] = MelanomaDataset(\n",
        "            X_val, y_val, istrain=False, transforms=get_valid_transforms()\n",
        "        )\n",
        "        return datasets\n",
        "\n",
        "    datasets = SERIAL_EXEC.run(\n",
        "        lambda: get_datasets(X_train, y_train, X_val, y_val)\n",
        "    )\n",
        "\n",
        "    labels_vcount = y_train['target'].value_counts()\n",
        "    class_counts = [labels_vcount[0].astype(np.float32), labels_vcount[1].astype(np.float32)]\n",
        "    num_samples = sum(class_counts)\n",
        "    class_weights = [num_samples/class_counts[i] for i in range(len(class_counts))]\n",
        "    weights = [class_weights[y_train['target'].values[i]] for i in range(int(num_samples))]\n",
        "    wrsampler = WeightedRandomSampler(\n",
        "        torch.DoubleTensor(weights), int(num_samples)\n",
        "    )\n",
        "    #BalanceClassSampler(labels=y_train['target'].values, mode=\"downsampling\"),\n",
        "\n",
        "    train_sampler = DistributedSamplerWrapper(\n",
        "        sampler=wrsampler,\n",
        "        num_replicas=xm.xrt_world_size(),\n",
        "        rank=xm.get_ordinal(),\n",
        "        shuffle=True\n",
        "    )\n",
        "    validation_sampler = DistributedSampler(\n",
        "        datasets['valid'],\n",
        "        num_replicas=xm.xrt_world_size(),\n",
        "        rank=xm.get_ordinal(),\n",
        "        shuffle=False\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        datasets['train'],\n",
        "        batch_size=FLAGS['batch_size'], \n",
        "        num_workers=FLAGS['num_workers'],\n",
        "        sampler=train_sampler,\n",
        "        drop_last=True,\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        datasets['valid'],\n",
        "        batch_size=FLAGS['batch_size'],\n",
        "        num_workers=FLAGS['num_workers'],\n",
        "        sampler=validation_sampler,\n",
        "        drop_last=True\n",
        "    )\n",
        "\n",
        "    device = xm.xla_device()\n",
        "    model = WRAPPED_MODEL.to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=FLAGS['learning_rate'] * xm.xrt_world_size(),\n",
        "        weight_decay=FLAGS['weight_decay']\n",
        "    )\n",
        "\n",
        "    #criterion = focal_criterion\n",
        "    criterion = bce_criterion\n",
        " \n",
        "    def train_one_epoch(loader):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        max_idx = 0\n",
        "        xm.master_print('Step\\t|\\tTime')\n",
        "        xm.master_print('-'*40)\n",
        "        for idx, (images, targets) in enumerate(loader):\n",
        "            #images = images.to(device)\n",
        "            #targets = targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(images.float())\n",
        "            loss = criterion(y_pred, targets)\n",
        "            running_loss += float(loss)\n",
        "            loss.backward()\n",
        "            xm.optimizer_step(optimizer)\n",
        "            # xm.mark_step() call everystep for grad accum\n",
        "            max_idx = float(idx)\n",
        "            if idx % FLAGS['log_steps'] == 0 and idx !=0:\n",
        "                xm.master_print('({})\\t|\\t{}'.format(\n",
        "                    idx, time.asctime(time.localtime()))\n",
        "                )\n",
        "        return running_loss/(max_idx+1)\n",
        "\n",
        "    def val_one_epoch(loader):\n",
        "        model.eval()\n",
        "        running_loss = 0\n",
        "        max_idx = 0\n",
        "        roc_auc_scores = RocAucMeter()\n",
        "        with torch.no_grad():\n",
        "            for idx, (images, targets) in enumerate(loader):\n",
        "                #images = images.to(device)\n",
        "                #targets = targets.to(device)\n",
        "                y_pred = model(images.float())\n",
        "                loss = criterion(y_pred, targets)\n",
        "                running_loss += float(loss)\n",
        "                max_idx = float(idx)\n",
        "                roc_auc_scores.update(targets, y_pred)\n",
        "        score = roc_auc_scores.avg\n",
        "        return running_loss/(max_idx+1), score\n",
        "\n",
        "    xm.master_print('='*26 + f'Fold #{fold_no} started' + '='*27)\n",
        "\n",
        "    for epoch in range(0, FLAGS['num_epochs']):\n",
        "        xm.master_print('-'*26 + f'Epoch #{epoch+1} started' + '-'*26)\n",
        "        xm.master_print(f'Epoch start {time.asctime(time.localtime())}')\n",
        "\n",
        "        train_start = time.time()\n",
        "        para_loader = pl.ParallelLoader(train_loader, [device])\n",
        "        train_loss = train_one_epoch(para_loader.per_device_loader(device))\n",
        "        xm.master_print(\"finished training epoch {}\".format(epoch+1))\n",
        "        xm.master_print(f'elapsed time: {(time.time()-train_start)/60.0:.2f}mins')\n",
        "        #print(\n",
        "        #    f'[xla:{xm.get_ordinal()}] average loss for epoch #{epoch+1} : {train_loss:.5f}',\n",
        "        #    flush=True\n",
        "        #)\n",
        "        reduced_loss = xm.mesh_reduce('train_loss', train_loss, lambda x: np.array(x).mean())\n",
        "        xm.master_print(f\"reduced loss {reduced_loss:.5f}\")\n",
        "        if (epoch+1) % 1 == 0:\n",
        "            val_start = time.time()\n",
        "            para_loader = pl.ParallelLoader(val_loader, [device])\n",
        "            val_loss, auc_score = val_one_epoch(para_loader.per_device_loader(device))    \n",
        "            xm.master_print(\"finished validating epoch {}\".format(epoch+1))\n",
        "            #print(f'[xla:{xm.get_ordinal()}] average loss for val epoch: {val_loss:.5f}', flush=True)\n",
        "            #print(f'[xla:{xm.get_ordinal()}] roc_auc_score: {auc_score:.5f}', flush=True)\n",
        "            reduced_val_loss = xm.mesh_reduce('val_loss', val_loss, lambda x: np.array(x).mean())\n",
        "            reduced_auc_score = xm.mesh_reduce('auc_score', auc_score, lambda x: np.array(x).mean())\n",
        "            xm.master_print(f\"reduced val loss {reduced_val_loss:.5f}\")\n",
        "            xm.master_print(f\"reduced auc score {reduced_auc_score:.5f}\")\n",
        "            xm.master_print(f'elapsed time: {(time.time()-val_start)/60.0:.2f}mins')\n",
        "            xm.save(\n",
        "                model.state_dict(), \n",
        "                f\"./{FLAGS['exp_name']}_fold_{fold_no}_epoch_{epoch+1}_auc_{reduced_auc_score:.5f}.pth\"\n",
        "            )\n",
        "            xm.master_print(f'saved model...')\n",
        "\n",
        "        xm.master_print(f'Epoch end {time.asctime(time.localtime())}')\n",
        "        xm.master_print('-'*27 + f'Epoch #{epoch+1} ended' + '-'*27)\n",
        "\n",
        "    xm.master_print('='*27 + f'Fold #{fold_no} ended' + '='*27)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmLSHZZdS_1i",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHJtNpfOUdqD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "2d5ac984-c084-44e3-e01c-f4c1efc840a2"
      },
      "source": [
        "FLAGS = {}\n",
        "FLAGS['batch_size'] = 32\n",
        "FLAGS['num_workers'] = 8\n",
        "FLAGS['learning_rate'] = 2e-4\n",
        "FLAGS['num_epochs'] = 10\n",
        "FLAGS['weight_decay'] = 1e-4\n",
        "FLAGS['log_steps'] = 45\n",
        "FLAGS['img_size'] = IMG_SIZE\n",
        "FLAGS['loss'] = 'BCE'\n",
        "FLAGS['optimizer'] = 'AdamW'\n",
        "FLAGS['exp_name'] = 'enet_b3'\n",
        "FLAGS['fold'] = 1\n",
        "FLAGS['num_cores'] = 8\n",
        "\n",
        "def _mp_fn(rank, flags):\n",
        "    global FLAGS\n",
        "    FLAGS = flags\n",
        "    torch.set_default_tensor_type('torch.FloatTensor')\n",
        "    \n",
        "    for fold_no in range(1, 6):\n",
        "        WRAPPED_MODEL = xmp.MpModelWrapper(EfficientNet('tf_efficientnet_b3_ns'))\n",
        "        X_train = df_train[df_train['fold'] != fold_no][[col for col in df_train.columns if col != 'target']]\n",
        "        y_train = df_train[df_train['fold'] != fold_no][[col for col in df_train.columns if col == 'target']]\n",
        "        X_val = df_train[df_train['fold'] == fold_no][[col for col in df_train.columns if col != 'target']]\n",
        "        y_val = df_train[df_train['fold'] == fold_no][[col for col in df_train.columns if col == 'target']]\n",
        "        train_model(WRAPPED_MODEL, X_train, y_train, X_val, y_val, fold_no)\n",
        "\n",
        "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=FLAGS['num_cores'],\n",
        "          start_method='fork')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==========================Fold #1 started===========================\n",
            "--------------------------Epoch #1 started--------------------------\n",
            "Epoch start Thu Jun 25 18:47:49 2020\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tThu Jun 25 18:49:56 2020\n",
            "(90)\t|\tThu Jun 25 18:51:43 2020\n",
            "(135)\t|\tThu Jun 25 18:53:27 2020\n",
            "(180)\t|\tThu Jun 25 18:55:12 2020\n",
            "[xla:4] average loss for epoch #1 : 0.44984\n",
            "[xla:1] average loss for epoch #1 : 0.44856\n",
            "[xla:6] average loss for epoch #1 : 0.46615\n",
            "[xla:5] average loss for epoch #1 : 0.45241\n",
            "[xla:2] average loss for epoch #1 : 0.45649\n",
            "[xla:7] average loss for epoch #1 : 0.45948\n",
            "[xla:3] average loss for epoch #1 : 0.45681\n",
            "finished training epoch 1\n",
            "elapsed time: 7.63mins\n",
            "[xla:0] average loss for epoch #1 : 0.45226\n",
            "reduced loss 0.45525\n",
            "finished validating epoch 1\n",
            "reduced val loss 0.35028\n",
            "reduced auc score 0.92106\n",
            "elapsed time: 1.90mins\n",
            "saved model...\n",
            "Epoch end Thu Jun 25 18:57:21 2020\n",
            "---------------------------Epoch #1 ended---------------------------\n",
            "--------------------------Epoch #2 started--------------------------\n",
            "Epoch start Thu Jun 25 18:57:21 2020\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hp14r-XHSBga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}