{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TPU SIIM-ISIC Melanoma Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "L-_fb9AEa6Wq",
        "wFoIORVvcWn5",
        "CwWrFw5C-R5J",
        "kFL68tLin0Ky"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMWIUUL2im598yaDVtBS9MN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utsavnandi/Kaggle-SIIM-ISIC-Melanoma-Classification/blob/master/TPU_SIIM_ISIC_Melanoma_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-_fb9AEa6Wq",
        "colab_type": "text"
      },
      "source": [
        "## One-time\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tBeE-hte5Tl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import os\n",
        "# assert os.environ['COLAB_TPU_ADDR']\n",
        "# VERSION = \"nightly\"  #@param [\"1.5\" , \"20200516\", \"nightly\"]\n",
        "# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "# !python pytorch-xla-env-setup.py --version $VERSION # --apt-packages libomp5 libopenblas-dev\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymGpJBznaEZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%time\n",
        "# !pip uninstall kaggle -y\n",
        "# !pip install kaggle==1.5.6 -q\n",
        "# !pip install -U catalyst -q\n",
        "# !pip install -U git+https://github.com/albu/albumentations -q\n",
        "# !pip install -U git+https://github.com/rwightman/pytorch-image-models -q\n",
        "# !pip install -U git+https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer -q\n",
        "\n",
        "# !mkdir ~/.kaggle/\n",
        "# !cp ./kaggle.json  ~/.kaggle/kaggle.json\n",
        "# !chmod 600 ~/.kaggle/kaggle.json\n",
        "# !kaggle datasets download -d shonenkov/melanoma-merged-external-data-512x512-jpeg\n",
        "# !unzip melanoma-merged-external-data-512x512-jpeg.zip -d ./data/\n",
        "# !rm melanoma-merged-external-data-512x512-jpeg.zip\n",
        "# !kaggle competitions download siim-isic-melanoma-classification -f sample_submission.csv\n",
        "# !kaggle competitions download siim-isic-melanoma-classification -f test.csv\n",
        "# !kaggle competitions download siim-isic-melanoma-classification -f train.csv\n",
        "# !unzip train.csv -d ./data/\n",
        "# !mv ./test.csv ./data/\n",
        "# !mv ./sample_submission.csv ./data/\n",
        "# !rm train.csv.zip\n",
        "# !mkdir ./logs/"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFoIORVvcWn5",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cB029jhCcJFF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2a0f8a2a-a099-4de8-f218-d441ec3cb385"
      },
      "source": [
        "import gc\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "#os.environ['XLA_USE_BF16'] = \"0\"\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pandas as pd\n",
        "\n",
        "#from google.colab import auth\n",
        "#from google.cloud import storage\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torchvision import transforms, models\n",
        "\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.utils.serialization as xser\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.utils.utils as xu\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "from ranger import Ranger\n",
        "from catalyst.data.sampler import DistributedSamplerWrapper, BalanceClassSampler\n",
        "import timm\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    xm.set_rng_state(seed, device=xm.xla_device())\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning:\n",
            "\n",
            "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-XASBvOcVwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_DIR = '/content/data/'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uG9Xunfy3N3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(DATA_DIR+'folds.csv')\n",
        "df_test = pd.read_csv(DATA_DIR+'test.csv').rename(columns={'image_name':'image_id'})\n",
        "sample_submission = pd.read_csv(DATA_DIR+'sample_submission.csv')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIKzfKD2zA9A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "80a8d226-1931-4231-a9c7-6887f2fabf4d"
      },
      "source": [
        "df_train['fold'].value_counts()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    12219\n",
              "1    12106\n",
              "2    12072\n",
              "4    12048\n",
              "3    12042\n",
              "Name: fold, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOuLmekTzlM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fold_no = 1\n",
        "X_train = df_train[df_train['fold'] != fold_no][[col for col in df_train.columns if col != 'target']]\n",
        "y_train = df_train[df_train['fold'] != fold_no][[col for col in df_train.columns if col == 'target']]\n",
        "X_val = df_train[df_train['fold'] == fold_no][[col for col in df_train.columns if col != 'target']]\n",
        "y_val = df_train[df_train['fold'] == fold_no][[col for col in df_train.columns if col == 'target']]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yovLpNhMcvnW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "79861c92-33f3-45af-f53c-af0f1ad7244b"
      },
      "source": [
        "print('X_train', X_train.shape)\n",
        "print('y_train', y_train.shape)\n",
        "print('X_val', X_val.shape)\n",
        "print('y_val', y_val.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train (48381, 8)\n",
            "y_train (48381, 1)\n",
            "X_val (12106, 8)\n",
            "y_val (12106, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWae70vKk9dw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "6bf3dff7-84e0-4712-835e-0404f206eeca"
      },
      "source": [
        "print('Train target distribution: ')\n",
        "print(y_train['target'].value_counts())\n",
        "print('Val target distribution: ')\n",
        "print(y_val['target'].value_counts())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train target distribution: \n",
            "0    43997\n",
            "1     4384\n",
            "Name: target, dtype: int64\n",
            "Val target distribution: \n",
            "0    11011\n",
            "1     1095\n",
            "Name: target, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwWrFw5C-R5J",
        "colab_type": "text"
      },
      "source": [
        "##  Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wK-NTFy-PwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MelanomaDataset(Dataset):\n",
        "\n",
        "    def __init__(self, df, labels, istrain=False, transforms=None):\n",
        "        super().__init__()\n",
        "        self.image_id = df['image_id'].values\n",
        "        self.transforms = transforms\n",
        "        self.labels = labels.values\n",
        "        self.neg_indices = np.where(self.labels==0)[0]\n",
        "        self.pos_indices = np.where(self.labels==1)[0]\n",
        "        self.istrain = istrain\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_id)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if torch.is_tensor(index):\n",
        "            index = index.tolist()\n",
        "        \n",
        "        image, target = self.load_image(index)\n",
        "        \n",
        "        if not self.istrain:\n",
        "            if self.transforms:\n",
        "                image = self.transforms(image=image)['image']\n",
        "                return image, target\n",
        "\n",
        "        if np.random.random() < 0.5:\n",
        "            image, target = self.cutmix(image, target)\n",
        "\n",
        "        if self.transforms:\n",
        "            image = self.transforms(image=image)['image']\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def load_image(self, index):\n",
        "        if torch.is_tensor(index):\n",
        "            index = index.tolist()\n",
        "        image_name = DATA_DIR + f'512x512-dataset-melanoma/512x512-dataset-melanoma/{self.image_id[index]}.jpg'\n",
        "        image = cv2.imread(image_name, cv2.IMREAD_COLOR).astype(np.uint8)\n",
        "        target = self.labels[index].astype(np.float32)\n",
        "        return image, target\n",
        "\n",
        "    def cutmix(self, data, target, alpha=1):\n",
        "        rand_index = self.get_rand_index()\n",
        "        random_image, random_target = self.load_image(rand_index)\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "        bbx1, bby1, bbx2, bby2 = self.rand_bbox(data.shape, lam)\n",
        "        data[:, bbx1:bbx2, bby1:bby2] = random_image[ :, bbx1:bbx2, bby1:bby2]\n",
        "        lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (data.shape[-1] * data.shape[-2]))\n",
        "        new_target = lam * target + (1-lam) * random_target\n",
        "        return data, new_target\n",
        "\n",
        "    def mixup(self, data, target, alpha=1):\n",
        "        rand_index = self.get_rand_index()\n",
        "        random_image, random_target = self.load_image(rand_index)\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "        data = data * lam + random_image * (1 - lam)\n",
        "        new_target = lam * target + (1-lam) * random_target\n",
        "        return data, new_target\n",
        "\n",
        "    def get_rand_index(self):\n",
        "        if random.random()>0.5:\n",
        "            rand_index = np.random.choice(self.pos_indices)\n",
        "        else:\n",
        "            rand_index = np.random.choice(self.neg_indices)\n",
        "        return rand_index\n",
        "\n",
        "    def rand_bbox(self, size, lam):\n",
        "        W = size[1]\n",
        "        H = size[2]\n",
        "        cut_rat = np.sqrt(1. - lam)\n",
        "        cut_w = np.int(W * cut_rat)\n",
        "        cut_h = np.int(H * cut_rat)\n",
        "        cx = np.random.randint(W)\n",
        "        cy = np.random.randint(H)\n",
        "        bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "        bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "        bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "        bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "        return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "def get_datasets():\n",
        "    datasets = {}\n",
        "    datasets['train'] = MelanomaDataset(\n",
        "        X_train, y_train, istrain=True, transforms=get_train_transforms()\n",
        "    )\n",
        "    datasets['valid'] = MelanomaDataset(\n",
        "        X_val, y_val, istrain=False, transforms=get_valid_transforms()\n",
        "    )\n",
        "    return datasets\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFL68tLin0Ky",
        "colab_type": "text"
      },
      "source": [
        "## Augmentations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPWu-IIliVVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%writefile augmentations.txt\n",
        "# Reference IMG_SIZE\n",
        "# B0    - 224\n",
        "# B1    - 240\n",
        "# B2    - 260\n",
        "# B3    - 300\n",
        "# B4    - 380\n",
        "# B5    - 456\n",
        "# B6    - 520\n",
        "# B7    - 600\n",
        "# B8    - 672\n",
        "# L2 NS - 475\n",
        "# L2    - 800\n",
        "\n",
        "# Transforms\n",
        "IMG_SIZE = 224\n",
        "mean = (0.485, 0.456, 0.406)\n",
        "std = (0.229, 0.224, 0.225)\n",
        "\n",
        "def get_train_transforms(p=1.0):\n",
        "    return A.Compose([\n",
        "        A.Resize(IMG_SIZE, IMG_SIZE, interpolation=2, always_apply=True, p=1),\n",
        "        A.RandomResizedCrop(\n",
        "            IMG_SIZE, IMG_SIZE, scale=(0.8, 1.2), interpolation=2, p=0.33\n",
        "        ),\n",
        "        A.Flip(p=0.5),\n",
        "        A.Transpose(p=0.25),\n",
        "        A.OneOf([\n",
        "            A.MedianBlur(blur_limit=3, p=0.5),\n",
        "            A.Blur(blur_limit=3, p=0.5),\n",
        "        ], p=0.5),\n",
        "        A.ShiftScaleRotate(\n",
        "            interpolation=2,\n",
        "            shift_limit=0.0625, scale_limit=0.15, \n",
        "            rotate_limit=15, p=0.3\n",
        "        ),\n",
        "        A.OneOf([\n",
        "            A.OpticalDistortion(p=0.3),\n",
        "            A.GridDistortion(p=.1),\n",
        "            A.IAAPiecewiseAffine(p=0.3),\n",
        "        ], p=0.5),\n",
        "        A.OneOf([\n",
        "            A.CLAHE(clip_limit=2),\n",
        "            A.IAASharpen(),\n",
        "            A.IAAEmboss(),\n",
        "            A.RandomBrightnessContrast(),            \n",
        "        ], p=0.5),\n",
        "        A.HueSaturationValue(\n",
        "            hue_shift_limit=20, sat_shift_limit=30, \n",
        "            val_shift_limit=20, p=0.33\n",
        "        ),\n",
        "        A.MultiplicativeNoise(\n",
        "            multiplier=[0.75, 1.25], \n",
        "            elementwise=True, p=0.33\n",
        "        ),\n",
        "        A.Normalize(mean, std, max_pixel_value=255.0, always_apply=True),\n",
        "        ToTensorV2(p=1.0),\n",
        "    ], p=p)\n",
        "\n",
        "def get_valid_transforms():\n",
        "    return A.Compose([\n",
        "        A.Resize(IMG_SIZE, IMG_SIZE, interpolation=2, always_apply=True, p=1),\n",
        "        A.Normalize(mean, std, max_pixel_value=255.0, always_apply=True),\n",
        "        ToTensorV2(p=1.0),\n",
        "    ])\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9m9lx4ewYzk",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92GalIsk0KFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gem(x, p=3, eps=1e-6):\n",
        "    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n",
        "\n",
        "class GeM(nn.Module):\n",
        "    def __init__(self, p=3, eps=1e-6):\n",
        "        super(GeM,self).__init__()\n",
        "        self.p = Parameter(torch.ones(1)*p)\n",
        "        self.eps = eps\n",
        "    def forward(self, x):\n",
        "        return gem(x, p=self.p, eps=self.eps)       \n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + ', ' + 'eps=' + str(self.eps) + ')'\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfPBXkoYyPAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EfficientNet(nn.Module):\n",
        "\n",
        "    def __init__(self, name='tf_efficientnet_b0_ns'):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model(name, pretrained=True)\n",
        "        self.model.global_pool = GeM()\n",
        "        in_features = self.model.classifier.in_features\n",
        "        self.model.classifier = nn.Linear(in_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class SEResNext50_32x4d(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model('gluon_seresnext50_32x4d', pretrained=True)\n",
        "        in_features = self.model.fc.in_features\n",
        "        self.model.fc = nn.Linear(in_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class SEResNext101_32x4d(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model('gluon_seresnext101_32x4d', pretrained=True)\n",
        "        in_features = self.model.fc.in_features\n",
        "        self.model.fc = nn.Linear(in_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndmsl1XUrWFp",
        "colab_type": "text"
      },
      "source": [
        "## Custom Losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IICyzNqJrZnl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2, logits=True, reduce=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.logits = logits\n",
        "        self.reduce = reduce\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        if self.logits:\n",
        "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
        "        else:\n",
        "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-BCE_loss)\n",
        "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
        "\n",
        "        if self.reduce:\n",
        "            return torch.mean(F_loss)\n",
        "        else:\n",
        "            return F_loss\n",
        "\n",
        "def ohem_loss(cls_pred, cls_target, rate):\n",
        "    ohem_cls_loss = F.binary_cross_entropy_with_logits(cls_pred, cls_target, reduction='none')\n",
        "    batch_size = cls_pred.size(0)\n",
        "    sorted_ohem_loss, idx = torch.sort(ohem_cls_loss, descending=True)\n",
        "    keep_num = min(sorted_ohem_loss.size()[0], int(batch_size*rate))\n",
        "    if keep_num < sorted_ohem_loss.size()[0]:\n",
        "        keep_idx_cuda = idx[:keep_num]\n",
        "        ohem_cls_loss = ohem_cls_loss[keep_idx_cuda]\n",
        "    cls_loss = ohem_cls_loss.sum() / keep_num\n",
        "    return cls_loss\n",
        "\n",
        "def bce_criterion(y_pred, y_true):\n",
        "    return nn.BCEWithLogitsLoss()(y_pred, y_true)\n",
        "\n",
        "def focal_criterion(y_pred, y_true):\n",
        "    return FocalLoss(alpha=(43997/4384))(y_pred, y_true)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPAHFSOoSiI2",
        "colab_type": "text"
      },
      "source": [
        "## Metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmPxF8kEnTrY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RocAucMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.y_true = np.array([0,1])\n",
        "        self.y_pred = np.array([0.5,0.5])\n",
        "        self.score = 0\n",
        "\n",
        "    def update(self, y_true, y_pred):\n",
        "        y_true = y_true.cpu().numpy()\n",
        "        y_pred = torch.flatten(torch.sigmoid(y_pred)).data.cpu().numpy()\n",
        "        self.y_true = np.append(self.y_true, y_true)\n",
        "        self.y_pred = np.append(self.y_pred, y_pred)\n",
        "        self.score = roc_auc_score(self.y_true, self.y_pred)\n",
        "\n",
        "    @property\n",
        "    def avg(self):\n",
        "        return self.score\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruo8SHLGnZJU",
        "colab_type": "text"
      },
      "source": [
        "## Train script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3N0J0GZYTUT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# optimizer = Ranger(\n",
        "#     model.parameters(),\n",
        "#     lr=FLAGS['learning_rate'] * xm.xrt_world_size(), \n",
        "#     alpha=0.5, k=6, N_sma_threshhold=5,\n",
        "#     weight_decay=FLAGS['weight_decay']\n",
        "# )"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TggTDLcOxUT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(data, fold_no):\n",
        "    seed_everything(43)\n",
        "\n",
        "    def get_datasets(data):\n",
        "        X_train, y_train, X_val, y_val = data\n",
        "        datasets = {}\n",
        "        datasets['train'] = MelanomaDataset(\n",
        "            X_train, y_train, istrain=True, transforms=get_train_transforms()\n",
        "        )\n",
        "        datasets['valid'] = MelanomaDataset(\n",
        "            X_val, y_val, istrain=False, transforms=get_valid_transforms()\n",
        "        )\n",
        "        return datasets\n",
        "\n",
        "    datasets = SERIAL_EXEC.run(\n",
        "        lambda: get_datasets(data)\n",
        "    )\n",
        "\n",
        "    labels_vcount = y_train['target'].value_counts()\n",
        "    class_counts = [labels_vcount[0].astype(np.float32), labels_vcount[1].astype(np.float32)]\n",
        "    num_samples = sum(class_counts)\n",
        "    class_weights = [num_samples/class_counts[i] for i in range(len(class_counts))]\n",
        "    weights = [class_weights[y_train['target'].values[i]] for i in range(int(num_samples))]\n",
        "    wrsampler = WeightedRandomSampler(\n",
        "        torch.DoubleTensor(weights), int(num_samples)\n",
        "    )\n",
        "    #BalanceClassSampler(labels=y_train['target'].values, mode=\"downsampling\"),\n",
        "\n",
        "    train_sampler = DistributedSamplerWrapper(\n",
        "        sampler=wrsampler,\n",
        "        num_replicas=xm.xrt_world_size(),\n",
        "        rank=xm.get_ordinal(),\n",
        "        shuffle=True\n",
        "    )\n",
        "    validation_sampler = DistributedSampler(\n",
        "        datasets['valid'],\n",
        "        num_replicas=xm.xrt_world_size(),\n",
        "        rank=xm.get_ordinal(),\n",
        "        shuffle=False\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        datasets['train'],\n",
        "        batch_size=FLAGS['batch_size'], \n",
        "        num_workers=FLAGS['num_workers'],\n",
        "        sampler=train_sampler,\n",
        "        drop_last=True,\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        datasets['valid'],\n",
        "        batch_size=FLAGS['batch_size'],\n",
        "        num_workers=FLAGS['num_workers'],\n",
        "        sampler=validation_sampler,\n",
        "        drop_last=True\n",
        "    )\n",
        "\n",
        "    device = xm.xla_device()\n",
        "    model = WRAPPED_MODEL.to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=FLAGS['learning_rate'] * xm.xrt_world_size(),\n",
        "        weight_decay=FLAGS['weight_decay']\n",
        "    )\n",
        "\n",
        "    criterion = bce_criterion\n",
        " \n",
        "    def train_one_epoch(loader):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        max_idx = 0\n",
        "        xm.master_print('-'*40)\n",
        "        xm.master_print('Step\\t|\\tTime')\n",
        "        xm.master_print('-'*40)\n",
        "        for idx, (images, targets) in enumerate(loader):\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(images.float())\n",
        "            loss = criterion(y_pred, targets)\n",
        "            running_loss += float(loss)\n",
        "            loss.backward()\n",
        "            xm.optimizer_step(optimizer)\n",
        "            # xm.mark_step() call everystep for grad accum\n",
        "            max_idx = float(idx)\n",
        "            if idx % FLAGS['log_steps'] == 0 and idx !=0:\n",
        "                xm.master_print('({})\\t|\\t{}'.format(\n",
        "                    idx, time.asctime(time.localtime()))\n",
        "                )\n",
        "        xm.master_print('-'*40)\n",
        "        return running_loss/(max_idx+1)\n",
        "\n",
        "    def val_one_epoch(loader):\n",
        "        model.eval()\n",
        "        running_loss = 0\n",
        "        max_idx = 0\n",
        "        roc_auc_scores = RocAucMeter()\n",
        "        with torch.no_grad():\n",
        "            for idx, (images, targets) in enumerate(loader):\n",
        "                y_pred = model(images.float())\n",
        "                loss = criterion(y_pred, targets)\n",
        "                running_loss += float(loss)\n",
        "                max_idx = float(idx)\n",
        "                roc_auc_scores.update(targets, y_pred)\n",
        "        score = roc_auc_scores.avg\n",
        "        return running_loss/(max_idx+1), score\n",
        "\n",
        "    def _reduce_fn(x):\n",
        "        return np.array(x).mean()\n",
        "\n",
        "    best_score = 0\n",
        "    xm.master_print('='*26 + f'Fold #{fold_no} started' + '='*27)\n",
        "    for epoch in range(0, FLAGS['num_epochs']):\n",
        "        xm.master_print('-'*26 + f'Epoch #{epoch+1} started' + '-'*26)\n",
        "        xm.master_print(f'Epoch start {time.asctime(time.localtime())}')\n",
        "        train_start = time.time()\n",
        "        para_loader = pl.ParallelLoader(train_loader, [device])\n",
        "        train_loss = train_one_epoch(para_loader.per_device_loader(device))\n",
        "        xm.master_print(f\"finished training epoch {epoch+1}\")\n",
        "        elapsed_time = int(time.time() - train_start)\n",
        "        xm.master_print(\n",
        "            f'elapsed time: {(elapsed_time)//60}mins {(elapsed_time)%60}s'\n",
        "        )\n",
        "        reduced_loss = xm.mesh_reduce('train_loss', train_loss, _reduce_fn)\n",
        "        xm.master_print(f\"reduced loss {reduced_loss:.5f}\")\n",
        "        if (epoch+1) % FLAGS['val_freq'] == 0:\n",
        "            val_start = time.time()\n",
        "            para_loader = pl.ParallelLoader(val_loader, [device])\n",
        "            val_loss, auc_score = val_one_epoch(para_loader.per_device_loader(device))    \n",
        "            xm.master_print(f\"finished validating epoch {epoch+1}\")\n",
        "            reduced_val_loss = xm.mesh_reduce('val_loss', val_loss, _reduce_fn)\n",
        "            reduced_auc_score = xm.mesh_reduce('auc_score', auc_score, _reduce_fn)\n",
        "            xm.master_print(f\"reduced val loss {reduced_val_loss:.5f}\")\n",
        "            xm.master_print(f\"reduced auc score {reduced_auc_score:.5f}\")\n",
        "            val_elapsed_time = int(time.time() - val_start)\n",
        "            xm.master_print(\n",
        "                f'elapsed time: {(val_elapsed_time)//60}mins {(val_elapsed_time)%60}s'\n",
        "            )\n",
        "            if best_score < reduced_auc_score:\n",
        "                best_score = reduced_auc_score\n",
        "                file_name = f\"./{FLAGS['exp_name']}_fold_{fold_no+1}_epoch_{epoch+1}_auc_{reduced_auc_score:.5f}.pth\"\n",
        "                xm.save(model.state_dict(), file_name)\n",
        "                xm.master_print(f'saved model...')\n",
        "                xm.master_print(f'new best score: {best_score:.5f}')\n",
        "\n",
        "        xm.master_print(f'Epoch end {time.asctime(time.localtime())}')\n",
        "        xm.master_print('-'*27 + f'Epoch #{epoch+1} ended' + '-'*27)\n",
        "\n",
        "    xm.master_print('='*27 + f'Fold #{fold_no} ended' + '='*27)\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3F2uId8RkXf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _mp_fn(rank, flags, data, fold_no):\n",
        "    global FLAGS\n",
        "    global WRAPPED_MODEL\n",
        "    global SERIAL_EXEC\n",
        "    FLAGS = flags\n",
        "    torch.set_default_tensor_type('torch.FloatTensor')\n",
        "    train_model(data, fold_no)\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmLSHZZdS_1i",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHJtNpfOUdqD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dbea844b-c846-4d15-a7d9-623fb0ec6f32"
      },
      "source": [
        "FLAGS = {}\n",
        "FLAGS['batch_size'] = 32\n",
        "FLAGS['num_workers'] = 8\n",
        "FLAGS['learning_rate'] = 2e-4\n",
        "FLAGS['num_epochs'] = 10\n",
        "FLAGS['weight_decay'] = 1e-4\n",
        "FLAGS['log_steps'] = 45\n",
        "FLAGS['img_size'] = IMG_SIZE\n",
        "FLAGS['loss'] = 'BCE'\n",
        "FLAGS['optimizer'] = 'AdamW'\n",
        "FLAGS['exp_name'] = 'enet_b0'\n",
        "FLAGS['fold'] = 1\n",
        "FLAGS['val_freq'] = 1\n",
        "FLAGS['num_cores'] = 8\n",
        "\n",
        "WRAPPED_MODEL = xmp.MpModelWrapper(EfficientNet('tf_efficientnet_b0_ns'))\n",
        "SERIAL_EXEC = xmp.MpSerialExecutor()\n",
        "\n",
        "for fold_no in range(0, 6):\n",
        "    X_train = df_train[df_train['fold'] != fold_no][[col for col in df_train.columns if col != 'target']]\n",
        "    y_train = df_train[df_train['fold'] != fold_no][[col for col in df_train.columns if col == 'target']]\n",
        "    X_val = df_train[df_train['fold'] == fold_no][[col for col in df_train.columns if col != 'target']]\n",
        "    y_val = df_train[df_train['fold'] == fold_no][[col for col in df_train.columns if col == 'target']]\n",
        "    data = X_train, y_train, X_val, y_val\n",
        "    xmp.spawn(\n",
        "        _mp_fn, args=(FLAGS, data, fold_no), \n",
        "        nprocs=FLAGS['num_cores'], start_method='fork'\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==========================Fold #0 started===========================\n",
            "--------------------------Epoch #1 started--------------------------\n",
            "Epoch start Fri Jun 26 04:12:44 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tFri Jun 26 04:15:06 2020\n",
            "(90)\t|\tFri Jun 26 04:16:12 2020\n",
            "(135)\t|\tFri Jun 26 04:17:18 2020\n",
            "(180)\t|\tFri Jun 26 04:18:25 2020\n",
            "----------------------------------------\n",
            "finished training epoch 1\n",
            "elapsed time: 5mins 45s\n",
            "reduced loss 0.50609\n",
            "finished validating epoch 1\n",
            "reduced val loss 0.29496\n",
            "reduced auc score 0.91307\n",
            "elapsed time: 1mins 7s\n",
            "saved model...\n",
            "new best score: 0.913065164865634\n",
            "Epoch end Fri Jun 26 04:19:37 2020\n",
            "---------------------------Epoch #1 ended---------------------------\n",
            "--------------------------Epoch #2 started--------------------------\n",
            "Epoch start Fri Jun 26 04:19:37 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tFri Jun 26 04:21:00 2020\n",
            "(90)\t|\tFri Jun 26 04:22:04 2020\n",
            "(135)\t|\tFri Jun 26 04:23:10 2020\n",
            "(180)\t|\tFri Jun 26 04:24:16 2020\n",
            "----------------------------------------\n",
            "finished training epoch 2\n",
            "elapsed time: 4mins 44s\n",
            "reduced loss 0.43613\n",
            "finished validating epoch 2\n",
            "reduced val loss 0.31702\n",
            "reduced auc score 0.91214\n",
            "elapsed time: 1mins 2s\n",
            "Epoch end Fri Jun 26 04:25:24 2020\n",
            "---------------------------Epoch #2 ended---------------------------\n",
            "--------------------------Epoch #3 started--------------------------\n",
            "Epoch start Fri Jun 26 04:25:24 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tFri Jun 26 04:26:47 2020\n",
            "(90)\t|\tFri Jun 26 04:27:50 2020\n",
            "(135)\t|\tFri Jun 26 04:28:57 2020\n",
            "(180)\t|\tFri Jun 26 04:30:03 2020\n",
            "----------------------------------------\n",
            "finished training epoch 3\n",
            "elapsed time: 4mins 44s\n",
            "reduced loss 0.41227\n",
            "finished validating epoch 3\n",
            "reduced val loss 0.31290\n",
            "reduced auc score 0.90242\n",
            "elapsed time: 1mins 2s\n",
            "Epoch end Fri Jun 26 04:31:12 2020\n",
            "---------------------------Epoch #3 ended---------------------------\n",
            "--------------------------Epoch #4 started--------------------------\n",
            "Epoch start Fri Jun 26 04:31:12 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tFri Jun 26 04:32:33 2020\n",
            "(90)\t|\tFri Jun 26 04:33:40 2020\n",
            "(135)\t|\tFri Jun 26 04:34:46 2020\n",
            "(180)\t|\tFri Jun 26 04:35:50 2020\n",
            "----------------------------------------\n",
            "finished training epoch 4\n",
            "elapsed time: 4mins 46s\n",
            "reduced loss 0.39853\n",
            "finished validating epoch 4\n",
            "reduced val loss 0.38124\n",
            "reduced auc score 0.91731\n",
            "elapsed time: 1mins 3s\n",
            "saved model...\n",
            "new best score: 0.9173112974461093\n",
            "Epoch end Fri Jun 26 04:37:02 2020\n",
            "---------------------------Epoch #4 ended---------------------------\n",
            "--------------------------Epoch #5 started--------------------------\n",
            "Epoch start Fri Jun 26 04:37:02 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tFri Jun 26 04:38:24 2020\n",
            "(90)\t|\tFri Jun 26 04:39:30 2020\n",
            "(135)\t|\tFri Jun 26 04:40:35 2020\n",
            "(180)\t|\tFri Jun 26 04:41:40 2020\n",
            "----------------------------------------\n",
            "finished training epoch 5\n",
            "elapsed time: 4mins 45s\n",
            "reduced loss 0.38659\n",
            "finished validating epoch 5\n",
            "reduced val loss 0.30316\n",
            "reduced auc score 0.91629\n",
            "elapsed time: 1mins 2s\n",
            "Epoch end Fri Jun 26 04:42:50 2020\n",
            "---------------------------Epoch #5 ended---------------------------\n",
            "--------------------------Epoch #6 started--------------------------\n",
            "Epoch start Fri Jun 26 04:42:50 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tFri Jun 26 04:44:13 2020\n",
            "(90)\t|\tFri Jun 26 04:45:17 2020\n",
            "(135)\t|\tFri Jun 26 04:46:23 2020\n",
            "(180)\t|\tFri Jun 26 04:47:27 2020\n",
            "----------------------------------------\n",
            "finished training epoch 6\n",
            "elapsed time: 4mins 44s\n",
            "reduced loss 0.36964\n",
            "finished validating epoch 6\n",
            "reduced val loss 0.29679\n",
            "reduced auc score 0.91698\n",
            "elapsed time: 1mins 2s\n",
            "Epoch end Fri Jun 26 04:48:37 2020\n",
            "---------------------------Epoch #6 ended---------------------------\n",
            "--------------------------Epoch #7 started--------------------------\n",
            "Epoch start Fri Jun 26 04:48:37 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tFri Jun 26 04:49:59 2020\n",
            "(90)\t|\tFri Jun 26 04:51:03 2020\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hp14r-XHSBga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}