{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TPU SIIM-ISIC Melanoma Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "L-_fb9AEa6Wq",
        "wFoIORVvcWn5",
        "CwWrFw5C-R5J",
        "kFL68tLin0Ky",
        "Z9m9lx4ewYzk",
        "ndmsl1XUrWFp",
        "PPAHFSOoSiI2",
        "ruo8SHLGnZJU"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOqubwf+zGIuEATVZymt3+v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utsavnandi/Kaggle-SIIM-ISIC-Melanoma-Classification/blob/master/TPU_SIIM_ISIC_Melanoma_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-_fb9AEa6Wq",
        "colab_type": "text"
      },
      "source": [
        "## One-time\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tBeE-hte5Tl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import os\n",
        "# assert os.environ['COLAB_TPU_ADDR']\n",
        "# VERSION = \"nightly\"  #@param [\"1.5\" , \"20200516\", \"nightly\"]\n",
        "# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "# !python pytorch-xla-env-setup.py --version $VERSION # --apt-packages libomp5 libopenblas-dev\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymGpJBznaEZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%time\n",
        "# !pip uninstall kaggle -y\n",
        "# !pip install kaggle==1.5.6 -q\n",
        "# !pip install -U catalyst -q\n",
        "# !pip install -U git+https://github.com/albu/albumentations -q\n",
        "# !pip install -U git+https://github.com/rwightman/pytorch-image-models -q\n",
        "# !pip install -U git+https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer -q\n",
        "\n",
        "# !mkdir ~/.kaggle/\n",
        "# !cp ./kaggle.json  ~/.kaggle/kaggle.json\n",
        "# !chmod 600 ~/.kaggle/kaggle.json\n",
        "# !kaggle datasets download -d shonenkov/melanoma-merged-external-data-512x512-jpeg\n",
        "# !unzip melanoma-merged-external-data-512x512-jpeg.zip -d ./data/\n",
        "# !rm melanoma-merged-external-data-512x512-jpeg.zip\n",
        "# !kaggle competitions download siim-isic-melanoma-classification -f sample_submission.csv\n",
        "# !kaggle competitions download siim-isic-melanoma-classification -f test.csv\n",
        "# !kaggle competitions download siim-isic-melanoma-classification -f train.csv\n",
        "# !unzip train.csv -d ./data/\n",
        "# !mv ./test.csv ./data/\n",
        "# !mv ./sample_submission.csv ./data/\n",
        "# !rm train.csv.zip\n",
        "# !mkdir ./logs/\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFoIORVvcWn5",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cB029jhCcJFF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "d13312de-2980-41ae-afd6-46f3301ecdf4"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "#os.environ['XLA_USE_BF16'] = \"0\"\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pandas as pd\n",
        "\n",
        "#from google.colab import auth\n",
        "#from google.cloud import storage\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torchvision import transforms, models\n",
        "\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.utils.serialization as xser\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.utils.utils as xu\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "from ranger import Ranger\n",
        "from catalyst.data.sampler import DistributedSamplerWrapper, BalanceClassSampler\n",
        "import timm\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    xm.set_rng_state(seed, device=xm.xla_device())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning:\n",
            "\n",
            "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-XASBvOcVwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_DIR = '/content/data/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uG9Xunfy3N3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(DATA_DIR+'folds_13062020.csv')\n",
        "df_test = pd.read_csv(DATA_DIR+'test.csv').rename(columns={'image_name':'image_id'})\n",
        "sample_submission = pd.read_csv(DATA_DIR+'sample_submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIKzfKD2zA9A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "67a9e696-fc80-41c9-bdb1-c42cfa3e1a43"
      },
      "source": [
        "df_train['fold'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    11562\n",
              "4    11506\n",
              "0    11444\n",
              "1    11400\n",
              "2    11312\n",
              "Name: fold, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOuLmekTzlM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fold_no = 1\n",
        "X_train = df_train[df_train['fold'] != fold_no][[col for col in df_train.columns if col != 'target']]\n",
        "y_train = df_train[df_train['fold'] != fold_no][[col for col in df_train.columns if col == 'target']]\n",
        "X_val = df_train[df_train['fold'] == fold_no][[col for col in df_train.columns if col != 'target']]\n",
        "y_val = df_train[df_train['fold'] == fold_no][[col for col in df_train.columns if col == 'target']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yovLpNhMcvnW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "5f958027-a5ce-4e2a-874d-bdc1b8f13170"
      },
      "source": [
        "print('X_train', X_train.shape)\n",
        "print('y_train', y_train.shape)\n",
        "print('X_val', X_val.shape)\n",
        "print('y_val', y_val.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train (45824, 8)\n",
            "y_train (45824, 1)\n",
            "X_val (11400, 8)\n",
            "y_val (11400, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWae70vKk9dw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "d7f25005-d369-43fb-8e1a-0ba43f52d25a"
      },
      "source": [
        "print('Train target distribution: ')\n",
        "print(y_train['target'].value_counts())\n",
        "print('Val target distribution: ')\n",
        "print(y_val['target'].value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train target distribution: \n",
            "0    41910\n",
            "1     3914\n",
            "Name: target, dtype: int64\n",
            "Val target distribution: \n",
            "0    10392\n",
            "1     1008\n",
            "Name: target, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwWrFw5C-R5J",
        "colab_type": "text"
      },
      "source": [
        "##  Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wK-NTFy-PwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MelanomaDataset(Dataset):\n",
        "\n",
        "    def __init__(self, df, labels, istrain=False, transforms=None):\n",
        "        super().__init__()\n",
        "        self.image_id = df['image_id'].values\n",
        "        self.transforms = transforms\n",
        "        self.labels = labels.values\n",
        "        #self.neg_indices = np.where(self.labels==0)[0]\n",
        "        #self.pos_indices = np.where(self.labels==1)[0]\n",
        "        self.neg_indices = np.where(self.labels[:, 0] == 1)[0]\n",
        "        self.pos_indices = np.where(self.labels[:, 1] == 1)[0]\n",
        "        self.istrain = istrain\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_id)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if torch.is_tensor(index):\n",
        "            index = index.tolist()\n",
        "        \n",
        "        image, target = self.load_image(index)\n",
        "        \n",
        "        if not self.istrain:\n",
        "            if self.transforms:\n",
        "                image = self.transforms(image=image)['image']\n",
        "                return image, target\n",
        "\n",
        "        if np.random.random() < 0.33:\n",
        "            image, target = self.cutmix(image, target)\n",
        "        \n",
        "        if self.transforms:\n",
        "            image = self.transforms(image=image)['image']\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def load_image(self, index):\n",
        "        if torch.is_tensor(index):\n",
        "            index = index.tolist()\n",
        "        image_name = DATA_DIR + f'512x512-dataset-melanoma/512x512-dataset-melanoma/{self.image_id[index]}.jpg'\n",
        "        image = cv2.imread(image_name, cv2.IMREAD_COLOR).astype(np.uint8)\n",
        "        target = self.labels[index].astype(np.float32)\n",
        "        return image, target\n",
        "\n",
        "    def cutmix(self, data, target, alpha=3):\n",
        "        rand_index = self.get_rand_index()\n",
        "        random_image, random_target = self.load_image(rand_index)\n",
        "        h, w, _ = random_image.shape\n",
        "        ncx, ncy = w/2, h/2 \n",
        "        img_shape = (IMG_SIZE, IMG_SIZE, 3)\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "        bbx1, bby1, bbx2, bby2 = self.rand_bbox(data.shape, lam)\n",
        "        data[bby1:bby2, bbx1:bbx2 :] = random_image[bby1:bby2, bbx1:bbx2, :]\n",
        "        lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (data.shape[0] * data.shape[1]))\n",
        "        new_target = lam * target + (1-lam) * random_target\n",
        "        return data, new_target\n",
        "\n",
        "    def mixup(self, data, target, alpha=5):\n",
        "        rand_index = self.get_rand_index()\n",
        "        random_image, random_target = self.load_image(rand_index)\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "        data = data * lam + random_image * (1 - lam)\n",
        "        data = data.astype(np.uint8)\n",
        "        new_target = lam * target + (1-lam) * random_target\n",
        "        return data, new_target\n",
        "\n",
        "    def get_rand_index(self):\n",
        "        if random.random()>0.5:\n",
        "            rand_index = np.random.choice(self.pos_indices)\n",
        "        else:\n",
        "            rand_index = np.random.choice(self.neg_indices)\n",
        "        return rand_index\n",
        "\n",
        "    def rand_bbox(self, size, lam):\n",
        "        W = size[1]\n",
        "        H = size[0]\n",
        "        cut_rat = np.sqrt(1. - lam)\n",
        "        cut_w = np.int(W * cut_rat)\n",
        "        cut_h = np.int(H * cut_rat)\n",
        "        cx = np.random.randint(W)\n",
        "        cy = np.random.randint(H)\n",
        "        bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "        bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "        bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "        bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "        return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "# def get_datasets():\n",
        "#     datasets = {}\n",
        "#     datasets['train'] = MelanomaDataset(\n",
        "#         X_train, y_train, istrain=True, transforms=get_train_transforms()\n",
        "#     )\n",
        "#     datasets['valid'] = MelanomaDataset(\n",
        "#         X_val, y_val, istrain=False, transforms=get_valid_transforms()\n",
        "#     )\n",
        "#     return datasets\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFL68tLin0Ky",
        "colab_type": "text"
      },
      "source": [
        "## Augmentations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPWu-IIliVVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%writefile augmentations.txt\n",
        "# Reference IMG_SIZE\n",
        "# B0    - 224\n",
        "# B1    - 240\n",
        "# B2    - 260\n",
        "# B3    - 300\n",
        "# B4    - 380\n",
        "# B5    - 456\n",
        "# B6    - 520\n",
        "# B7    - 600\n",
        "# B8    - 672\n",
        "# L2 NS - 475\n",
        "# L2    - 800\n",
        "\n",
        "# Transforms\n",
        "IMG_SIZE = 300\n",
        "mean = (0.485, 0.456, 0.406)\n",
        "std = (0.229, 0.224, 0.225)\n",
        "\n",
        "def get_train_transforms(p=1.0):\n",
        "    return A.Compose([\n",
        "        A.OneOf([\n",
        "            A.CenterCrop(3*IMG_SIZE//4, 3*IMG_SIZE//4, p=0.5),\n",
        "            A.CenterCrop(4*IMG_SIZE//5, 4*IMG_SIZE//5, p=0.5),\n",
        "        ], p=0.2),\n",
        "        A.Resize(IMG_SIZE, IMG_SIZE, interpolation=1, always_apply=True, p=1),\n",
        "        A.Flip(),\n",
        "        A.Transpose(),\n",
        "        A.OneOf([\n",
        "            A.MedianBlur(blur_limit=3, p=0.5),\n",
        "            A.Blur(blur_limit=3, p=0.5),\n",
        "        ], p=0.5),\n",
        "        A.ShiftScaleRotate(\n",
        "            interpolation=1,\n",
        "            shift_limit=0.05, scale_limit=0.1, \n",
        "            rotate_limit=45, p=0.33\n",
        "        ),\n",
        "        A.OneOf([\n",
        "            A.OpticalDistortion(p=0.33),\n",
        "            A.GridDistortion(p=.33),\n",
        "            A.IAAPiecewiseAffine(p=0.33),\n",
        "        ], p=0.2),\n",
        "        A.OneOf([\n",
        "            A.HueSaturationValue(\n",
        "                hue_shift_limit=20, sat_shift_limit=30, \n",
        "                val_shift_limit=20, p=0.5\n",
        "            ),    \n",
        "            A.RandomBrightnessContrast(p=0.5),            \n",
        "        ], p=0.5),\n",
        "        A.CLAHE(clip_limit=(1,3), p=0.33),\n",
        "        A.MultiplicativeNoise(\n",
        "            multiplier=[0.9, 1.1], \n",
        "            elementwise=True, p=0.5\n",
        "        ),\n",
        "        A.Normalize(mean, std, max_pixel_value=255.0, always_apply=True),\n",
        "        ToTensorV2(p=1.0),\n",
        "    ], p=p)\n",
        "\n",
        "def get_valid_transforms():\n",
        "    return A.Compose([\n",
        "        A.Resize(IMG_SIZE, IMG_SIZE, interpolation=2, always_apply=True, p=1),\n",
        "        A.Normalize(mean, std, max_pixel_value=255.0, always_apply=True),\n",
        "        ToTensorV2(p=1.0),\n",
        "    ])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9m9lx4ewYzk",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92GalIsk0KFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gem(x, p=3, eps=1e-6):\n",
        "    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n",
        "\n",
        "class GeM(nn.Module):\n",
        "    def __init__(self, p=3, eps=1e-6):\n",
        "        super(GeM,self).__init__()\n",
        "        self.p = Parameter(torch.ones(1)*p)\n",
        "        self.eps = eps\n",
        "    def forward(self, x):\n",
        "        return gem(x, p=self.p, eps=self.eps)       \n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + ', ' + 'eps=' + str(self.eps) + ')'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfPBXkoYyPAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EfficientNet(nn.Module):\n",
        "\n",
        "    def __init__(self, name='tf_efficientnet_b0_ns'):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model(name, pretrained=True)\n",
        "        #self.model.global_pool = GeM()\n",
        "        in_features = self.model.classifier.in_features\n",
        "        self.model.classifier = nn.Linear(in_features, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class SEResNext50_32x4d(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model('gluon_seresnext50_32x4d', pretrained=True)\n",
        "        in_features = self.model.fc.in_features\n",
        "        self.model.fc = nn.Linear(in_features, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class SEResNext101_32x4d(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model('gluon_seresnext101_32x4d', pretrained=True)\n",
        "        in_features = self.model.fc.in_features\n",
        "        self.model.fc = nn.Linear(in_features, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndmsl1XUrWFp",
        "colab_type": "text"
      },
      "source": [
        "## Custom Losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IICyzNqJrZnl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "def sigmoid_focal_loss(\n",
        "    inputs: torch.Tensor,\n",
        "    targets: torch.Tensor,\n",
        "    alpha: float = 0.25,\n",
        "    gamma: float = 2,\n",
        "    reduction: str = \"none\"\n",
        "    ) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n",
        "    Args:\n",
        "        inputs: A float tensor of arbitrary shape.\n",
        "                The predictions for each example.\n",
        "        targets: A float tensor with the same shape as inputs. Stores the binary\n",
        "                 classification label for each element in inputs\n",
        "                (0 for the negative class and 1 for the positive class).\n",
        "        alpha: (optional) Weighting factor in range (0,1) to balance\n",
        "                positive vs negative examples. Default = -1 (no weighting).\n",
        "        gamma: Exponent of the modulating factor (1 - p_t) to\n",
        "               balance easy vs hard examples.\n",
        "        reduction: 'none' | 'mean' | 'sum'\n",
        "                 'none': No reduction will be applied to the output.\n",
        "                 'mean': The output will be averaged.\n",
        "                 'sum': The output will be summed.\n",
        "    Returns:\n",
        "        Loss tensor with the reduction option applied.\n",
        "    \"\"\"\n",
        "    p = torch.sigmoid(inputs)\n",
        "    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n",
        "    p_t = p * targets + (1 - p) * (1 - targets)\n",
        "    loss = ce_loss * ((1 - p_t) ** gamma)\n",
        "\n",
        "    if alpha >= 0:\n",
        "        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
        "        loss = alpha_t * loss\n",
        "\n",
        "    if reduction == \"mean\":\n",
        "        loss = loss.mean()\n",
        "    elif reduction == \"sum\":\n",
        "        loss = loss.sum()\n",
        "\n",
        "    return loss\n",
        "\n",
        "class CutMixCrossEntropyLoss(nn.Module):\n",
        "    def __init__(self, size_average=True):\n",
        "        super().__init__()\n",
        "        self.size_average = size_average\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        if len(target.size()) == 1:\n",
        "            target = torch.nn.functional.one_hot(target, num_classes=input.size(-1))\n",
        "            target = target.float()\n",
        "        return cross_entropy(input, target, self.size_average)\n",
        "\n",
        "def cross_entropy(input, target, size_average=True):\n",
        "    \"\"\" Cross entropy that accepts soft targets\n",
        "    Args:\n",
        "         pred: predictions for neural network\n",
        "         targets: targets, can be soft\n",
        "         size_average: if false, sum is returned instead of mean\n",
        "    Examples::\n",
        "        input = torch.FloatTensor([[1.1, 2.8, 1.3], [1.1, 2.1, 4.8]])\n",
        "        input = torch.autograd.Variable(out, requires_grad=True)\n",
        "        target = torch.FloatTensor([[0.05, 0.9, 0.05], [0.05, 0.05, 0.9]])\n",
        "        target = torch.autograd.Variable(y1)\n",
        "        loss = cross_entropy(input, target)\n",
        "        loss.backward()\n",
        "    \"\"\"\n",
        "    logsoftmax = torch.nn.LogSoftmax(dim=1)\n",
        "    if size_average:\n",
        "        return torch.mean(torch.sum(-target * logsoftmax(input), dim=1))\n",
        "    else:\n",
        "        return torch.sum(torch.sum(-target * logsoftmax(input), dim=1))\n",
        "\n",
        "def bce_criterion(y_pred, y_true):\n",
        "    return nn.BCEWithLogitsLoss()(y_pred, y_true)\n",
        "\n",
        "def cutmix_ce_criterion(y_pred, y_true):\n",
        "    return CutMixCrossEntropyLoss()(y_pred, y_true)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPAHFSOoSiI2",
        "colab_type": "text"
      },
      "source": [
        "## Metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmPxF8kEnTrY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RocAucMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.y_true = np.array([0,1])\n",
        "        self.y_pred = np.array([0.5,0.5])\n",
        "        self.score = 0\n",
        "\n",
        "    def update(self, y_true, y_pred):\n",
        "        y_true = y_true.cpu().numpy()\n",
        "        y_pred = torch.flatten(torch.sigmoid(y_pred)).data.cpu().numpy()\n",
        "        self.y_true = np.append(self.y_true, y_true)\n",
        "        self.y_pred = np.append(self.y_pred, y_pred)\n",
        "        self.score = roc_auc_score(self.y_true, self.y_pred)\n",
        "\n",
        "    @property\n",
        "    def avg(self):\n",
        "        return self.score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruo8SHLGnZJU",
        "colab_type": "text"
      },
      "source": [
        "## Train script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3N0J0GZYTUT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# optimizer = Ranger(\n",
        "#     model.parameters(),\n",
        "#     lr=FLAGS['learning_rate'] * xm.xrt_world_size(), \n",
        "#     alpha=0.5, k=6, N_sma_threshhold=5,\n",
        "#     weight_decay=FLAGS['weight_decay']\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TggTDLcOxUT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(data, fold_no):\n",
        "    seed_everything(43)\n",
        "\n",
        "    def get_datasets(data):\n",
        "        X_train, y_train, X_val, y_val = data\n",
        "        datasets = {}\n",
        "        datasets['train'] = MelanomaDataset(\n",
        "            X_train, y_train, istrain=True, transforms=get_train_transforms()\n",
        "        )\n",
        "        datasets['valid'] = MelanomaDataset(\n",
        "            X_val, y_val, istrain=False, transforms=get_valid_transforms()\n",
        "        )\n",
        "        return datasets\n",
        "\n",
        "    datasets = SERIAL_EXEC.run(\n",
        "        lambda: get_datasets(data)\n",
        "    )\n",
        "\n",
        "    labels_vcount = y_train['target_1'].value_counts()\n",
        "    class_counts = [labels_vcount[0].astype(np.float32), labels_vcount[1].astype(np.float32)]\n",
        "    num_samples = sum(class_counts)\n",
        "    class_weights = [num_samples/class_counts[i] for i in range(len(class_counts))]\n",
        "    weights = [class_weights[y_train['target_1'].values[i]] for i in range(int(num_samples))]\n",
        "    wrsampler = WeightedRandomSampler(\n",
        "        torch.DoubleTensor(weights), int(num_samples)\n",
        "    )\n",
        "    #BalanceClassSampler(labels=y_train['target'].values, mode=\"downsampling\"),\n",
        "\n",
        "    train_sampler = DistributedSamplerWrapper(\n",
        "        sampler=wrsampler,\n",
        "        num_replicas=xm.xrt_world_size(),\n",
        "        rank=xm.get_ordinal(),\n",
        "        shuffle=True\n",
        "    )\n",
        "    validation_sampler = DistributedSampler(\n",
        "        datasets['valid'],\n",
        "        num_replicas=xm.xrt_world_size(),\n",
        "        rank=xm.get_ordinal(),\n",
        "        shuffle=False\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        datasets['train'],\n",
        "        batch_size=FLAGS['batch_size'], \n",
        "        num_workers=FLAGS['num_workers'],\n",
        "        sampler=train_sampler,\n",
        "        drop_last=True,\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        datasets['valid'],\n",
        "        batch_size=FLAGS['batch_size'],\n",
        "        num_workers=FLAGS['num_workers'],\n",
        "        sampler=validation_sampler,\n",
        "        drop_last=True\n",
        "    )\n",
        "\n",
        "    device = xm.xla_device()\n",
        "    model = WRAPPED_MODEL.to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=FLAGS['learning_rate'] * xm.xrt_world_size(),\n",
        "        weight_decay=FLAGS['weight_decay']\n",
        "    )\n",
        "\n",
        "    criterion = cutmix_ce_criterion\n",
        " \n",
        "    def train_one_epoch(loader):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        max_idx = 0\n",
        "        xm.master_print('-'*40)\n",
        "        xm.master_print('Step\\t|\\tTime')\n",
        "        xm.master_print('-'*40)\n",
        "        for idx, (images, targets) in enumerate(loader):\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(images.float())\n",
        "            loss = criterion(y_pred, targets)\n",
        "            running_loss += float(loss)\n",
        "            loss.backward()\n",
        "            xm.optimizer_step(optimizer)\n",
        "            # xm.mark_step() call everystep for grad accum\n",
        "            max_idx = float(idx)\n",
        "            if idx % FLAGS['log_steps'] == 0 and idx !=0:\n",
        "                xm.master_print('({})\\t|\\t{}'.format(\n",
        "                    idx, time.asctime(time.localtime()))\n",
        "                )\n",
        "        xm.master_print('-'*40)\n",
        "        return running_loss/(max_idx+1)\n",
        "\n",
        "    def val_one_epoch(loader):\n",
        "        model.eval()\n",
        "        running_loss = 0\n",
        "        max_idx = 0\n",
        "        roc_auc_scores = RocAucMeter()\n",
        "        with torch.no_grad():\n",
        "            for idx, (images, targets) in enumerate(loader):\n",
        "                y_pred = model(images.float())\n",
        "                loss = criterion(y_pred, targets)\n",
        "                running_loss += float(loss)\n",
        "                max_idx = float(idx)\n",
        "                roc_auc_scores.update(targets[:, 1], y_pred[:, 1])\n",
        "        score = roc_auc_scores.avg\n",
        "        return running_loss/(max_idx+1), score\n",
        "\n",
        "    def _reduce_fn(x):\n",
        "        return np.array(x).mean()\n",
        "\n",
        "    best_score = 0\n",
        "    xm.master_print('='*26 + f'Fold #{fold_no} started' + '='*27)\n",
        "    for epoch in range(0, FLAGS['num_epochs']):\n",
        "        xm.master_print('-'*26 + f'Epoch #{epoch+1} started' + '-'*26)\n",
        "        xm.master_print(f'Epoch start {time.asctime(time.localtime())}')\n",
        "        train_start = time.time()\n",
        "        para_loader = pl.ParallelLoader(train_loader, [device])\n",
        "        train_loss = train_one_epoch(para_loader.per_device_loader(device))\n",
        "        xm.master_print(f\"finished training epoch {epoch+1}\")\n",
        "        elapsed_time = int(time.time() - train_start)\n",
        "        xm.master_print(\n",
        "            f'elapsed time: {(elapsed_time)//60}mins {(elapsed_time)%60}s'\n",
        "        )\n",
        "        reduced_loss = xm.mesh_reduce('train_loss', train_loss, _reduce_fn)\n",
        "        xm.master_print(f\"reduced loss {reduced_loss:.5f}\")\n",
        "        if (epoch+1) % FLAGS['val_freq'] == 0:\n",
        "            val_start = time.time()\n",
        "            para_loader = pl.ParallelLoader(val_loader, [device])\n",
        "            val_loss, auc_score = val_one_epoch(para_loader.per_device_loader(device))    \n",
        "            xm.master_print(f\"finished validating epoch {epoch+1}\")\n",
        "            reduced_val_loss = xm.mesh_reduce('val_loss', val_loss, _reduce_fn)\n",
        "            reduced_auc_score = xm.mesh_reduce('auc_score', auc_score, _reduce_fn)\n",
        "            xm.master_print(f\"reduced val loss {reduced_val_loss:.5f}\")\n",
        "            xm.master_print(f\"reduced auc score {reduced_auc_score:.5f}\")\n",
        "            val_elapsed_time = int(time.time() - val_start)\n",
        "            xm.master_print(\n",
        "                f'elapsed time: {(val_elapsed_time)//60}mins {(val_elapsed_time)%60}s'\n",
        "            )\n",
        "            if best_score < reduced_auc_score:\n",
        "                best_score = reduced_auc_score\n",
        "                file_name = f\"./{FLAGS['exp_name']}_fold_{fold_no+1}_epoch_{epoch+1}_auc_{reduced_auc_score:.5f}.pth\"\n",
        "                #xm.save(model.state_dict(), file_name)\n",
        "                xser.save(model.state_dict(), file_name, master_only=True)\n",
        "                xm.master_print(f'saved model...')\n",
        "                xm.master_print(f'new best score: {best_score:.5f}')\n",
        "\n",
        "        xm.master_print(f'Epoch end {time.asctime(time.localtime())}')\n",
        "        xm.master_print('-'*27 + f'Epoch #{epoch+1} ended' + '-'*26)\n",
        "\n",
        "    xm.master_print('='*28 + f'Fold #{fold_no} ended' + '='*27)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3F2uId8RkXf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _mp_fn(rank, flags, data, fold_no):\n",
        "    global FLAGS\n",
        "    global WRAPPED_MODEL\n",
        "    global SERIAL_EXEC\n",
        "    FLAGS = flags\n",
        "    torch.set_default_tensor_type('torch.FloatTensor')\n",
        "    train_model(data, fold_no)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmLSHZZdS_1i",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHJtNpfOUdqD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3a1e44ac-089f-4f26-e953-0edebb8225e5"
      },
      "source": [
        "FLAGS = {}\n",
        "FLAGS['batch_size'] = 32\n",
        "FLAGS['num_workers'] = 8\n",
        "FLAGS['learning_rate'] = 2e-4\n",
        "FLAGS['num_epochs'] = 10\n",
        "FLAGS['weight_decay'] = 1e-4\n",
        "FLAGS['log_steps'] = 40\n",
        "FLAGS['img_size'] = IMG_SIZE\n",
        "FLAGS['loss'] = 'BCE'\n",
        "FLAGS['optimizer'] = 'AdamW'\n",
        "FLAGS['exp_name'] = 'enet_b0'\n",
        "FLAGS['fold'] = 1\n",
        "FLAGS['val_freq'] = 1\n",
        "FLAGS['num_cores'] = 8\n",
        "\n",
        "WRAPPED_MODEL = xmp.MpModelWrapper(EfficientNet('tf_efficientnet_b0_ns'))\n",
        "SERIAL_EXEC = xmp.MpSerialExecutor()\n",
        "\n",
        "for fold_no in range(0, 5):\n",
        "    X_train = df_train[df_train['fold'] != fold_no][[col for col in df_train.columns if col != 'target']]\n",
        "    y_train = pd.get_dummies(df_train[df_train['fold'] != fold_no][[col for col in df_train.columns if col == 'target']], columns=['target'])\n",
        "    X_val = df_train[df_train['fold'] == fold_no][[col for col in df_train.columns if col != 'target']]\n",
        "    y_val = pd.get_dummies(df_train[df_train['fold'] == fold_no][[col for col in df_train.columns if col == 'target']], columns=['target'])\n",
        "    data = X_train, y_train, X_val, y_val\n",
        "    xmp.spawn(\n",
        "        _mp_fn, args=(FLAGS, data, fold_no), \n",
        "        nprocs=FLAGS['num_cores'], start_method='fork'\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b0_ns-c0e6a31c.pth\" to /root/.cache/torch/hub/checkpoints/tf_efficientnet_b0_ns-c0e6a31c.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "==========================Fold #0 started===========================\n",
            "--------------------------Epoch #1 started--------------------------\n",
            "Epoch start Sat Jun 27 10:01:36 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tSat Jun 27 10:04:10 2020\n",
            "(90)\t|\tSat Jun 27 10:05:50 2020\n",
            "(135)\t|\tSat Jun 27 10:07:37 2020\n",
            "----------------------------------------\n",
            "finished training epoch 1\n",
            "elapsed time: 7mins 32s\n",
            "reduced loss 0.48487\n",
            "finished validating epoch 1\n",
            "reduced val loss 0.38097\n",
            "reduced auc score 0.91115\n",
            "elapsed time: 1mins 47s\n",
            "saved model...\n",
            "new best score: 0.91115\n",
            "Epoch end Sat Jun 27 10:10:56 2020\n",
            "---------------------------Epoch #1 ended---------------------------\n",
            "--------------------------Epoch #2 started--------------------------\n",
            "Epoch start Sat Jun 27 10:10:56 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tSat Jun 27 10:13:01 2020\n",
            "(90)\t|\tSat Jun 27 10:14:48 2020\n",
            "(135)\t|\tSat Jun 27 10:16:31 2020\n",
            "----------------------------------------\n",
            "finished training epoch 2\n",
            "elapsed time: 7mins 6s\n",
            "reduced loss 0.44000\n",
            "finished validating epoch 2\n",
            "reduced val loss 0.30148\n",
            "reduced auc score 0.90708\n",
            "elapsed time: 1mins 46s\n",
            "Epoch end Sat Jun 27 10:19:49 2020\n",
            "---------------------------Epoch #2 ended---------------------------\n",
            "--------------------------Epoch #3 started--------------------------\n",
            "Epoch start Sat Jun 27 10:19:49 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tSat Jun 27 10:21:53 2020\n",
            "(90)\t|\tSat Jun 27 10:23:37 2020\n",
            "(135)\t|\tSat Jun 27 10:25:21 2020\n",
            "----------------------------------------\n",
            "finished training epoch 3\n",
            "elapsed time: 7mins 3s\n",
            "reduced loss 0.42506\n",
            "finished validating epoch 3\n",
            "reduced val loss 0.29574\n",
            "reduced auc score 0.91276\n",
            "elapsed time: 1mins 45s\n",
            "saved model...\n",
            "new best score: 0.91276\n",
            "Epoch end Sat Jun 27 10:28:38 2020\n",
            "---------------------------Epoch #3 ended---------------------------\n",
            "--------------------------Epoch #4 started--------------------------\n",
            "Epoch start Sat Jun 27 10:28:38 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tSat Jun 27 10:30:45 2020\n",
            "(90)\t|\tSat Jun 27 10:32:27 2020\n",
            "(135)\t|\tSat Jun 27 10:34:11 2020\n",
            "----------------------------------------\n",
            "finished training epoch 4\n",
            "elapsed time: 7mins 4s\n",
            "reduced loss 0.41113\n",
            "finished validating epoch 4\n",
            "reduced val loss 0.28028\n",
            "reduced auc score 0.91763\n",
            "elapsed time: 1mins 45s\n",
            "saved model...\n",
            "new best score: 0.91763\n",
            "Epoch end Sat Jun 27 10:37:29 2020\n",
            "---------------------------Epoch #4 ended---------------------------\n",
            "--------------------------Epoch #5 started--------------------------\n",
            "Epoch start Sat Jun 27 10:37:29 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tSat Jun 27 10:39:37 2020\n",
            "(90)\t|\tSat Jun 27 10:41:19 2020\n",
            "(135)\t|\tSat Jun 27 10:43:01 2020\n",
            "----------------------------------------\n",
            "finished training epoch 5\n",
            "elapsed time: 7mins 4s\n",
            "reduced loss 0.39640\n",
            "finished validating epoch 5\n",
            "reduced val loss 0.37772\n",
            "reduced auc score 0.89867\n",
            "elapsed time: 1mins 45s\n",
            "Epoch end Sat Jun 27 10:46:19 2020\n",
            "---------------------------Epoch #5 ended---------------------------\n",
            "--------------------------Epoch #6 started--------------------------\n",
            "Epoch start Sat Jun 27 10:46:19 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tSat Jun 27 10:48:22 2020\n",
            "(90)\t|\tSat Jun 27 10:50:08 2020\n",
            "(135)\t|\tSat Jun 27 10:51:50 2020\n",
            "----------------------------------------\n",
            "finished training epoch 6\n",
            "elapsed time: 7mins 4s\n",
            "reduced loss 0.38727\n",
            "finished validating epoch 6\n",
            "reduced val loss 0.29943\n",
            "reduced auc score 0.92054\n",
            "elapsed time: 1mins 45s\n",
            "saved model...\n",
            "new best score: 0.92054\n",
            "Epoch end Sat Jun 27 10:55:09 2020\n",
            "---------------------------Epoch #6 ended---------------------------\n",
            "--------------------------Epoch #7 started--------------------------\n",
            "Epoch start Sat Jun 27 10:55:09 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tSat Jun 27 10:57:16 2020\n",
            "(90)\t|\tSat Jun 27 10:58:59 2020\n",
            "(135)\t|\tSat Jun 27 11:00:41 2020\n",
            "----------------------------------------\n",
            "finished training epoch 7\n",
            "elapsed time: 7mins 5s\n",
            "reduced loss 0.38525\n",
            "finished validating epoch 7\n",
            "reduced val loss 0.31505\n",
            "reduced auc score 0.90192\n",
            "elapsed time: 1mins 46s\n",
            "Epoch end Sat Jun 27 11:04:00 2020\n",
            "---------------------------Epoch #7 ended---------------------------\n",
            "--------------------------Epoch #8 started--------------------------\n",
            "Epoch start Sat Jun 27 11:04:00 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tSat Jun 27 11:06:05 2020\n",
            "(90)\t|\tSat Jun 27 11:07:51 2020\n",
            "(135)\t|\tSat Jun 27 11:09:35 2020\n",
            "----------------------------------------\n",
            "finished training epoch 8\n",
            "elapsed time: 7mins 6s\n",
            "reduced loss 0.36541\n",
            "finished validating epoch 8\n",
            "reduced val loss 0.31466\n",
            "reduced auc score 0.91052\n",
            "elapsed time: 1mins 45s\n",
            "Epoch end Sat Jun 27 11:12:53 2020\n",
            "---------------------------Epoch #8 ended---------------------------\n",
            "--------------------------Epoch #9 started--------------------------\n",
            "Epoch start Sat Jun 27 11:12:53 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tSat Jun 27 11:14:59 2020\n",
            "(90)\t|\tSat Jun 27 11:16:45 2020\n",
            "(135)\t|\tSat Jun 27 11:18:26 2020\n",
            "----------------------------------------\n",
            "finished training epoch 9\n",
            "elapsed time: 7mins 6s\n",
            "reduced loss 0.35960\n",
            "finished validating epoch 9\n",
            "reduced val loss 0.32113\n",
            "reduced auc score 0.89971\n",
            "elapsed time: 1mins 46s\n",
            "Epoch end Sat Jun 27 11:21:45 2020\n",
            "---------------------------Epoch #9 ended---------------------------\n",
            "--------------------------Epoch #10 started--------------------------\n",
            "Epoch start Sat Jun 27 11:21:45 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tSat Jun 27 11:23:50 2020\n",
            "(90)\t|\tSat Jun 27 11:25:32 2020\n",
            "(135)\t|\tSat Jun 27 11:27:18 2020\n",
            "----------------------------------------\n",
            "finished training epoch 10\n",
            "elapsed time: 7mins 4s\n",
            "reduced loss 0.35118\n",
            "finished validating epoch 10\n",
            "reduced val loss 0.29400\n",
            "reduced auc score 0.91391\n",
            "elapsed time: 1mins 45s\n",
            "Epoch end Sat Jun 27 11:30:36 2020\n",
            "---------------------------Epoch #10 ended---------------------------\n",
            "===========================Fold #0 ended===========================\n",
            "==========================Fold #1 started===========================\n",
            "--------------------------Epoch #1 started--------------------------\n",
            "Epoch start Sat Jun 27 11:31:36 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tSat Jun 27 11:33:44 2020\n",
            "(90)\t|\tSat Jun 27 11:35:27 2020\n",
            "(135)\t|\tSat Jun 27 11:37:13 2020\n",
            "----------------------------------------\n",
            "finished training epoch 1\n",
            "elapsed time: 7mins 10s\n",
            "reduced loss 0.48623\n",
            "finished validating epoch 1\n",
            "reduced val loss 0.36556\n",
            "reduced auc score 0.90901\n",
            "elapsed time: 1mins 45s\n",
            "saved model...\n",
            "new best score: 0.90901\n",
            "Epoch end Sat Jun 27 11:40:32 2020\n",
            "---------------------------Epoch #1 ended---------------------------\n",
            "--------------------------Epoch #2 started--------------------------\n",
            "Epoch start Sat Jun 27 11:40:32 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tSat Jun 27 11:42:37 2020\n",
            "(90)\t|\tSat Jun 27 11:44:22 2020\n",
            "(135)\t|\tSat Jun 27 11:46:04 2020\n",
            "----------------------------------------\n",
            "finished training epoch 2\n",
            "elapsed time: 7mins 6s\n",
            "reduced loss 0.44367\n",
            "finished validating epoch 2\n",
            "reduced val loss 0.27203\n",
            "reduced auc score 0.92658\n",
            "elapsed time: 1mins 45s\n",
            "saved model...\n",
            "new best score: 0.92658\n",
            "Epoch end Sat Jun 27 11:49:24 2020\n",
            "---------------------------Epoch #2 ended---------------------------\n",
            "--------------------------Epoch #3 started--------------------------\n",
            "Epoch start Sat Jun 27 11:49:24 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tSat Jun 27 11:51:28 2020\n",
            "(90)\t|\tSat Jun 27 11:53:14 2020\n",
            "(135)\t|\tSat Jun 27 11:54:57 2020\n",
            "----------------------------------------\n",
            "finished training epoch 3\n",
            "elapsed time: 7mins 7s\n",
            "reduced loss 0.42913\n",
            "finished validating epoch 3\n",
            "reduced val loss 0.32679\n",
            "reduced auc score 0.92070\n",
            "elapsed time: 1mins 45s\n",
            "Epoch end Sat Jun 27 11:58:17 2020\n",
            "---------------------------Epoch #3 ended---------------------------\n",
            "--------------------------Epoch #4 started--------------------------\n",
            "Epoch start Sat Jun 27 11:58:17 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tSat Jun 27 12:00:20 2020\n",
            "(90)\t|\tSat Jun 27 12:02:06 2020\n",
            "(135)\t|\tSat Jun 27 12:03:51 2020\n",
            "----------------------------------------\n",
            "finished training epoch 4\n",
            "elapsed time: 7mins 7s\n",
            "reduced loss 0.41473\n",
            "finished validating epoch 4\n",
            "reduced val loss 0.28129\n",
            "reduced auc score 0.93297\n",
            "elapsed time: 1mins 46s\n",
            "saved model...\n",
            "new best score: 0.93297\n",
            "Epoch end Sat Jun 27 12:07:11 2020\n",
            "---------------------------Epoch #4 ended---------------------------\n",
            "--------------------------Epoch #5 started--------------------------\n",
            "Epoch start Sat Jun 27 12:07:11 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tSat Jun 27 12:09:17 2020\n",
            "(90)\t|\tSat Jun 27 12:10:59 2020\n",
            "(135)\t|\tSat Jun 27 12:12:43 2020\n",
            "----------------------------------------\n",
            "finished training epoch 5\n",
            "elapsed time: 7mins 7s\n",
            "reduced loss 0.39885\n",
            "finished validating epoch 5\n",
            "reduced val loss 0.31234\n",
            "reduced auc score 0.92436\n",
            "elapsed time: 1mins 45s\n",
            "Epoch end Sat Jun 27 12:16:04 2020\n",
            "---------------------------Epoch #5 ended---------------------------\n",
            "--------------------------Epoch #6 started--------------------------\n",
            "Epoch start Sat Jun 27 12:16:04 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tSat Jun 27 12:18:10 2020\n",
            "(90)\t|\tSat Jun 27 12:19:53 2020\n",
            "(135)\t|\tSat Jun 27 12:21:35 2020\n",
            "----------------------------------------\n",
            "finished training epoch 6\n",
            "elapsed time: 7mins 6s\n",
            "reduced loss 0.39232\n",
            "finished validating epoch 6\n",
            "reduced val loss 0.27713\n",
            "reduced auc score 0.93369\n",
            "elapsed time: 1mins 45s\n",
            "saved model...\n",
            "new best score: 0.93369\n",
            "Epoch end Sat Jun 27 12:24:56 2020\n",
            "---------------------------Epoch #6 ended---------------------------\n",
            "--------------------------Epoch #7 started--------------------------\n",
            "Epoch start Sat Jun 27 12:24:56 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tSat Jun 27 12:27:01 2020\n",
            "(90)\t|\tSat Jun 27 12:28:45 2020\n",
            "(135)\t|\tSat Jun 27 12:30:28 2020\n",
            "----------------------------------------\n",
            "finished training epoch 7\n",
            "elapsed time: 7mins 6s\n",
            "reduced loss 0.37932\n",
            "finished validating epoch 7\n",
            "reduced val loss 0.33273\n",
            "reduced auc score 0.93141\n",
            "elapsed time: 1mins 45s\n",
            "Epoch end Sat Jun 27 12:33:48 2020\n",
            "---------------------------Epoch #7 ended---------------------------\n",
            "--------------------------Epoch #8 started--------------------------\n",
            "Epoch start Sat Jun 27 12:33:48 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tSat Jun 27 12:35:56 2020\n",
            "(90)\t|\tSat Jun 27 12:37:37 2020\n",
            "(135)\t|\tSat Jun 27 12:39:20 2020\n",
            "----------------------------------------\n",
            "finished training epoch 8\n",
            "elapsed time: 7mins 7s\n",
            "reduced loss 0.37521\n",
            "finished validating epoch 8\n",
            "reduced val loss 0.28626\n",
            "reduced auc score 0.93141\n",
            "elapsed time: 1mins 45s\n",
            "Epoch end Sat Jun 27 12:42:41 2020\n",
            "---------------------------Epoch #8 ended---------------------------\n",
            "--------------------------Epoch #9 started--------------------------\n",
            "Epoch start Sat Jun 27 12:42:41 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tSat Jun 27 12:44:47 2020\n",
            "(90)\t|\tSat Jun 27 12:46:31 2020\n",
            "(135)\t|\tSat Jun 27 12:48:15 2020\n",
            "----------------------------------------\n",
            "finished training epoch 9\n",
            "elapsed time: 7mins 6s\n",
            "reduced loss 0.36521\n",
            "finished validating epoch 9\n",
            "reduced val loss 0.24545\n",
            "reduced auc score 0.93510\n",
            "elapsed time: 1mins 45s\n",
            "saved model...\n",
            "new best score: 0.93510\n",
            "Epoch end Sat Jun 27 12:51:33 2020\n",
            "---------------------------Epoch #9 ended---------------------------\n",
            "--------------------------Epoch #10 started--------------------------\n",
            "Epoch start Sat Jun 27 12:51:33 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tSat Jun 27 12:53:38 2020\n",
            "(90)\t|\tSat Jun 27 12:55:22 2020\n",
            "(135)\t|\tSat Jun 27 12:57:07 2020\n",
            "----------------------------------------\n",
            "finished training epoch 10\n",
            "elapsed time: 7mins 6s\n",
            "reduced loss 0.35441\n",
            "finished validating epoch 10\n",
            "reduced val loss 0.25103\n",
            "reduced auc score 0.92776\n",
            "elapsed time: 1mins 46s\n",
            "Epoch end Sat Jun 27 13:00:26 2020\n",
            "---------------------------Epoch #10 ended---------------------------\n",
            "===========================Fold #1 ended===========================\n",
            "==========================Fold #2 started===========================\n",
            "--------------------------Epoch #1 started--------------------------\n",
            "Epoch start Sat Jun 27 13:00:41 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tSat Jun 27 13:02:46 2020\n",
            "(90)\t|\tSat Jun 27 13:04:35 2020\n",
            "(135)\t|\tSat Jun 27 13:06:18 2020\n",
            "----------------------------------------\n",
            "finished training epoch 1\n",
            "elapsed time: 7mins 11s\n",
            "reduced loss 0.48212\n",
            "finished validating epoch 1\n",
            "reduced val loss 0.41820\n",
            "reduced auc score 0.90570\n",
            "elapsed time: 1mins 45s\n",
            "saved model...\n",
            "new best score: 0.90570\n",
            "Epoch end Sat Jun 27 13:09:38 2020\n",
            "---------------------------Epoch #1 ended---------------------------\n",
            "--------------------------Epoch #2 started--------------------------\n",
            "Epoch start Sat Jun 27 13:09:38 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tSat Jun 27 13:11:44 2020\n",
            "(90)\t|\tSat Jun 27 13:13:28 2020\n",
            "(135)\t|\tSat Jun 27 13:15:13 2020\n",
            "----------------------------------------\n",
            "finished training epoch 2\n",
            "elapsed time: 7mins 7s\n",
            "reduced loss 0.44465\n",
            "finished validating epoch 2\n",
            "reduced val loss 0.45774\n",
            "reduced auc score 0.91230\n",
            "elapsed time: 1mins 45s\n",
            "saved model...\n",
            "new best score: 0.91230\n",
            "Epoch end Sat Jun 27 13:18:32 2020\n",
            "---------------------------Epoch #2 ended---------------------------\n",
            "--------------------------Epoch #3 started--------------------------\n",
            "Epoch start Sat Jun 27 13:18:32 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tSat Jun 27 13:20:39 2020\n",
            "(90)\t|\tSat Jun 27 13:22:22 2020\n",
            "(135)\t|\tSat Jun 27 13:24:06 2020\n",
            "----------------------------------------\n",
            "finished training epoch 3\n",
            "elapsed time: 7mins 7s\n",
            "reduced loss 0.42431\n",
            "finished validating epoch 3\n",
            "reduced val loss 0.36118\n",
            "reduced auc score 0.91847\n",
            "elapsed time: 1mins 45s\n",
            "saved model...\n",
            "new best score: 0.91847\n",
            "Epoch end Sat Jun 27 13:27:25 2020\n",
            "---------------------------Epoch #3 ended---------------------------\n",
            "--------------------------Epoch #4 started--------------------------\n",
            "Epoch start Sat Jun 27 13:27:25 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tSat Jun 27 13:29:27 2020\n",
            "(90)\t|\tSat Jun 27 13:31:15 2020\n",
            "(135)\t|\tSat Jun 27 13:32:58 2020\n",
            "----------------------------------------\n",
            "finished training epoch 4\n",
            "elapsed time: 7mins 7s\n",
            "reduced loss 0.41635\n",
            "finished validating epoch 4\n",
            "reduced val loss 0.33419\n",
            "reduced auc score 0.92218\n",
            "elapsed time: 1mins 45s\n",
            "saved model...\n",
            "new best score: 0.92218\n",
            "Epoch end Sat Jun 27 13:36:18 2020\n",
            "---------------------------Epoch #4 ended---------------------------\n",
            "--------------------------Epoch #5 started--------------------------\n",
            "Epoch start Sat Jun 27 13:36:18 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tSat Jun 27 13:38:25 2020\n",
            "(90)\t|\tSat Jun 27 13:40:07 2020\n",
            "(135)\t|\tSat Jun 27 13:41:51 2020\n",
            "----------------------------------------\n",
            "finished training epoch 5\n",
            "elapsed time: 7mins 7s\n",
            "reduced loss 0.40327\n",
            "finished validating epoch 5\n",
            "reduced val loss 0.34762\n",
            "reduced auc score 0.92142\n",
            "elapsed time: 1mins 45s\n",
            "Epoch end Sat Jun 27 13:45:11 2020\n",
            "---------------------------Epoch #5 ended---------------------------\n",
            "--------------------------Epoch #6 started--------------------------\n",
            "Epoch start Sat Jun 27 13:45:11 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tSat Jun 27 13:47:17 2020\n",
            "(90)\t|\tSat Jun 27 13:48:58 2020\n",
            "(135)\t|\tSat Jun 27 13:50:43 2020\n",
            "----------------------------------------\n",
            "finished training epoch 6\n",
            "elapsed time: 7mins 6s\n",
            "reduced loss 0.39213\n",
            "finished validating epoch 6\n",
            "reduced val loss 0.24628\n",
            "reduced auc score 0.92961\n",
            "elapsed time: 1mins 45s\n",
            "saved model...\n",
            "new best score: 0.92961\n",
            "Epoch end Sat Jun 27 13:54:03 2020\n",
            "---------------------------Epoch #6 ended---------------------------\n",
            "--------------------------Epoch #7 started--------------------------\n",
            "Epoch start Sat Jun 27 13:54:03 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tSat Jun 27 13:56:08 2020\n",
            "(90)\t|\tSat Jun 27 13:57:52 2020\n",
            "(135)\t|\tSat Jun 27 13:59:36 2020\n",
            "----------------------------------------\n",
            "finished training epoch 7\n",
            "elapsed time: 7mins 6s\n",
            "reduced loss 0.37928\n",
            "finished validating epoch 7\n",
            "reduced val loss 0.39312\n",
            "reduced auc score 0.92704\n",
            "elapsed time: 1mins 46s\n",
            "Epoch end Sat Jun 27 14:02:56 2020\n",
            "---------------------------Epoch #7 ended---------------------------\n",
            "--------------------------Epoch #8 started--------------------------\n",
            "Epoch start Sat Jun 27 14:02:56 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tSat Jun 27 14:05:04 2020\n",
            "(90)\t|\tSat Jun 27 14:06:46 2020\n",
            "(135)\t|\tSat Jun 27 14:08:30 2020\n",
            "----------------------------------------\n",
            "finished training epoch 8\n",
            "elapsed time: 7mins 8s\n",
            "reduced loss 0.37125\n",
            "finished validating epoch 8\n",
            "reduced val loss 0.28831\n",
            "reduced auc score 0.92470\n",
            "elapsed time: 1mins 45s\n",
            "Epoch end Sat Jun 27 14:11:50 2020\n",
            "---------------------------Epoch #8 ended---------------------------\n",
            "--------------------------Epoch #9 started--------------------------\n",
            "Epoch start Sat Jun 27 14:11:50 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tSat Jun 27 14:13:58 2020\n",
            "(90)\t|\tSat Jun 27 14:15:43 2020\n",
            "(135)\t|\tSat Jun 27 14:17:26 2020\n",
            "----------------------------------------\n",
            "finished training epoch 9\n",
            "elapsed time: 7mins 8s\n",
            "reduced loss 0.36188\n",
            "finished validating epoch 9\n",
            "reduced val loss 0.28745\n",
            "reduced auc score 0.93054\n",
            "elapsed time: 1mins 45s\n",
            "saved model...\n",
            "new best score: 0.93054\n",
            "Epoch end Sat Jun 27 14:20:44 2020\n",
            "---------------------------Epoch #9 ended---------------------------\n",
            "--------------------------Epoch #10 started--------------------------\n",
            "Epoch start Sat Jun 27 14:20:44 2020\n",
            "----------------------------------------\n",
            "Step\t|\tTime\n",
            "----------------------------------------\n",
            "(45)\t|\tSat Jun 27 14:22:49 2020\n",
            "(90)\t|\tSat Jun 27 14:24:34 2020\n",
            "(135)\t|\tSat Jun 27 14:26:17 2020\n",
            "----------------------------------------\n",
            "finished training epoch 10\n",
            "elapsed time: 7mins 6s\n",
            "reduced loss 0.35686\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hp14r-XHSBga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()\n",
        "PROJECT_ID = 'utsav-37'\n",
        "!gcloud config set project {PROJECT_ID}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uLwlwayB4zP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}