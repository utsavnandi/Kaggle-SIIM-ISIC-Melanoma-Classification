{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TPU SIIM-ISIC Melanoma Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "L-_fb9AEa6Wq",
        "wFoIORVvcWn5",
        "CwWrFw5C-R5J",
        "kFL68tLin0Ky"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMtcuRDdAPGdwkgslycK2b7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utsavnandi/Kaggle-SIIM-ISIC-Melanoma-Classification/blob/master/TPU_SIIM_ISIC_Melanoma_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-_fb9AEa6Wq",
        "colab_type": "text"
      },
      "source": [
        "## One-time\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tBeE-hte5Tl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import os\n",
        "# assert os.environ['COLAB_TPU_ADDR']\n",
        "# VERSION = \"nightly\"  #@param [\"1.5\" , \"20200516\", \"nightly\"]\n",
        "# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "# !python pytorch-xla-env-setup.py --version $VERSION # --apt-packages libomp5 libopenblas-dev\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymGpJBznaEZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%time\n",
        "# !pip uninstall kaggle -y\n",
        "# !pip install kaggle==1.5.6 -q\n",
        "# !pip install -U catalyst -q\n",
        "# !pip install -U git+https://github.com/albu/albumentations -q\n",
        "# !pip install -U git+https://github.com/rwightman/pytorch-image-models -q\n",
        "# !pip install -U git+https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer -q\n",
        "# !pip install -U git+https://github.com/PyTorchLightning/pytorch-lightning -q\n",
        "# !pip install neptune-client -q\n",
        "# !mkdir ~/.kaggle/\n",
        "# !cp ./kaggle.json  ~/.kaggle/kaggle.json\n",
        "# !chmod 600 ~/.kaggle/kaggle.json\n",
        "# !kaggle datasets download -d shonenkov/melanoma-merged-external-data-512x512-jpeg\n",
        "# !unzip melanoma-merged-external-data-512x512-jpeg.zip -d ./data/\n",
        "# !rm melanoma-merged-external-data-512x512-jpeg.zip\n",
        "# !kaggle competitions download siim-isic-melanoma-classification -f sample_submission.csv\n",
        "# !kaggle competitions download siim-isic-melanoma-classification -f test.csv\n",
        "# !kaggle competitions download siim-isic-melanoma-classification -f train.csv\n",
        "# !unzip train.csv -d ./data/\n",
        "# !mv ./test.csv ./data/\n",
        "# !mv ./sample_submission.csv ./data/\n",
        "# !rm train.csv.zip\n",
        "# !mkdir ./logs/"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFoIORVvcWn5",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cB029jhCcJFF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3a3f4542-3f1b-490a-cd84-d79c23175050"
      },
      "source": [
        "import os\n",
        "#os.environ['XLA_USE_BF16'] = \"0\"\n",
        "import gc\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "from getpass import getpass\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "#import seaborn as sns\n",
        "##from tqdm.notebook import tqdm\n",
        "from google.colab import auth\n",
        "from google.cloud import storage\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torchvision import transforms, models\n",
        "\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.utils.serialization as xser\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.utils.utils as xu\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "# from pytorch_lightning.metrics.classification import AUROC\n",
        "# from pytorch_lightning.core.lightning import LightningModule\n",
        "# from pytorch_lightning import Trainer\n",
        "# #from pytorch_lightning.loggers import TensorBoardLogger\n",
        "# from pytorch_lightning.metrics.functional import auroc\n",
        "\n",
        "from ranger import Ranger\n",
        "from catalyst.data.sampler import DistributedSamplerWrapper, BalanceClassSampler\n",
        "import timm\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "#seed_everything(43)\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning:\n",
            "\n",
            "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-XASBvOcVwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_DIR = '/content/data/'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uG9Xunfy3N3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(DATA_DIR+'folds.csv')\n",
        "df_test = pd.read_csv(DATA_DIR+'test.csv').rename(columns={'image_name':'image_id'})\n",
        "sample_submission = pd.read_csv(DATA_DIR+'sample_submission.csv')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIKzfKD2zA9A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "53301c85-ce60-4bf6-e5d6-f3509a7a81d2"
      },
      "source": [
        "df_train['fold'].value_counts()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    12219\n",
              "1    12106\n",
              "2    12072\n",
              "4    12048\n",
              "3    12042\n",
              "Name: fold, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOuLmekTzlM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fold_no = 1\n",
        "X_train = df_train[df_train['fold'] != fold_no][[col for col in df_train.columns if col != 'target']]\n",
        "y_train = df_train[df_train['fold'] != fold_no][[col for col in df_train.columns if col == 'target']]\n",
        "X_val = df_train[df_train['fold'] == fold_no][[col for col in df_train.columns if col != 'target']]\n",
        "y_val = df_train[df_train['fold'] == fold_no][[col for col in df_train.columns if col == 'target']]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yovLpNhMcvnW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d4afdbc6-9abb-44a7-ebf2-3d6c50f8c6c7"
      },
      "source": [
        "print('X_train', X_train.shape)\n",
        "print('y_train', y_train.shape)\n",
        "print('X_val', X_val.shape)\n",
        "print('y_val', y_val.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train (48381, 8)\n",
            "y_train (48381, 1)\n",
            "X_val (12106, 8)\n",
            "y_val (12106, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWae70vKk9dw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "1c16077d-1024-4247-94d2-54bc0accbcdf"
      },
      "source": [
        "print('Train target distribution: ')\n",
        "print(y_train['target'].value_counts())\n",
        "print('Val target distribution: ')\n",
        "print(y_val['target'].value_counts())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train target distribution: \n",
            "0    43997\n",
            "1     4384\n",
            "Name: target, dtype: int64\n",
            "Val target distribution: \n",
            "0    11011\n",
            "1     1095\n",
            "Name: target, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwWrFw5C-R5J",
        "colab_type": "text"
      },
      "source": [
        "##  Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wK-NTFy-PwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MelanomaDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, df, labels, istrain=False, transforms=None):\n",
        "        super().__init__()\n",
        "        self.image_id = df['image_id'].values\n",
        "        self.transforms = transforms\n",
        "        self.labels = labels.values\n",
        "        self.neg_indices = np.where(self.labels==0)[0]\n",
        "        self.pos_indices = np.where(self.labels==1)[0]\n",
        "        self.istrain = istrain\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_id)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if torch.is_tensor(index):\n",
        "            index = index.tolist()\n",
        "        \n",
        "        image, target = self.load_image(index)\n",
        "\n",
        "        if self.transforms:\n",
        "            image = self.transforms(image=image)['image']\n",
        "\n",
        "        #if self.istrain == False:\n",
        "        #      return image, target\n",
        "\n",
        "        #rng = np.random.choice([1, 2])\n",
        "        #if rng == 1:\n",
        "        #    image, target = self.cutmix(image, target, alpha=1)\n",
        "        #elif rng == 2:\n",
        "        #    image, target = self.mixup(image, target, alpha=1) \n",
        "        ##else:\n",
        "        ##    target = [target]\n",
        "\n",
        "        return image, target#, rng\n",
        "    \n",
        "    def load_image(self, index):\n",
        "        if torch.is_tensor(index):\n",
        "            index = index.tolist()\n",
        "        image_name = DATA_DIR + f'512x512-dataset-melanoma/512x512-dataset-melanoma/{self.image_id[index]}.jpg'\n",
        "        image = cv2.imread(image_name, cv2.IMREAD_COLOR).astype(np.uint8)\n",
        "        target = self.labels[index].astype(np.float32)\n",
        "        return image, target\n",
        "\n",
        "    def cutmix(self, data, target, alpha=1):\n",
        "        rand_index = self.get_rand_index()\n",
        "        random_image, random_target = self.load_image(rand_index)\n",
        "        if self.transforms:\n",
        "            random_image = self.transforms(image=random_image)['image']\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "        bbx1, bby1, bbx2, bby2 = self.rand_bbox(data.size(), lam)\n",
        "        data[:, bbx1:bbx2, bby1:bby2] = random_image[ :, bbx1:bbx2, bby1:bby2]\n",
        "        # adjust lambda to exactly match pixel ratio\n",
        "        lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (data.size()[-1] * data.size()[-2]))\n",
        "        targets = [target, random_target, lam]\n",
        "        return data, targets\n",
        "\n",
        "    def mixup(self, data, target, alpha=1):\n",
        "        rand_index = self.get_rand_index()\n",
        "        random_image, random_target = self.load_image(rand_index)\n",
        "        if self.transforms:\n",
        "            random_image = self.transforms(image=random_image)['image']\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "        data = data * lam + random_image * (1 - lam)\n",
        "        targets = [target, random_target, lam]\n",
        "        return data, targets\n",
        "\n",
        "    def get_rand_index(self):\n",
        "        if random.random()>0.5:\n",
        "            rand_index = np.random.choice(self.pos_indices)\n",
        "        else:\n",
        "            rand_index = np.random.choice(self.neg_indices)\n",
        "        return rand_index\n",
        "\n",
        "    def rand_bbox(self, size, lam):\n",
        "        W = size[1]\n",
        "        H = size[2]\n",
        "        cut_rat = np.sqrt(1. - lam)\n",
        "        cut_w = np.int(W * cut_rat)\n",
        "        cut_h = np.int(H * cut_rat)\n",
        "        cx = np.random.randint(W)\n",
        "        cy = np.random.randint(H)\n",
        "        bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "        bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "        bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "        bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "        return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "def get_datasets():\n",
        "    datasets = {}\n",
        "    datasets['train'] = MelanomaDataset(\n",
        "        X_train, y_train, istrain=True, transforms=get_train_transforms()\n",
        "    )\n",
        "    datasets['valid'] = MelanomaDataset(\n",
        "        X_val, y_val, istrain=False, transforms=get_valid_transforms()\n",
        "    )\n",
        "    return datasets\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFL68tLin0Ky",
        "colab_type": "text"
      },
      "source": [
        "## Augmentations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPWu-IIliVVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%writefile augmentations.txt\n",
        "# Reference IMG_SIZE\n",
        "# B0 - 224\n",
        "# B1 - 240\n",
        "# B2 - 260\n",
        "# B3 - 300\n",
        "# B4 - 380\n",
        "# B5 - 456\n",
        "# B6 - 520\n",
        "# B7 - 600\n",
        "# B8 - 672\n",
        "# L2 NS - 475\n",
        "# L2 - 800\n",
        "\n",
        "# Transforms\n",
        "IMG_SIZE = 224\n",
        "mean = (0.485, 0.456, 0.406)\n",
        "std = (0.229, 0.224, 0.225)\n",
        "\n",
        "def get_train_transforms(p=1.0):\n",
        "    return A.Compose([\n",
        "        A.Resize(IMG_SIZE, IMG_SIZE, interpolation=2, always_apply=True, p=1),\n",
        "        A.RandomResizedCrop(\n",
        "            IMG_SIZE, IMG_SIZE, scale=(0.8, 1.2), interpolation=2, p=0.33\n",
        "        ),\n",
        "        A.Flip(p=0.33),\n",
        "        A.Transpose(p=0.33),\n",
        "        #A.OneOf([\n",
        "        #    A.MedianBlur(blur_limit=3, p=0.5),\n",
        "        #    A.Blur(blur_limit=3, p=0.5),\n",
        "        #], p=0.5),\n",
        "        A.ShiftScaleRotate(\n",
        "            interpolation=2,\n",
        "            shift_limit=0.0625, scale_limit=0.15, \n",
        "            rotate_limit=15, p=0.3\n",
        "        ),\n",
        "        #A.OneOf([\n",
        "        #    A.OpticalDistortion(p=0.3),\n",
        "        #    A.GridDistortion(p=.1),\n",
        "        #    A.IAAPiecewiseAffine(p=0.3),\n",
        "        #], p=0.5),\n",
        "        #A.OneOf([\n",
        "        #    A.CLAHE(clip_limit=2),\n",
        "        #    A.IAASharpen(),\n",
        "        #    A.IAAEmboss(),\n",
        "        #    A.RandomBrightnessContrast(),            \n",
        "        #], p=0.5),\n",
        "        A.HueSaturationValue(\n",
        "            hue_shift_limit=20, sat_shift_limit=30, \n",
        "            val_shift_limit=20, p=0.33\n",
        "        ),\n",
        "        A.MultiplicativeNoise(\n",
        "            multiplier=[0.75, 1.25], \n",
        "            elementwise=True, p=0.33\n",
        "        ),\n",
        "        A.Normalize(mean, std, max_pixel_value=255.0, always_apply=True),\n",
        "        ToTensorV2(p=1.0),\n",
        "    ], p=p)\n",
        "\n",
        "def get_valid_transforms():\n",
        "    return A.Compose([\n",
        "        A.Resize(IMG_SIZE, IMG_SIZE, interpolation=2, always_apply=True, p=1),\n",
        "        A.Normalize(mean, std, max_pixel_value=255.0, always_apply=True),\n",
        "        ToTensorV2(p=1.0),\n",
        "    ])\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9m9lx4ewYzk",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfPBXkoYyPAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResNet18(nn.Module): \n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = models.resnet18(pretrained=True)\n",
        "        in_features = self.model.fc.in_features\n",
        "        self.model.fc = nn.Linear(in_features, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "class Tf_efficientnet_b0_ns(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model('tf_efficientnet_b0_ns', pretrained=True)\n",
        "        in_features = self.model.classifier.in_features\n",
        "        self.model.classifier = nn.Linear(in_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class Tf_efficientnet_b3_ns(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model('tf_efficientnet_b3_ns', pretrained=True)\n",
        "        in_features = self.model.classifier.in_features\n",
        "        self.model.classifier = nn.Linear(in_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class Tf_efficientnet_b3_ns_Mod(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model('tf_efficientnet_b3_ns', pretrained=True)\n",
        "        in_features = self.model.classifier.in_features\n",
        "        self.model.classifier = nn.Linear(in_features, int(in_features/2))\n",
        "        self.bn_1 = nn.BatchNorm1d(int(in_features/2))\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.drop_1 = nn.Dropout(0.2)\n",
        "        self.fc_2 = nn.Linear(int(in_features/2), 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x = self.bn_1(x)\n",
        "        x = self.relu_1(x)\n",
        "        x = self.drop_1(x)\n",
        "        x = self.fc_2(x)\n",
        "        return x\n",
        "\n",
        "class Tf_efficientnet_b3_ns_Mod_v2(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model('tf_efficientnet_b3_ns', pretrained=True)\n",
        "        in_features = self.model.classifier.in_features\n",
        "        self.model.classifier = nn.Linear(in_features, in_features)\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.bn_1 = nn.BatchNorm1d(in_features)\n",
        "        self.fc_1 = nn.Linear(in_features, int(in_features/2))\n",
        "        self.bn_2 = nn.BatchNorm1d(int(in_features/2))\n",
        "        self.fc_2 = nn.Linear(int(in_features/2), 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x = self.relu_1(x)\n",
        "        x = self.bn_1(x)\n",
        "        x = self.fc_1(x)\n",
        "        x = self.bn_2(x)\n",
        "        x = self.fc_2(x)\n",
        "        return x\n",
        "\n",
        "class Gluon_seresnext101_32x4d(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model('gluon_seresnext101_32x4d', pretrained=True)\n",
        "        in_features = self.model.classifier.in_features\n",
        "        self.model.classifier = nn.Linear(in_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class Gluon_seresnext50_32x4d(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model('gluon_seresnext50_32x4d', pretrained=True)\n",
        "        in_features = self.model.fc.in_features\n",
        "        self.model.fc = nn.Linear(in_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndmsl1XUrWFp",
        "colab_type": "text"
      },
      "source": [
        "## Custom Losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IICyzNqJrZnl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2, logits=True, reduce=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.logits = logits\n",
        "        self.reduce = reduce\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        if self.logits:\n",
        "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
        "        else:\n",
        "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-BCE_loss)\n",
        "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
        "\n",
        "        if self.reduce:\n",
        "            return torch.mean(F_loss)\n",
        "        else:\n",
        "            return F_loss\n",
        "\n",
        "def ohem_loss(cls_pred, cls_target, rate):\n",
        "    ohem_cls_loss = F.binary_cross_entropy_with_logits(cls_pred, cls_target, reduction='none')\n",
        "    batch_size = cls_pred.size(0)\n",
        "    sorted_ohem_loss, idx = torch.sort(ohem_cls_loss, descending=True)\n",
        "    keep_num = min(sorted_ohem_loss.size()[0], int(batch_size*rate))\n",
        "    if keep_num < sorted_ohem_loss.size()[0]:\n",
        "        keep_idx_cuda = idx[:keep_num]\n",
        "        ohem_cls_loss = ohem_cls_loss[keep_idx_cuda]\n",
        "    cls_loss = ohem_cls_loss.sum() / keep_num\n",
        "    return cls_loss\n",
        "\n",
        "def bce_criterion(y_pred, y_true):\n",
        "    return nn.BCEWithLogitsLoss()(y_pred, y_true)\n",
        "\n",
        "def focal_criterion(y_pred, y_true):\n",
        "    return FocalLoss(alpha=(43997/4384))(y_pred, y_true)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruo8SHLGnZJU",
        "colab_type": "text"
      },
      "source": [
        "## Train script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmPxF8kEnTrY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class RocAucMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.y_true = np.array([0,1])\n",
        "        self.y_pred = np.array([0.5,0.5])\n",
        "        self.score = 0\n",
        "\n",
        "    def update(self, y_true, y_pred):\n",
        "        y_true = y_true.cpu().numpy()\n",
        "        y_pred = torch.flatten(torch.sigmoid(y_pred)).data.cpu().numpy()\n",
        "        self.y_true = np.append(self.y_true, y_true)\n",
        "        self.y_pred = np.append(self.y_pred, y_pred)\n",
        "        self.score = roc_auc_score(self.y_true, self.y_pred)\n",
        "\n",
        "    @property\n",
        "    def avg(self):\n",
        "        return self.score\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TggTDLcOxUT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "SERIAL_EXEC = xmp.MpSerialExecutor()\n",
        "WRAPPED_MODEL = xmp.MpModelWrapper(Tf_efficientnet_b0_ns())\n",
        "\n",
        "def train_model():\n",
        "    seed_everything(43)\n",
        "    best_score = 0.0\n",
        "    datasets = SERIAL_EXEC.run(get_datasets)\n",
        "\n",
        "    #sampler\n",
        "    labels_vcount = y_train['target'].value_counts()\n",
        "    class_counts = [labels_vcount[0].astype(np.float32), labels_vcount[1].astype(np.float32)]\n",
        "    num_samples = sum(class_counts)\n",
        "    class_weights = [num_samples/class_counts[i] for i in range(len(class_counts))]\n",
        "    weights = [class_weights[y_train['target'].values[i]] for i in range(int(num_samples))]\n",
        "    wrsampler = WeightedRandomSampler(\n",
        "        torch.DoubleTensor(weights), int(num_samples)\n",
        "    )\n",
        "    #BalanceClassSampler(labels=y_train['target'].values, mode=\"downsampling\"),\n",
        "    \n",
        "    train_sampler = DistributedSamplerWrapper(\n",
        "        sampler=wrsampler,\n",
        "        num_replicas=xm.xrt_world_size(),\n",
        "        rank=xm.get_ordinal(),\n",
        "        shuffle=True\n",
        "    )\n",
        "    validation_sampler = DistributedSampler(\n",
        "        datasets['valid'],\n",
        "        num_replicas=xm.xrt_world_size(),\n",
        "        rank=xm.get_ordinal(),\n",
        "        shuffle=False\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        datasets['train'],\n",
        "        batch_size=FLAGS['batch_size'], \n",
        "        num_workers=FLAGS['num_workers'],\n",
        "        sampler=train_sampler,\n",
        "        drop_last=True,\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        datasets['valid'],\n",
        "        batch_size=FLAGS['batch_size'],\n",
        "        num_workers=FLAGS['num_workers'],\n",
        "        sampler=validation_sampler,\n",
        "        drop_last=True\n",
        "    )\n",
        "    \n",
        "    device = xm.xla_device()\n",
        "    model = WRAPPED_MODEL.to(device)\n",
        "    optimizer = Ranger(\n",
        "        model.parameters(),\n",
        "        lr=FLAGS['learning_rate'] * xm.xrt_world_size(), \n",
        "        alpha=0.5, k=6, N_sma_threshhold=5,\n",
        "        weight_decay=FLAGS['weight_decay']\n",
        "    )\n",
        "    #optimizer = torch.optim.AdamW(\n",
        "    #    model.parameters(), \n",
        "    #    lr=FLAGS['learning_rate'] * xm.xrt_world_size(), \n",
        "    #    weight_decay=FLAGS['weight_decay']\n",
        "    #)\n",
        "\n",
        "    #criterion = focal_criterion\n",
        "    criterion = bce_criterion\n",
        "\n",
        "    def train_one_epoch(loader):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        max_idx = 0\n",
        "        \n",
        "        xm.master_print('Step\\t|\\tTime')\n",
        "        for idx, (images, targets) in enumerate(loader):\n",
        "            images = images.to(device)\n",
        "            targets = targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(images.float())\n",
        "            loss = criterion(y_pred, targets)\n",
        "            running_loss += float(loss)\n",
        "            loss.backward()\n",
        "            xm.optimizer_step(optimizer)\n",
        "            max_idx = float(idx)\n",
        "            if idx % FLAGS['log_steps'] == 0 and idx !=0:\n",
        "                xm.master_print('({})\\t|\\t{}'.format(\n",
        "                    idx, time.asctime(time.localtime()))\n",
        "                )\n",
        "        \n",
        "        return running_loss/(max_idx+1)\n",
        "\n",
        "    def val_one_epoch(loader):\n",
        "        model.eval()\n",
        "        running_loss = 0\n",
        "        max_idx = 0\n",
        "        roc_auc_scores = RocAucMeter()\n",
        "        with torch.no_grad():\n",
        "            for idx, (images, targets) in enumerate(loader):\n",
        "                images = images.to(device)\n",
        "                targets = targets.to(device)\n",
        "                y_pred = model(images.float())\n",
        "                loss = criterion(y_pred, targets)\n",
        "                running_loss += float(loss)\n",
        "                max_idx = float(idx)\n",
        "                roc_auc_scores.update(targets, y_pred)\n",
        "        score = roc_auc_scores.avg\n",
        "        return running_loss/(max_idx+1), score\n",
        "\n",
        "    for epoch in range(0, FLAGS['num_epochs']):\n",
        "\n",
        "        xm.master_print('-'*27 + f'Epoch #{epoch+1} started' + '-'*27)\n",
        "        xm.master_print(f'Epoch start {time.asctime(time.localtime())}')\n",
        "\n",
        "        train_start = time.time()\n",
        "        para_loader = pl.ParallelLoader(train_loader, [device])\n",
        "        train_loss = train_one_epoch(para_loader.per_device_loader(device)) \n",
        "        xm.master_print(\"finished training epoch {}\".format(epoch+1))\n",
        "        xm.master_print(f'elapsed time: {(time.time()-train_start)/60.0:.2f}mins')\n",
        "        xm.master_print(\n",
        "            f'average loss for epoch #{epoch+1} : {train_loss:.5f}'\n",
        "        )\n",
        "        #gc.collect()\n",
        "        \n",
        "        if (epoch+1) % 1 == 0:\n",
        "            val_start = time.time()\n",
        "            para_loader = pl.ParallelLoader(val_loader, [device])\n",
        "            val_loss, auc_score = val_one_epoch(para_loader.per_device_loader(device))    \n",
        "            xm.master_print(\"finished validating epoch {}\".format(epoch+1))\n",
        "            xm.master_print(f'roc_auc_score: {auc_score:.5f}')\n",
        "            xm.master_print(f'average loss for val epoch: {val_loss:.5f}')\n",
        "            xm.master_print(f'elapsed time: {(time.time()-val_start)/60.0:.2f}mins')\n",
        "            #gc.collect()\n",
        "            #if auc_score > best_score:\n",
        "            #    best_score = auc_score\n",
        "            xm.save(\n",
        "                model.state_dict(), \n",
        "                f\"./{FLAGS['exp_name']}_epoch_{epoch+1}_auc_{auc_score:.5f}.pth\"\n",
        "            )\n",
        "            #    xm.master_print(f'new best score: {best_score:.5f}')\n",
        "            xm.master_print(f'saved model...')\n",
        "                #if epoch > 3:\n",
        "\n",
        "            #xser.save(model.state_dict(), f\"model.bin\", master_only=True)\n",
        "            #model.load_state_dict(xser.load(f\"./model_epoch_{epoch+1}.pth\"))\n",
        "                #gc.collect()\n",
        "        \n",
        "        xm.master_print(f'Epoch end {time.asctime(time.localtime())}')\n",
        "        xm.master_print('-'*28 + f'Epoch #{epoch+1} ended' + '-'*28)\n",
        "    \n",
        "    #return best_score\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmLSHZZdS_1i",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHJtNpfOUdqD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e94e754d-e441-4dec-e7b7-2371c870ba5d"
      },
      "source": [
        "FLAGS = {}\n",
        "FLAGS['batch_size'] = 32\n",
        "FLAGS['num_workers'] = 0\n",
        "FLAGS['learning_rate'] = 2e-4\n",
        "FLAGS['num_epochs'] = 10\n",
        "FLAGS['weight_decay'] = 1e-4\n",
        "FLAGS['log_steps'] = 25\n",
        "FLAGS['img_size'] = IMG_SIZE\n",
        "FLAGS['loss'] = 'BCE'\n",
        "FLAGS['optimizer'] = 'AdamW'\n",
        "FLAGS['exp_name'] = 'Tf_efficientnet_b0_ns'\n",
        "FLAGS['fold'] = 1\n",
        "FLAGS['num_cores'] = 8\n",
        "\n",
        "def _mp_fn(rank, flags):\n",
        "  global FLAGS\n",
        "  FLAGS = flags\n",
        "  torch.set_default_tensor_type('torch.FloatTensor')\n",
        "  train_model()\n",
        "\n",
        "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8,\n",
        "          start_method='fork')\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ranger optimizer loaded. \n",
            "Gradient Centralization usage = True\n",
            "GC applied to both conv and fc layers\n",
            "---------------------------Epoch #1 started---------------------------\n",
            "Epoch start Wed Jun 24 11:26:12 2020\n",
            "Step\t|\tTime\n",
            "Ranger optimizer loaded. \n",
            "Gradient Centralization usage = True\n",
            "GC applied to both conv and fc layers\n",
            "Ranger optimizer loaded. \n",
            "Gradient Centralization usage = True\n",
            "GC applied to both conv and fc layers\n",
            "Ranger optimizer loaded. \n",
            "Gradient Centralization usage = True\n",
            "GC applied to both conv and fc layers\n",
            "Ranger optimizer loaded. \n",
            "Gradient Centralization usage = True\n",
            "GC applied to both conv and fc layers\n",
            "Ranger optimizer loaded. \n",
            "Gradient Centralization usage = True\n",
            "GC applied to both conv and fc layers\n",
            "Ranger optimizer loaded. \n",
            "Gradient Centralization usage = True\n",
            "GC applied to both conv and fc layers\n",
            "Ranger optimizer loaded. \n",
            "Gradient Centralization usage = True\n",
            "GC applied to both conv and fc layers\n",
            "(25)\t|\tWed Jun 24 11:27:01 2020\n",
            "(50)\t|\tWed Jun 24 11:27:33 2020\n",
            "(75)\t|\tWed Jun 24 11:28:04 2020\n",
            "(100)\t|\tWed Jun 24 11:28:37 2020\n",
            "(125)\t|\tWed Jun 24 11:29:12 2020\n",
            "(150)\t|\tWed Jun 24 11:29:43 2020\n",
            "(175)\t|\tWed Jun 24 11:30:14 2020\n",
            "finished training epoch 1\n",
            "elapsed time: 4.26mins\n",
            "average loss for epoch #1 : 0.47514\n",
            "finished validating epoch 1\n",
            "roc_auc_score: 0.91640\n",
            "average loss for val epoch: 0.36316\n",
            "elapsed time: 0.99mins\n",
            "saved model...\n",
            "Epoch end Wed Jun 24 11:31:31 2020\n",
            "----------------------------Epoch #1 ended----------------------------\n",
            "---------------------------Epoch #2 started---------------------------\n",
            "Epoch start Wed Jun 24 11:31:31 2020\n",
            "Step\t|\tTime\n",
            "(25)\t|\tWed Jun 24 11:32:14 2020\n",
            "(50)\t|\tWed Jun 24 11:32:45 2020\n",
            "(75)\t|\tWed Jun 24 11:33:17 2020\n",
            "(100)\t|\tWed Jun 24 11:33:50 2020\n",
            "(125)\t|\tWed Jun 24 11:34:24 2020\n",
            "(150)\t|\tWed Jun 24 11:34:56 2020\n",
            "(175)\t|\tWed Jun 24 11:35:27 2020\n",
            "finished training epoch 2\n",
            "elapsed time: 4.15mins\n",
            "average loss for epoch #2 : 0.34141\n",
            "finished validating epoch 2\n",
            "roc_auc_score: 0.93150\n",
            "average loss for val epoch: 0.26530\n",
            "elapsed time: 1.04mins\n",
            "saved model...\n",
            "Epoch end Wed Jun 24 11:36:43 2020\n",
            "----------------------------Epoch #2 ended----------------------------\n",
            "---------------------------Epoch #3 started---------------------------\n",
            "Epoch start Wed Jun 24 11:36:43 2020\n",
            "Step\t|\tTime\n",
            "(25)\t|\tWed Jun 24 11:37:26 2020\n",
            "(50)\t|\tWed Jun 24 11:37:58 2020\n",
            "(75)\t|\tWed Jun 24 11:38:30 2020\n",
            "(100)\t|\tWed Jun 24 11:39:01 2020\n",
            "(125)\t|\tWed Jun 24 11:39:35 2020\n",
            "(150)\t|\tWed Jun 24 11:40:08 2020\n",
            "(175)\t|\tWed Jun 24 11:40:40 2020\n",
            "finished training epoch 3\n",
            "elapsed time: 4.16mins\n",
            "average loss for epoch #3 : 0.27735\n",
            "finished validating epoch 3\n",
            "roc_auc_score: 0.91968\n",
            "average loss for val epoch: 0.25735\n",
            "elapsed time: 1.05mins\n",
            "saved model...\n",
            "Epoch end Wed Jun 24 11:41:56 2020\n",
            "----------------------------Epoch #3 ended----------------------------\n",
            "---------------------------Epoch #4 started---------------------------\n",
            "Epoch start Wed Jun 24 11:41:56 2020\n",
            "Step\t|\tTime\n",
            "(25)\t|\tWed Jun 24 11:42:37 2020\n",
            "(50)\t|\tWed Jun 24 11:43:09 2020\n",
            "(75)\t|\tWed Jun 24 11:43:42 2020\n",
            "(100)\t|\tWed Jun 24 11:44:14 2020\n",
            "(125)\t|\tWed Jun 24 11:44:48 2020\n",
            "(150)\t|\tWed Jun 24 11:45:21 2020\n",
            "(175)\t|\tWed Jun 24 11:45:52 2020\n",
            "finished training epoch 4\n",
            "elapsed time: 4.15mins\n",
            "average loss for epoch #4 : 0.23555\n",
            "finished validating epoch 4\n",
            "roc_auc_score: 0.92447\n",
            "average loss for val epoch: 0.27288\n",
            "elapsed time: 0.86mins\n",
            "saved model...\n",
            "Epoch end Wed Jun 24 11:47:08 2020\n",
            "----------------------------Epoch #4 ended----------------------------\n",
            "---------------------------Epoch #5 started---------------------------\n",
            "Epoch start Wed Jun 24 11:47:08 2020\n",
            "Step\t|\tTime\n",
            "(25)\t|\tWed Jun 24 11:47:51 2020\n",
            "(50)\t|\tWed Jun 24 11:48:23 2020\n",
            "(75)\t|\tWed Jun 24 11:48:53 2020\n",
            "(100)\t|\tWed Jun 24 11:49:25 2020\n",
            "(125)\t|\tWed Jun 24 11:50:00 2020\n",
            "(150)\t|\tWed Jun 24 11:50:33 2020\n",
            "(175)\t|\tWed Jun 24 11:51:04 2020\n",
            "finished training epoch 5\n",
            "elapsed time: 4.15mins\n",
            "average loss for epoch #5 : 0.19821\n",
            "finished validating epoch 5\n",
            "roc_auc_score: 0.92410\n",
            "average loss for val epoch: 0.23684\n",
            "elapsed time: 0.86mins\n",
            "saved model...\n",
            "Epoch end Wed Jun 24 11:52:20 2020\n",
            "----------------------------Epoch #5 ended----------------------------\n",
            "---------------------------Epoch #6 started---------------------------\n",
            "Epoch start Wed Jun 24 11:52:20 2020\n",
            "Step\t|\tTime\n",
            "(25)\t|\tWed Jun 24 11:53:02 2020\n",
            "(50)\t|\tWed Jun 24 11:53:35 2020\n",
            "(75)\t|\tWed Jun 24 11:54:07 2020\n",
            "(100)\t|\tWed Jun 24 11:54:39 2020\n",
            "(125)\t|\tWed Jun 24 11:55:13 2020\n",
            "(150)\t|\tWed Jun 24 11:55:44 2020\n",
            "(175)\t|\tWed Jun 24 11:56:16 2020\n",
            "finished training epoch 6\n",
            "elapsed time: 4.15mins\n",
            "average loss for epoch #6 : 0.19406\n",
            "finished validating epoch 6\n",
            "roc_auc_score: 0.93116\n",
            "average loss for val epoch: 0.23151\n",
            "elapsed time: 1.05mins\n",
            "saved model...\n",
            "Epoch end Wed Jun 24 11:57:33 2020\n",
            "----------------------------Epoch #6 ended----------------------------\n",
            "---------------------------Epoch #7 started---------------------------\n",
            "Epoch start Wed Jun 24 11:57:33 2020\n",
            "Step\t|\tTime\n",
            "(25)\t|\tWed Jun 24 11:58:15 2020\n",
            "(50)\t|\tWed Jun 24 11:58:47 2020\n",
            "(75)\t|\tWed Jun 24 11:59:19 2020\n",
            "(100)\t|\tWed Jun 24 11:59:51 2020\n",
            "(125)\t|\tWed Jun 24 12:00:26 2020\n",
            "(150)\t|\tWed Jun 24 12:00:57 2020\n",
            "(175)\t|\tWed Jun 24 12:01:29 2020\n",
            "finished training epoch 7\n",
            "elapsed time: 4.15mins\n",
            "average loss for epoch #7 : 0.17538\n",
            "finished validating epoch 7\n",
            "roc_auc_score: 0.93457\n",
            "average loss for val epoch: 0.24719\n",
            "elapsed time: 0.92mins\n",
            "saved model...\n",
            "Epoch end Wed Jun 24 12:02:45 2020\n",
            "----------------------------Epoch #7 ended----------------------------\n",
            "---------------------------Epoch #8 started---------------------------\n",
            "Epoch start Wed Jun 24 12:02:45 2020\n",
            "Step\t|\tTime\n",
            "(25)\t|\tWed Jun 24 12:03:26 2020\n",
            "(50)\t|\tWed Jun 24 12:03:59 2020\n",
            "(75)\t|\tWed Jun 24 12:04:30 2020\n",
            "(100)\t|\tWed Jun 24 12:05:02 2020\n",
            "(125)\t|\tWed Jun 24 12:05:36 2020\n",
            "(150)\t|\tWed Jun 24 12:06:09 2020\n",
            "(175)\t|\tWed Jun 24 12:06:40 2020\n",
            "finished training epoch 8\n",
            "elapsed time: 4.15mins\n",
            "average loss for epoch #8 : 0.15109\n",
            "finished validating epoch 8\n",
            "roc_auc_score: 0.92524\n",
            "average loss for val epoch: 0.24610\n",
            "elapsed time: 0.91mins\n",
            "saved model...\n",
            "Epoch end Wed Jun 24 12:07:57 2020\n",
            "----------------------------Epoch #8 ended----------------------------\n",
            "---------------------------Epoch #9 started---------------------------\n",
            "Epoch start Wed Jun 24 12:07:57 2020\n",
            "Step\t|\tTime\n",
            "(25)\t|\tWed Jun 24 12:08:37 2020\n",
            "(50)\t|\tWed Jun 24 12:09:11 2020\n",
            "(75)\t|\tWed Jun 24 12:09:42 2020\n",
            "(100)\t|\tWed Jun 24 12:10:14 2020\n",
            "(125)\t|\tWed Jun 24 12:10:49 2020\n",
            "(150)\t|\tWed Jun 24 12:11:21 2020\n",
            "(175)\t|\tWed Jun 24 12:11:53 2020\n",
            "finished training epoch 9\n",
            "elapsed time: 4.15mins\n",
            "average loss for epoch #9 : 0.13667\n",
            "finished validating epoch 9\n",
            "roc_auc_score: 0.92398\n",
            "average loss for val epoch: 0.25562\n",
            "elapsed time: 1.03mins\n",
            "saved model...\n",
            "Epoch end Wed Jun 24 12:13:09 2020\n",
            "----------------------------Epoch #9 ended----------------------------\n",
            "---------------------------Epoch #10 started---------------------------\n",
            "Epoch start Wed Jun 24 12:13:09 2020\n",
            "Step\t|\tTime\n",
            "(25)\t|\tWed Jun 24 12:13:49 2020\n",
            "(50)\t|\tWed Jun 24 12:14:23 2020\n",
            "(75)\t|\tWed Jun 24 12:14:53 2020\n",
            "(100)\t|\tWed Jun 24 12:15:27 2020\n",
            "(125)\t|\tWed Jun 24 12:16:02 2020\n",
            "(150)\t|\tWed Jun 24 12:16:34 2020\n",
            "(175)\t|\tWed Jun 24 12:17:05 2020\n",
            "finished training epoch 10\n",
            "elapsed time: 4.16mins\n",
            "average loss for epoch #10 : 0.11574\n",
            "finished validating epoch 10\n",
            "roc_auc_score: 0.92775\n",
            "average loss for val epoch: 0.23026\n",
            "elapsed time: 0.88mins\n",
            "saved model...\n",
            "Epoch end Wed Jun 24 12:18:21 2020\n",
            "----------------------------Epoch #10 ended----------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NY4yOrBgdKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}